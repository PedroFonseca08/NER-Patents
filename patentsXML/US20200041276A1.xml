<html xmlns="http://www.w3.org/1999/xhtml"><head><style id="xml-viewer-style">/* Copyright 2014 The Chromium Authors
 * Use of this source code is governed by a BSD-style license that can be
 * found in the LICENSE file.
 */

:root {
  color-scheme: light dark;
}

div.header {
    border-bottom: 2px solid black;
    padding-bottom: 5px;
    margin: 10px;
}

@media (prefers-color-scheme: dark) {
  div.header {
    border-bottom: 2px solid white;
  }
}

div.folder &gt; div.hidden {
    display:none;
}

div.folder &gt; span.hidden {
    display:none;
}

.pretty-print {
    margin-top: 1em;
    margin-left: 20px;
    font-family: monospace;
    font-size: 13px;
}

#webkit-xml-viewer-source-xml {
    display: none;
}

.opened {
    margin-left: 1em;
}

.comment {
    white-space: pre;
}

.folder-button {
    user-select: none;
    cursor: pointer;
    display: inline-block;
    margin-left: -10px;
    width: 10px;
    background-repeat: no-repeat;
    background-position: left top;
    vertical-align: bottom;
}

.fold {
    background: url("data:image/svg+xml,&lt;svg xmlns='http://www.w3.org/2000/svg' fill='%23909090' width='10' height='10'&gt;&lt;path d='M0 0 L8 0 L4 7 Z'/&gt;&lt;/svg&gt;");
    height: 10px;
}

.open {
    background: url("data:image/svg+xml,&lt;svg xmlns='http://www.w3.org/2000/svg' fill='%23909090' width='10' height='10'&gt;&lt;path d='M0 0 L0 8 L7 4 Z'/&gt;&lt;/svg&gt;");
    height: 10px;
}
</style></head><body><div id="webkit-xml-viewer-source-xml"><us-patent-application xmlns="" lang="EN" dtd-version="v4.4 2014-04-03" file="US20200041276A1-20200206.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20200122" date-publ="20200206">
<us-bibliographic-data-application lang="EN" country="US">
<publication-reference>
<document-id>
<country>US</country>
<doc-number>20200041276</doc-number>
<kind>A1</kind>
<date>20200206</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>16054694</doc-number>
<date>20180803</date>
</document-id>
</application-reference>
<us-application-series-code>16</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>01</class>
<subclass>C</subclass>
<main-group>21</main-group>
<subgroup>32</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>30</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>08</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>04</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>7</main-group>
<subgroup>73</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>01</class>
<subclass>C</subclass>
<main-group>21</main-group>
<subgroup>32</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
<further-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>30241</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>088</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>0454</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>2207</main-group>
<subgroup>20084</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>2207</main-group>
<subgroup>10028</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>2207</main-group>
<subgroup>30252</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>2207</main-group>
<subgroup>20081</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20170101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>7</main-group>
<subgroup>74</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20200206</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</further-cpc>
</classifications-cpc>
<invention-title id="d2e43">End-To-End Deep Generative Model For Simultaneous Localization And Mapping</invention-title>
<us-parties>
<us-applicants>
<us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Ford Global Technologies, LLC</orgname>
<address>
<city>Dearborn</city>
<state>MI</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="00" designation="us-only">
<addressbook>
<last-name>Chakravarty</last-name>
<first-name>Punarjay</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="01" designation="us-only">
<addressbook>
<last-name>Narayanan</last-name>
<first-name>Praveen</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
</us-bibliographic-data-application>
<abstract id="abstract">
<p id="p-0001" num="0000">The disclosure relates to systems, methods, and devices for simultaneous localization and mapping of a robot in an environment utilizing a variational autoencoder generative adversarial network (VAE-GAN). A method includes receiving an image from a camera of a vehicle and providing the image to a VAE-GAN. The method includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image. The method includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map. The method is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="111.84mm" wi="158.75mm" file="US20200041276A1-20200206-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="183.90mm" wi="136.74mm" file="US20200041276A1-20200206-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="226.23mm" wi="188.47mm" orientation="landscape" file="US20200041276A1-20200206-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="234.70mm" wi="177.55mm" orientation="landscape" file="US20200041276A1-20200206-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="216.75mm" wi="180.93mm" file="US20200041276A1-20200206-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="172.13mm" wi="172.38mm" file="US20200041276A1-20200206-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="231.73mm" wi="175.43mm" file="US20200041276A1-20200206-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="186.86mm" wi="175.43mm" file="US20200041276A1-20200206-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="225.30mm" wi="123.95mm" file="US20200041276A1-20200206-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?summary-of-invention description="Summary of Invention" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The present disclosure relates to methods, systems, and apparatuses for simultaneous localization and mapping of an apparatus in an environment, and particularly relates to simultaneous localization and mapping of a vehicle using a variational autoencoder generative adversarial network.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Localization, mapping, and depth perception in real-time are requirements for certain autonomous systems, including autonomous driving systems or mobile robotics systems. Each of localization, mapping, and depth perception are key components for carrying out certain tasks such as obstacle avoidance, route planning, mapping, localization, pedestrian detection, and human-robot interaction. Depth perception and localization are traditionally performed by expensive active sensing systems such as LIDAR sensors or passive sensing systems such as binocular vision or stereo cameras.</p>
<p id="p-0004" num="0003">Systems, methods, and devices for computing localization, mapping, and depth perception can be integrated in automobiles such as autonomous vehicles and driving assistance systems. Such systems are currently being developed and deployed to provide safety features, reduce an amount of user input required, or even eliminate user involvement entirely. For example, some driving assistance systems, such as crash avoidance systems, may monitor driving, positions, and a velocity of the vehicle and other objects while a human is driving. When the system detects that a crash or impact is imminent the crash avoidance system may intervene and apply a brake, steer the vehicle, or perform other avoidance or safety maneuvers. As another example, autonomous vehicles may drive, navigate, and/or park a vehicle with little or no user input. However, due to the dangers involved in driving and the costs of vehicles, it is extremely important that autonomous vehicles and driving assistance systems operate safely and are able to accurately navigate roads in a variety of different driving environments.</p>
<?summary-of-invention description="Summary of Invention" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0005" num="0004">Non-limiting and non-exhaustive implementations of the present disclosure are described with reference to the following figures, wherein like reference numerals refer to like parts throughout the various views unless otherwise specified. Advantages of the present disclosure will become better understood with regard to the following description and accompanying drawings where:</p>
<p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic block diagram illustrating an example vehicle control system or autonomous vehicle system, according to one embodiment;</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic block diagram of a variational autoencoder generative adversarial network in a training phase, according to one embodiment;</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic block diagram of a variational autoencoder generative adversarial network in a computation phase, according to one embodiment;</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic block diagram illustrating a process for determining a depth map of an environment, according to one embodiment;</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic flow chart diagram of a method for utilizing simultaneous localization and mapping of a vehicle in an environment, according to one embodiment;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 6</figref> is a schematic flow chart diagram of a method for utilizing simultaneous localization and mapping of a vehicle in an environment, according to one embodiment;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 7</figref> is a schematic flow chart diagram of a method for training a variational autoencoder generative adversarial network, according to one embodiment; and</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 8</figref> is a schematic block diagram illustrating an example computing system, according to one embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?detailed-description description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0014" num="0013">Localization of a vehicle along with mapping and depth perception of drivable surfaces or regions is an important aspect of allowing for and improving operation of autonomous vehicle or driver assistance features. For example, a vehicle must know precisely where obstacles or drivable surfaces are located to navigate safely around objects.</p>
<p id="p-0015" num="0014">Simultaneous Localization and Mapping (SLAM) forms the basis for operational functionality of mobile robots, including autonomous vehicles and other mobile robots. Examples of such robots include an indoor mobile robot configured for delivering items in a warehouse or an autonomous drone configured for traversing a building or other environment in a disaster scenario. SLAM is directed to sensing the robot's environment and building a map of its surroundings as the robot moves through its environment. SLAM is further directed to simultaneously localizing the robot within its environment by extracting pose vector data, including six Degree of Freedom (DoF) poses relative to a starting point of the robot. SLAM thus incrementally generates a map of the robot's environment. In the case of a robot repeating a route that it has previously mapped, the robot can solve for the localization subset of the problem without generating a new map. The generating of building a map of a new area necessitates SLAM.</p>
<p id="p-0016" num="0015">SLAM is commonly implemented utilizing a depth sensor, such as a LIDAR sensor or a stereo camera. SLAM normally necessitates such devices for enabling the SLAM process to measure the depth and distance of three-dimensional landmarks and to calculate the robot's position in relation to those landmarks. SLAM may also be implemented using monocular vision, but the depth recovered through triangulation of landmarks from a moving camera over time is up to scale only, such that relative depths of objects in the scene are recovered without absolute depth measurements.</p>
<p id="p-0017" num="0016">Applicant recognizes than allied problem in robots is one of obstacle avoidance. Robots must know how far an object is from the robot such that the robot can determine a collision-free path around the object. Robots utilize LIDAR sensors and stereo camera to determine a dense depth-map of obstacles around the robot. Some of the same obstacles determined through this process may be utilized as three-dimensional landmarks in the SLAM implementation.</p>
<p id="p-0018" num="0017">Applicant has developed systems, methods, and devices for improving operations in both SLAM and obstacle avoidance. Applicant presents systems, methods, and devices for generating a dense depth map for obstacle avoidance, determining a robot's location, and determining pose vector data as a robot traverses its environment. The systems, methods, and devices of the present disclosure utilize a monocular camera and do not necessitate the use of expensive LIDAR sensors or stereo cameras that further require intensive computing resources. The disclosure presents lightweight, inexpensive, and low-computing methods for sensing a robot's surrounding, localizing a robot within its environment, and enabling the robot to generate obstacle avoidance movement procedures. Such systems, methods, and devices of the present disclosure may be implemented on any suitable robotics system, including for example, an autonomous vehicle, a mobile robot, and/or a drone or smart mobility vehicle.</p>
<p id="p-0019" num="0018">Variational autoencoders (VAEs) are a class of latent variable models that provide compressed latent representations of data. A VAE can serve as an autoencoder while further serving as a generative model from which new data can be generated by sampling from a latent manifold. The VAE consists of an encoder, which maps the input to a compressed latent representation. The VAE further includes a decoder configured to decode the latent vector back to an output. The entire VAE system may be trained end to end as a deep neural network.</p>
<p id="p-0020" num="0019">The VAE may be configured to encode meaningful information about various data attributes in its latent manifold which can then be exploited to carry out pertinent tasks. In an implementation of the disclosure, Applicant presents utilizing a shared latent space assumption of a VAE between an image, pose vector data of the image, and a depth map of the image, to facilitate the use of SLAM in conjunction with the VAE.</p>
<p id="p-0021" num="0020">Generative adversarial networks (GANs) are a class of generative models configured to produce high quality samples from probability distributions of interest. In the image domain, a GAN may generate output samples of stellar artistic quality. The training methodology for a GAN is adversarial, in that the generator (the network that produces samples, often called “fakes”) learns by fooling another network called the discriminator that decides whether the samples produced are real or fake. The generator network and the discriminator network are trained in tandem, with the generator network eventually learning to produce samples that succeed in fooling the discriminator network. At such a point, the GAN is able to generate samples from the probability distribution underlying the generative process.</p>
<p id="p-0022" num="0021">Applicant recognizes that VAEs confer advantages in providing latent representations of data for further us. However, one drawback of the VAE is the blurriness of the samples produced. GANs, on the other hand, produce excellent samples but do not have a useful latent representation available. The variational autoencoder generative adversarial network (VAE-GAN) utilizes and combines each system such that one obtains a tractable VAE latent representation while also improving upon the quality of the samples by using a GAN as the generator in the decoder of the VAE. This results in crisper images than a VAE alone.</p>
<p id="p-0023" num="0022">The systems, methods, and devices of the present disclosure utilize the VAE-GAN as the central machinery in the SLAM algorithm. Such systems, methods, and devices receiving an input such as a red-green-blue (RGB) image and outputs corresponding depth maps and pose vector data for the camera that captured the RGB image. The system is trained using a regular stereo visual SLAM pipeline, where stereo visual simultaneous localization and mapping (vSLAM) receives a sequence of stereoscopic images, generates the depth maps and corresponding six Degree of Freedom poses as the stereo camera moves through space. Stereo vSLAM trains the VAE-GAN-SLAM algorithm using a sequence of RGB images, the corresponding depth maps for the images, and the corresponding pose vector data for the images. The VAE-GAN is trained to reconstruct the RGB image, the pose vector data for the image, and the depth map for the image while creating a shared latent space representation of the same. The assumption is that the RGB image, depth map of the image, and pose vector data of the image are sampled from places close together in the real world, are close together in the learnt shared latent space as well. After the networks are trained, the VAE-GAN take as its input an RGB image coming from a monocular camera moving through the same environment and produce both a depth map and pose vector data for the monocular camera.</p>
<p id="p-0024" num="0023">In an embodiment, the latent space representation of the VAE-GAN also enables disentanglement and latent space arithmetic. An example of such an embodiment would be to isolate a dimension in the latent vector responsible for a certain attribute of interest, such as a pose dimension, and create previously unseen view of a scene by changing the pose vector.</p>
<p id="p-0025" num="0024">Applicant recognizes that the systems, methods, and devices disclosed herein enable the use of the system as a SLAM box for facilitating fast and easy single-image inference producing the pose of a robot and the positions of obstacles in the environment around the robot.</p>
<p id="p-0026" num="0025">Generative adversarial networks (GANs) have shown that image-to-image transformation, for instance segmentation or labelling tasks, can be achieved with smaller amounts of training data compared to regular convolutional neural networks by training generative networks and discriminative networks in an adversarial manner. Applicant presents systems, methods, and devices for depth estimation of a single image using a GAN. Such systems, methods, and devices improve performance over known depth estimation systems, and further require a smaller number of training images. The use of GAN as opposed to a regular convolutional neural network enables the collection of a small amount of training data in each environment, typically in the hundreds of images as opposed to the hundreds of thousands of images required by convolutional neural networks. Such systems, methods, and devices reduce the burden for data collection by an order of magnitude.</p>
<p id="p-0027" num="0026">Applicant further presents systems, methods, and devices for depth estimation utilizing visual simultaneous localization and mapping (vSLAM) methods for ensuring temporal consistency in the generated depth maps produced by the GAN as the camera moves through an environment. The vSLAM module provides pose information of the camera, e.g. how much the camera has moved between successive images. Such pose information is provided to the GAN as a temporal constraint on training the GAN to promote the GAN to generate consistent depth maps for successive images.</p>
<p id="p-0028" num="0027">Before the methods, systems, and devices for determining simultaneous localization and mapping for a robot are disclosed and described, it is to be understood that this disclosure is not limited to the configurations, process steps, and materials disclosed herein as such configurations, process steps, and materials may vary somewhat. It is also to be understood that the terminology employed herein is used for describing implementations only and is not intended to be limiting since the scope of the disclosure will be limited only by the appended claims and equivalents thereof.</p>
<p id="p-0029" num="0028">In describing and claiming the disclosure, the following terminology will be used in accordance with the definitions set out below.</p>
<p id="p-0030" num="0029">It must be noted that, as used in this specification and the appended claims, the singular forms “a,” “an,” and “the” include plural referents unless the context clearly dictates otherwise.</p>
<p id="p-0031" num="0030">As used herein, the terms “comprising,” “including,” “containing,” “characterized by,” and grammatical equivalents thereof are inclusive or open-ended terms that do not exclude additional, unrecited elements or method steps.</p>
<p id="p-0032" num="0031">In one embodiment, a method for mapping and localizing a robot, such as an autonomous vehicle, in an environment is disclosed. The method includes receiving an image from a camera of a vehicle. The method includes providing the image to a variational autoencoder generative adversarial network (VAE-GAN). The method includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image. The method includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map. The method is such that the VAE-GAN comprises a single latent space for encoding a plurality of inputs.</p>
<p id="p-0033" num="0032">Further embodiments and examples will be discussed in relation to the figures below.</p>
<p id="p-0034" num="0033">Referring now to the figures, <figref idref="DRAWINGS">FIG. 1</figref> illustrates an example vehicle control system <b>100</b> that may be used for autonomous or assisted driving. The automated driving/assistance system <b>102</b> may be used to automate or control operation of a vehicle or to aid a human driver. For example, the automated driving/assistance system <b>102</b> may control one or more of braking, steering, acceleration, lights, alerts, driver notifications, radio, or any other auxiliary systems of the vehicle. In another example, the automated driving/assistance system <b>102</b> may not be able to provide any control of the driving (e.g., steering, acceleration, or braking), but may provide notifications and alerts to assist a human driver in driving safely. The automated driving/assistance system <b>102</b> may use a neural network, or other model or algorithm to detect or localize objects based on perception data gathered by one or more sensors.</p>
<p id="p-0035" num="0034">The vehicle control system <b>100</b> also includes one or more sensor systems/devices for detecting a presence of objects near or within a sensor range of a parent vehicle (e.g., a vehicle that includes the vehicle control system <b>100</b>). For example, the vehicle control system <b>100</b> may include one or more radar systems <b>106</b>, one or more LIDAR systems <b>108</b>, one or more camera systems <b>110</b>, a global positioning system (GPS) <b>112</b>, and/or one or more ultrasound systems <b>114</b>. The vehicle control system <b>100</b> may include a data store <b>116</b> for storing relevant or useful data for navigation and safety such as map data, driving history or other data. The vehicle control system <b>100</b> may also include a transceiver <b>118</b> for wireless communication with a mobile or wireless network, other vehicles, infrastructure, or any other communication system.</p>
<p id="p-0036" num="0035">The vehicle control system <b>100</b> may include vehicle control actuators <b>120</b> to control various aspects of the driving of the vehicle such as electric motors, switches or other actuators, to control braking, acceleration, steering or the like. The vehicle control system <b>100</b> may also include one or more displays <b>122</b>, speakers <b>124</b>, or other devices so that notifications to a human driver or passenger may be provided. A display <b>122</b> may include a heads-up display, dashboard display or indicator, a display screen, or any other visual indicator which may be seen by a driver or passenger of a vehicle. A heads-up display may be used to provide notifications or indicate locations of detected objects or overlay instructions or driving maneuvers for assisting a driver. The speakers <b>124</b> may include one or more speakers of a sound system of a vehicle or may include a speaker dedicated to driver notification.</p>
<p id="p-0037" num="0036">It will be appreciated that the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> is given by way of example only. Other embodiments may include fewer or additional components without departing from the scope of the disclosure. Additionally, illustrated components may be combined or included within other components without limitation.</p>
<p id="p-0038" num="0037">In one embodiment, the automated driving/assistance system <b>102</b> is configured to control driving or navigation of a parent vehicle. For example, the automated driving/assistance system <b>102</b> may control the vehicle control actuators <b>120</b> to drive a path on a road, parking lot, driveway or other location. For example, the automated driving/assistance system <b>102</b> may determine a path based on information or perception data provided by any of the components <b>106</b>-<b>114</b>. The sensor systems/devices <b>106</b>-<b>114</b> may be used to obtain real-time sensor data so that the automated driving/assistance system <b>102</b> can assist a driver or drive a vehicle in real-time.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a schematic block diagram of a training phase <b>200</b> of a variational autoencoder generative adversarial network (VAE-GAN) <b>201</b>. The VAE-GAN <b>201</b> includes an image encoder <b>204</b> and a corresponding image decoder <b>206</b>. The VAE-GAN <b>201</b> includes a pose encoder <b>212</b> and a corresponding pose decoder <b>214</b>. The VAE-GAN <b>201</b> includes a depth encoder <b>222</b> and a corresponding depth decoder <b>224</b>. Each of the image decoder <b>206</b>, the pose decoder <b>214</b>, and the depth decoder <b>224</b> includes a generative adversarial network (GAN) that comprises a GAN generator (see e.g. <b>404</b>) and a GAN discriminator (see e.g. <b>408</b>). The VAE-GAN <b>201</b> includes a latent space <b>230</b> that is shared by each of the image encoder <b>204</b>, the image decoder <b>206</b>, the pose encoder <b>212</b>, the pose decoder <b>214</b>, the depth encoder <b>222</b>, and the depth decoder <b>224</b>. The VAE-GAN <b>201</b> receives a training image <b>202</b> at the image encoder <b>204</b> and generates a reconstructed image <b>208</b> based on the training image <b>202</b>. The VAE-GAN <b>201</b> receives training pose vector data <b>210</b> that is based on the training image <b>202</b> at the pose encoder <b>212</b> and the VAE-GAN <b>201</b> generates reconstructed pose vector data <b>216</b> based on the training pose vector data <b>210</b>. The VAE-GAN <b>201</b> receives a training depth map <b>220</b> that is based on the training image <b>202</b> at the depth encoder <b>222</b> and outputs a reconstructed depth map <b>226</b> that is based on the training depth map <b>220</b>.</p>
<p id="p-0040" num="0039">The VAE-GAN <b>201</b> is the central machinery in the simultaneous localization and mapping (SLAM) algorithm of the present disclosure. In an embodiment the VAE-GAN <b>201</b> is trained utilizing a regular stereo visual SLAM pipeline. In such an embodiment, a stereo visual SLAM takes a sequence of stereoscopic images and generates depth maps and corresponding six Degrees of Freedom poses for the stereo camera as the camera moves through space. Stereo visual SLAM trains the VAE-GAN-SLAM algorithm using a sequence of red-green-blue (RGB) images where only the left image of a stereo pair is used, along with the corresponding depth maps and six Degrees of Freedom pose vector data for the sequence of RGB images. The VAE-GAN <b>201</b> is trained under the assumption that the RGB image, the depth map of the image, and the pose vector data of the image are sampled from locations close together in the real world that are also close together in the learnt shared latent space <b>230</b> as well. After the networks are trained, the VAE-GAN <b>201</b> can take as its input an RGB image coming from a monocular camera moving through the same environment and produce both a depth map and a six Degree of Freedom pose vector data for the camera.</p>
<p id="p-0041" num="0040">The training image <b>202</b> is provided to the VAE-GAN <b>201</b> for training the VAE-GAN <b>201</b> to generate pose vector data and/or depth map data based on an image. In an embodiment the training image <b>202</b> is a red-blue-green (RGB) image captured by a monocular camera. In an embodiment the training image <b>202</b> is a single image of a stereo image pair captured by a stereo camera. The reconstructed image <b>208</b> is generated by the VAE-GAN <b>201</b> based on the training image <b>202</b>. The image encoder <b>204</b> and the image decoder <b>206</b> are adversarial to one another and are configured to generate the reconstructed image <b>208</b>. The image encoder <b>204</b> is configured to receiving the training image <b>202</b> and map the training image <b>202</b> to a compress latent representation in the latent space <b>230</b>. The image decoder <b>206</b> comprises a GAN having a GAN generator and a GAN discriminator. The image decoder <b>206</b> is configured to decode the compressed latent representation of the training image <b>202</b> from the latent space <b>230</b>. The GAN of the image decoder <b>206</b> is configured to generate the reconstructed image <b>208</b>.</p>
<p id="p-0042" num="0041">The training pose vector data <b>210</b> is provided to the VAE-GAN <b>201</b> for training the VAE-GAN <b>201</b> to generate pose vector data of an image. In an embodiment, the training pose vector data <b>210</b> includes six Degree of Freedom pose data of a camera that captured the training image <b>202</b>, wherein the six Degree of Freedom pose data indicates a relative pose of the camera when the image was captured as the camera traversed an environment. The reconstructed pose vector data <b>216</b> is generated by the VAE-GAN <b>201</b> based on the training pose vector data <b>210</b>. The pose encoder <b>212</b> is configured to receive the training pose vector data <b>210</b> and map the training pose vector data <b>210</b> to a compressed latent representation in the latent space <b>230</b> of the VEA-GAN <b>201</b>. The pose decoder <b>214</b> is configured to decode the compressed latent representation of the training pose vector data <b>210</b> from the latent space <b>230</b>. The pose decoder <b>214</b> comprises a GAN that comprises a GAN generator and a GAN discriminator. The GAN of the pose decoder <b>214</b> is configured to generate the reconstructed pose vector data <b>216</b> based on the training pose vector data <b>210</b>.</p>
<p id="p-0043" num="0042">The training depth map <b>220</b> is provided to the VAE-GAN <b>201</b> for training the VAE-GAN <b>201</b> to generate a depth map of an image. In an embodiment, the depth map <b>220</b> is based on the training image <b>202</b> and includes depth information for the training image <b>202</b>. The reconstructed depth map <b>226</b> is generated by the VAE-GAN <b>201</b> based on the training depth map <b>220</b>. The depth encoder <b>222</b> is configured to receive the training depth map <b>220</b> and map the training depth map <b>220</b> to a compressed latent representation in the latent space <b>230</b> of the VAE-GAN <b>201</b>. The depth decoder <b>224</b> comprises a GAN that comprises a GAN generator and a GAN discriminator. The depth decoder <b>224</b> is configured to decode the compressed latent representation of the training depth map <b>220</b> from the latent space <b>230</b>. The GAN of the depth decoder <b>224</b> is configured to generate the reconstructed depth map <b>226</b> based on the training depth map <b>220</b>.</p>
<p id="p-0044" num="0043">The latent space <b>230</b> of the VAE-GAN <b>201</b> is shared by each of the image encoder <b>204</b>, the image decoder <b>206</b>, the pose encoder <b>212</b>, the pose decoder <b>214</b>, the depth encoder <b>222</b>, and the depth decoder <b>224</b>. Thus, the VAE-GAN <b>201</b> is trained to generate each of the reconstructed image <b>208</b>, the reconstructed pose vector data <b>216</b>, and the reconstructed depth map <b>226</b> in tandem. In an embodiment, the latent space <b>230</b> includes an encoded latent space vector applicable to each of an image, pose vector data of an image, and a depth map of an image. The latent space <b>230</b> representation of the VAE-GAN <b>201</b> enables disentanglement and latent space arithmetic. An example of the disentanglement and latent space arithmetic includes isolating a dimension in the latent space <b>230</b> responsible for a certain attribute of interest, such as a posed dimension. This may enable the creation of a previously unseen view of a scheme by changing the pose vector. In an embodiment, training the latent space <b>230</b> simultaneously for all three attributes, namely the image, the pose vector data, and the depth map, forces the latent space <b>230</b> to be consistent for each of the attributes. This provides an elegant formulation where the VAE-GAN <b>201</b> is not trained separately for each of an image, pose vector data, and a depth map. Thus, because the VAE-GAN <b>201</b> is trained in tandem, the trained VAE-GAN <b>201</b> may receive an input image and generate any outer output such as pose vector data based on the input image or a depth map based on the input image.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a schematic block diagram of a computing phase <b>300</b> (alternatively may be referred to as a generative or execution phase) of a variational autoencoder generative adversarial network (VAE-GAN) <b>301</b>. The VAE-GAN <b>301</b> includes an image encoder <b>304</b> and a corresponding image decoder <b>306</b>, wherein the image decoder <b>306</b> comprises a GAN configured to generate a reconstructed image based on the RGB image <b>302</b>. In an embodiment as illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, the image encoder <b>304</b> and the image decoder <b>306</b> have been trained (see <figref idref="DRAWINGS">FIG. 2</figref>). The VAE-GAN <b>301</b> includes a pose encoder <b>312</b> and a corresponding pose decoder <b>314</b>, wherein the pose decoder <b>314</b> comprises a GAN configured to generate the reconstructed pose vector data <b>316</b> based on the RGB image <b>302</b>. In an embodiment as illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, the pose encoder <b>312</b> and the pose decoder <b>314</b> have been trained (see <figref idref="DRAWINGS">FIG. 2</figref>). The VAE-GAN <b>301</b> includes a depth encoder <b>322</b> and a corresponding depth decoder <b>324</b>, wherein the depth decoder <b>324</b> comprises a GAN configured to generate the reconstructed depth map <b>326</b> based on the RGB image <b>302</b>. In an embodiment as illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, the depth encoder <b>322</b> and the depth decoder <b>324</b> have been trained (see <figref idref="DRAWINGS">FIG. 2</figref>). The VAE-GAN <b>301</b> includes a latent space <b>330</b> that is shared by the image encoder <b>304</b>, the image decoder <b>306</b>, the pose encoder <b>312</b>, the pose decoder <b>314</b>, the depth encoder <b>322</b>, and the depth decoder <b>324</b>. The VAE-GAN <b>301</b> receives an RGB image <b>302</b> at the image encoder <b>304</b>. The VAE-GAN outputs reconstructed pose vector data <b>316</b> at the trained pose decoder <b>314</b>. The VAE-GAN outputs a reconstructed depth map <b>326</b> at the trained depth decoder <b>324</b>.</p>
<p id="p-0046" num="0045">In an embodiment the RGB image <b>302</b> is a red-green-blue image captured by a monocular camera and provided to the VAE-GAN <b>301</b> after the VAE-GAN <b>301</b> has been trained. In an embodiment, the RGB image <b>302</b> is captured by a monocular camera of a vehicle, is provided to a vehicle controller, and is provided to the VAE-GAN <b>301</b> in real-time. The RGB image <b>302</b> may provide a capture of an environment of the vehicle and may be utilized to determine depth perception for the vehicle surroundings. In such an embodiment the vehicle controller may implement the result of the VAE-GAN <b>301</b> into a SLAM algorithm for computing simultaneous localization and mapping of the vehicle in real-time. The vehicle controller may further provide a notification to a driver, determine a driving maneuver, or execute a driving maneuver based on the results of the SLAM algorithm.</p>
<p id="p-0047" num="0046">The reconstructed pose vector data <b>316</b> is generated by a GAN embedded in the pose decoder <b>314</b> of the VAE-GAN <b>301</b>. The VAE-GAN <b>301</b> may be trained to generate the reconstructed pose vector data <b>316</b> based on a monocular image. In an embodiment as illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, the VAE-GAN <b>301</b> includes a latent space <b>330</b> that is shared by each of an image encoder/decoder, a pose encoder/decoder, and a depth encoder/decoder. The shared latent space <b>330</b> enables the VAE-GAN <b>301</b> to generate any trained output based on an RGB image <b>302</b> (or non-RGB image) as illustrated. The reconstructed pose vector data <b>316</b> includes six Degree of Freedom pose data for a monocular camera. The reconstructed pose vector data <b>316</b> may be utilized by a vehicle to determine a location of the vehicle in its environment and further utilized for simultaneous localization and mapping of the vehicle as it moves through space by implementing the data in a SLAM algorithm.</p>
<p id="p-0048" num="0047">The reconstructed depth map <b>326</b> is generated by a GAN embedded in the depth decoder <b>324</b> of the VAE-GAN <b>301</b>. The VAE-GAN <b>301</b> may be trained to generate the reconstructed depth map <b>326</b> based only on the RGB image <b>302</b>. The reconstructed depth map <b>326</b> provides a dense depth map based on the RGB image <b>302</b> and may provide a dense depth map of a surrounding of a robot or autonomous vehicle. The reconstructed depth map <b>326</b> may be provided to a SLAM algorithm for calculating simultaneous localization and mapping of a robot as the robot moves through its environment. In an embodiment where the robot is an autonomous vehicle, a vehicle controller may then provide a notification to a driver, determine a driving maneuver, and/or execute a driving maneuver such as an obstacle avoidance maneuver based on the reconstructed depth map <b>326</b> and the result of the SLAM algorithm.</p>
<p id="p-0049" num="0048">The latent space <b>330</b> is shared by each of the image encoder <b>304</b>, the image decoder <b>306</b>, the pose encoder <b>312</b>, the pose decoder <b>314</b>, the depth encoder <b>322</b>, and the depth decoder <b>324</b>. In an embodiment the latent space <b>330</b> comprises an encoded latent space vector that is utilized for each of an image, pose vector data of an image, and a depth map of an image. In such an embodiment, the VAE-GAN <b>301</b> is capable of determining any suitable output e.g. reconstructed pose vector data <b>316</b> and/or a reconstructed depth map <b>326</b> based on an RGB image <b>302</b> input. Each of the encoders, including the image encoder <b>304</b>, the pose encoder <b>312</b>, and the depth encoder <b>322</b> is configured to map an input into a compressed latent representation at the latent space <b>330</b>. Conversely, each of the decoders, including the image decoder <b>306</b>, the pose decoder <b>314</b>, and the depth decoder <b>324</b> are configured to decode the compressed latent representation of the input from the latent space <b>330</b>. The decoders of the VAE-GAN <b>301</b> further include a GAN that is configured to generate an output based on the decoded version of the input.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a schematic block diagram of a process <b>400</b> of determining a depth map of an environment, according to one embodiment. In an embodiment the process <b>400</b> is implemented in a depth decoder <b>324</b> that comprises a GAN configured to generate a reconstructed depth map <b>326</b>. It should be appreciated that a similar process <b>400</b> may be implemented in a pose decoder <b>314</b> that comprises a GAN that is configured to generate reconstructed pose vector data <b>316</b>. The process <b>400</b> includes receiving an RGB image <b>402</b> and feeding the RGB image <b>402</b> to a generative adversarial network (hereinafter “GAN”) generator <b>404</b>. The GAN generator <b>404</b> generates a depth map <b>406</b> based on the RGB image <b>402</b>. A generative adversarial network (“GAN”) discriminator <b>408</b> receives the RGB image <b>402</b> (i.e. the original image) and the depth map <b>406</b> generated by the GAN generator <b>404</b>. The GAN discriminator <b>408</b> is configured to distinguish real and fake image pairs <b>410</b>, e.g. genuine images received from a camera versus depth map images generated by the GAN generator <b>404</b>.</p>
<p id="p-0051" num="0050">In an embodiment, the RGB image <b>402</b> is received from a monocular camera and may be received from the monocular camera in real-time. In an embodiment, the monocular camera is attached to a moving device, such as a vehicle, and each RGB image <b>402</b> is captured when the monocular camera is in a unique position or is in a unique pose. In an embodiment, the monocular camera is attached to an exterior of a vehicle and provides the RGB image <b>402</b> to a vehicle controller, and the vehicle controller is in communication with the GAN generator <b>404</b>.</p>
<p id="p-0052" num="0051">The GAN (i.e. the combination of the GAN generator <b>404</b> and the GAN discriminator <b>408</b>) comprises a deep neural network architecture comprising two adversarial nets in a zero-sum game framework. In an embodiment, the GAN generator <b>404</b> is configured to generate new data instances and the GAN discriminator <b>408</b> is configured to evaluate the new data instances for authenticity. In such an embodiment, the GAN discriminator <b>408</b> is configured to analyze the new data instances and determine whether each new data instance belongs to the actual training data sets or if it was generated artificially (see <b>410</b>). The GAN generator <b>404</b> is configured to create new images that are passed to the GAN discriminator <b>408</b> and the GAN generator <b>404</b> is trained to generate images that fool the GAN discriminator <b>408</b> into determining that an artificial new data instance belongs to the actual training data.</p>
<p id="p-0053" num="0052">In an embodiment, the GAN generator <b>404</b> receives an RGB image <b>402</b> and returns a depth map <b>406</b> based on the RGB image <b>402</b>. The depth map <b>406</b> is fed to the GAN discriminator <b>408</b> alongside a stream of camera images from an actual dataset, and the GAN discriminator <b>408</b> determines a prediction of authenticity for each image, i.e. whether the image is a camera image from the actual dataset or a depth map <b>406</b> generated by the GAN generator <b>404</b>. Thus, in such an embodiment, the GAN includes a double feedback loop wherein the GAN discriminator <b>408</b> is in a feedback loop with the ground truth of the images and the GAN generator <b>404</b> is in a feedback loop with the GAN discriminator <b>408</b>. In an embodiment, the GAN discriminator <b>408</b> is a convolutional neural network configured to categorize images fed to it and the GAN generator <b>404</b> is an inverse convolutional neural network. In an embodiment, both the GAN generator <b>404</b> and the GAN discriminator <b>408</b> are seeking to optimize a different and opposing objective function or loss function. Thus, as the GAN generator <b>404</b> changes its behavior, so does the GAN discriminator <b>408</b>, and vice versa. The losses of the GAN generator <b>404</b> and the GAN discriminator <b>408</b> push against each other to improve the outputs of the GAN.</p>
<p id="p-0054" num="0053">In an embodiment, the GAN generator <b>404</b> is pretrained offline before the GAN generator <b>404</b> receives an RGB image <b>402</b> from a monocular camera. In an embodiment, the GAN discriminator <b>408</b> is pretrained before the GAN generator <b>404</b> is trained and this may provide a clearer gradient. In an embodiment, the GAN generator <b>404</b> is trained using a known dataset as the initial training data for the GAN discriminator <b>408</b>. The GAN generator <b>404</b> may be seeded with a randomized input that is sampled from a predefined latent space, and thereafter, samples synthesized by the GAN generator <b>404</b> are evaluated by the GAN discriminator <b>408</b>.</p>
<p id="p-0055" num="0054">In an embodiment, the GAN generator <b>404</b> circumvents the bottleneck for information commonly found in an encoder-decoder network known in the art. In such an embodiment, the GAN generator <b>404</b> includes skip connections between each layer of the GAN generator <b>404</b>, wherein each skip connection concatenates all channels of the GAN generator <b>404</b>. In an embodiment, the GAN generator <b>404</b> is optimized by alternating between one gradient descent step on the adversarial network then one step on the GAN generator <b>404</b>. At interference time, the generator net is run in the same manner as during the training phase. In an embodiment, instance normalization is applied to the GAN generator <b>404</b>, wherein dropout is applied at test time and batch normalization is applied using statistics of the test batch rather than aggregated statistics of the training batch.</p>
<p id="p-0056" num="0055">In an embodiment, the GAN comprises an encoder-decoder architecture as illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. In such an embodiment, the GAN generator <b>404</b> receives the RGB image <b>402</b> and generates the depth map <b>406</b>. The GAN discriminator <b>408</b> distinguishes between a pair comprising an RGB image <b>402</b> and a depth map <b>406</b>. The GAN generator <b>404</b> and the GAN discriminator <b>408</b> are trained alternatively until the GAN discriminator <b>408</b> cannot tell the difference between an RGB image <b>402</b> and a depth map <b>406</b>. This can encourage the GAN generator <b>404</b> to generate depth maps that are as close to ground truth as possible.</p>
<p id="p-0057" num="0056">The depth map <b>406</b> constitute image-to-image translation that is carried out by the GAN generator <b>404</b> and based on the RGB image <b>402</b>. In generating the depth map <b>406</b>, the GAN generator <b>404</b> learns a mapping from a random noise vector z to determine the depth map <b>406</b> output image. The GAN generator <b>404</b> is trained to produce outputs that cannot be distinguished from real images by an adversarial GAN discriminator <b>408</b>. In an embodiment, an adversarial GAN discriminator <b>408</b> learns to classify between an RGB image <b>402</b> and a depth map <b>406</b>, and the GAN generator <b>404</b> is trained to fool the adversarial GAN discriminator <b>408</b>. In such an embodiment, both the adversarial GAN discriminator <b>408</b> and the GAN generator <b>404</b> observe the depth map <b>406</b> output images.</p>
<p id="p-0058" num="0057">In an embodiment, the input images, i.e. the RGB image <b>402</b> and the output images, i.e. the depth map <b>406</b> differ in surface appearance but both include a rendering of the same underlying structure. Thus, structure in the RGB image <b>402</b> is roughly aligned with structure in the depth map <b>406</b>. In an embodiment, the GAN generator <b>404</b> architecture is designed around this consideration.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a schematic flow chart diagram of a method <b>500</b> for localizing a vehicle in an environment and mapping the environment of the vehicle. The method <b>500</b> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system <b>102</b>. The method <b>500</b> begins and the computing device receives an image from a camera of a vehicle at <b>502</b>. The computing device provides the image to a variational autoencoder generative adversarial network (VAE-GAN) at <b>504</b>. The computing device receives from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image at <b>506</b>. The computing device calculates simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map at <b>508</b>. The VAE-GAN is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs (see <b>510</b>).</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a schematic flow chart diagram of a method <b>600</b> for localizing a vehicle in an environment and mapping the environment of the vehicle. The method <b>100</b> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system <b>102</b>. The method <b>600</b> begins and the computing device receives an image from a camera of a vehicle at <b>602</b>. The computing devices provides the image to a variational autoencoder generative adversarial network (VAE-GAN) at <b>604</b>. The VAE-GAN is such that the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of an image encoder, an image decoder, a pose encoder, a pose decoder, a depth encoder, and a depth decoder are trained utilizing a single latent space of the VAE-GAN (see <b>606</b>). The VAE-GAN is such that the VEA-GAN comprises a trained image encoder configured to receive the image, a trained pose decoder comprising a GAN configured to generate reconstructed pose vector data based on the image, and a trained depth decoder comprising a GAN configured to generate a reconstructed depth map based on the image (see <b>608</b>). The computing device receives from the VAE-GAN the reconstructed pose vector data based on the image at <b>610</b>. The computing device receives from the VAE-GAN the reconstructed depth map based on the image at <b>612</b>. The computing device calculates simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map at <b>614</b>.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a schematic flow chart diagram of a method <b>700</b> for training a VAE-GAN. The method <b>700</b> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system <b>102</b>. The method <b>700</b> begins and the computing device provides a training image to an image encoder of a variational autoencoder generative adversarial network (VAE-GAN) at <b>702</b>. The computing device provides training pose vector data based on the training image to a pose encoder of the VAE-GAN at <b>704</b>. The computing devices provides a training depth map based on the training image to a depth encoder of the VAE-GAN at <b>706</b>. The VAE-GAN is such that the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of the image encoder, the pose encoder, and the depth encoder are trained in tandem utilizing a latent space of the VAE-GAN (see <b>708</b>). The VAE-GAN is such that the VAE-GAN comprises an encoded latent space vector applicable to each of the training image, the training pose vector data, and the training depth map (see <b>710</b>).</p>
<p id="p-0062" num="0061">Referring now to <figref idref="DRAWINGS">FIG. 8</figref>, a block diagram of an example computing device <b>800</b> is illustrated. Computing device <b>800</b> may be used to perform various procedures, such as those discussed herein. In one embodiment, the computing device <b>800</b> can function as a neural network such as a GAN generator <b>404</b>, a vehicle controller such as an autonomous driving/assistance system <b>102</b>, a VAE-GAN <b>201</b>, a server, and the like. Computing device <b>800</b> can perform various monitoring functions as discussed herein, and can execute one or more application programs, such as the application programs or functionality described herein. Computing device <b>800</b> can be any of a wide variety of computing devices, such as a desktop computer, in-dash computer, vehicle control system, a notebook computer, a server computer, a handheld computer, tablet computer and the like.</p>
<p id="p-0063" num="0062">Computing device <b>800</b> includes one or more processor(s) <b>802</b>, one or more memory device(s) <b>804</b>, one or more interface(s) <b>806</b>, one or more mass storage device(s) <b>808</b>, one or more Input/output (I/O) device(s) <b>810</b>, and a display device <b>830</b> all of which are coupled to a bus <b>812</b>. Processor(s) <b>802</b> include one or more processors or controllers that execute instructions stored in memory device(s) <b>804</b> and/or mass storage device(s) <b>808</b>. Processor(s) <b>802</b> may also include various types of computer-readable media, such as cache memory.</p>
<p id="p-0064" num="0063">Memory device(s) <b>804</b> include various computer-readable media, such as volatile memory (e.g., random access memory (RAM) <b>814</b>) and/or nonvolatile memory (e.g., read-only memory (ROM) <b>816</b>). Memory device(s) <b>804</b> may also include rewritable ROM, such as Flash memory.</p>
<p id="p-0065" num="0064">Mass storage device(s) <b>808</b> include various computer readable media, such as magnetic tapes, magnetic disks, optical disks, solid-state memory (e.g., Flash memory), and so forth. As shown in <figref idref="DRAWINGS">FIG. 8</figref>, a particular mass storage device is a hard disk drive <b>824</b>. Various drives may also be included in mass storage device(s) <b>808</b> to enable reading from and/or writing to the various computer readable media. Mass storage device(s) <b>808</b> include removable media <b>826</b> and/or non-removable media.</p>
<p id="p-0066" num="0065">I/O device(s) <b>810</b> include various devices that allow data and/or other information to be input to or retrieved from computing device <b>800</b>. Example I/O device(s) <b>810</b> include cursor control devices, keyboards, keypads, microphones, monitors or other display devices, speakers, printers, network interface cards, modems, and the like.</p>
<p id="p-0067" num="0066">Display device <b>830</b> includes any type of device capable of displaying information to one or more users of computing device <b>800</b>. Examples of display device <b>830</b> include a monitor, display terminal, video projection device, and the like.</p>
<p id="p-0068" num="0067">Interface(s) <b>806</b> include various interfaces that allow computing device <b>800</b> to interact with other systems, devices, or computing environments. Example interface(s) <b>806</b> may include any number of different network interfaces <b>820</b>, such as interfaces to local area networks (LANs), wide area networks (WANs), wireless networks, and the Internet. Other interface(s) include user interface <b>818</b> and peripheral device interface <b>822</b>. The interface(s) <b>806</b> may also include one or more user interface elements <b>818</b>. The interface(s) <b>806</b> may also include one or more peripheral interfaces such as interfaces for printers, pointing devices (mice, track pad, or any suitable user interface now known to those of ordinary skill in the field, or later discovered), keyboards, and the like.</p>
<p id="p-0069" num="0068">Bus <b>812</b> allows processor(s) <b>802</b>, memory device(s) <b>804</b>, interface(s) <b>806</b>, mass storage device(s) <b>808</b>, and I/O device(s) <b>810</b> to communicate with one another, as well as other devices or components coupled to bus <b>812</b>. Bus <b>812</b> represents one or more of several types of bus structures, such as a system bus, PCI bus, IEEE bus, USB bus, and so forth.</p>
<p id="p-0070" num="0069">For purposes of illustration, programs and other executable program components are shown herein as discrete blocks, although it is understood that such programs and components may reside at various times in different storage components of computing device <b>800</b> and are executed by processor(s) <b>802</b>. Alternatively, the systems and procedures described herein can be implemented in hardware, or a combination of hardware, software, and/or firmware. For example, one or more application specific integrated circuits (ASICs) can be programmed to carry out one or more of the systems and procedures described herein.</p>
<heading id="h-0005" level="1">EXAMPLES</heading>
<p id="p-0071" num="0070">The following examples pertain to further embodiments.</p>
<p id="p-0072" num="0071">Example 1 is a method for simultaneous localization and mapping of a robot in an environment. The method includes: receiving an image from a camera of a vehicle; providing the image to a variational autoencoder generative adversarial network (VAE-GAN); receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</p>
<p id="p-0073" num="0072">Example 2 is a method as in Example 1, further comprising training the VAE-GAN, wherein training the VAE-GAN comprises: providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation; providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to the compressed latent representation; and providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to the compressed latent representation.</p>
<p id="p-0074" num="0073">Example 3 is a method as in any of Examples 1-2, wherein the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of: the image encoder and the image decoder; the pose encoder and the pose decoder; and the depth encoder and the depth decoder are trained in tandem utilizing the latent space of the VAE-GAN.</p>
<p id="p-0075" num="0074">Example 4 is a method as in any of Examples 1-3, wherein each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</p>
<p id="p-0076" num="0075">Example 5 is a method as in any of Examples 1-4, wherein the VAE-GAN comprises an encoded latent space vector that is applicable to each of the training image, the training pose vector data, and the training depth map.</p>
<p id="p-0077" num="0076">Example 6 is a method as in any of Examples 1-5, further comprising determining the training pose vector data based on the training image, wherein determining the training pose vector data comprises: receiving a plurality of stereo images forming a stereo image sequence; and calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry; wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</p>
<p id="p-0078" num="0077">Example 7 is a method as in any of Examples 1-6, wherein the camera of the vehicle comprises a monocular camera configured to capture a sequence of images of an environment of the vehicle, and wherein the image comprises a red-green-blue (RGB) image.</p>
<p id="p-0079" num="0078">Example 8 is a method as in any of Examples 1-7, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</p>
<p id="p-0080" num="0079">Example 9 is a method as in any of Examples 1-8, wherein the VAE-GAN comprises: a trained image encoder configured to receive the image; a trained pose decoder comprising a GAN configured to generate the reconstructed pose vector data based on the image; and a trained depth decoder comprising a GAN configured to generate the reconstructed depth map based on the image.</p>
<p id="p-0081" num="0080">Example 10 is a method as in any of Examples 1-9, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</p>
<p id="p-0082" num="0081">Example 11 is a method as in any of Examples 1-10, wherein the latent space of the VAE-GAN comprises an encoded latent space vector utilized for each of the image encoder, the pose decoder, and the depth decoder.</p>
<p id="p-0083" num="0082">Example 12 is a method as in any of Examples 1-11, wherein the reconstructed pose vector data comprises six Degree of Freedom pose data pertaining to the camera of the vehicle.</p>
<p id="p-0084" num="0083">Example 13 is non-transitory computer-readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to: receive an image from a camera of a vehicle; provide the image to a variational autoencoder generative adversarial network (VAE-GAN); receive from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and calculate simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</p>
<p id="p-0085" num="0084">Example 14 is non-transitory computer-readable storage media as in Example 13, wherein the instructions further cause the one or more processors to train the VAE-GAN, wherein training the VAE-GAN comprises: providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation; providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to the compressed latent representation; and providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to the compressed latent representation.</p>
<p id="p-0086" num="0085">Example 15 is non-transitory computer-readable storage media as in any of Examples 13-14, wherein the instructions cause the one or more processors to train the VAE-GAN utilizing a plurality of inputs in tandem, such that each of: the image encoder and the image decoder; the pose encoder and the pose decoder; and the depth encoder and the depth decoder are trained in tandem such that each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</p>
<p id="p-0087" num="0086">Example 16 is non-transitory computer-readable storage media as in any of Examples 13-15, the instructions further cause the one or more processors to calculate the training pose vector data based on the training image, wherein calculating the training pose vector data comprises: receiving a plurality of stereo images forming a stereo image sequence; and calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry; wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</p>
<p id="p-0088" num="0087">Example 17 is non-transitory computer-readable storage media as in any of Examples 13-16, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</p>
<p id="p-0089" num="0088">Example 18 is a system for simultaneous localization and mapping of a vehicle in an environment, the system comprising: a monocular camera of a vehicle; a vehicle controller in communication with the monocular camera, wherein the vehicle controller comprises non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to: receive an image from the monocular camera of the vehicle; provide the image to a variational autoencoder generative adversarial network (VAE-GAN); receive from the VAE-GAN reconstructed pose vector data based on the image; receive from the VAE-GAN a reconstructed depth map based on the image; and calculate simultaneous localization and mapping for the vehicle based on one or more of the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</p>
<p id="p-0090" num="0089">Example 19 is a system as in Example 18, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</p>
<p id="p-0091" num="0090">Example 20 is a system as in any of Examples 18-19, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</p>
<p id="p-0092" num="0091">Example 21 is a system or device that includes means for implementing a method, system, or device as in any of Examples 1-20.</p>
<p id="p-0093" num="0092">In the above disclosure, reference has been made to the accompanying drawings, which form a part hereof, and in which is shown by way of illustration specific implementations in which the disclosure may be practiced. It is understood that other implementations may be utilized, and structural changes may be made without departing from the scope of the present disclosure. References in the specification to “one embodiment,” “an embodiment,” “an example embodiment,” etc., indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to affect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.</p>
<p id="p-0094" num="0093">Implementations of the systems, devices, and methods disclosed herein may comprise or utilize a special purpose or general-purpose computer including computer hardware, such as, for example, one or more processors and system memory, as discussed herein. Implementations within the scope of the present disclosure may also include physical and other computer-readable media for carrying or storing computer-executable instructions and/or data structures. Such computer-readable media can be any available media that can be accessed by a general purpose or special purpose computer system. Computer-readable media that store computer-executable instructions are computer storage media (devices). Computer-readable media that carry computer-executable instructions are transmission media. Thus, by way of example, and not limitation, implementations of the disclosure can comprise at least two distinctly different kinds of computer-readable media: computer storage media (devices) and transmission media.</p>
<p id="p-0095" num="0094">Computer storage media (devices) includes RAM, ROM, EEPROM, CD-ROM, solid state drives (“SSDs”) (e.g., based on RAM), Flash memory, phase-change memory (“PCM”), other types of memory, other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium, which can be used to store desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer.</p>
<p id="p-0096" num="0095">An implementation of the devices, systems, and methods disclosed herein may communicate over a computer network. A “network” is defined as one or more data links that enable the transport of electronic data between computer systems and/or modules and/or other electronic devices. When information is transferred or provided over a network or another communications connection (either hardwired, wireless, or a combination of hardwired or wireless) to a computer, the computer properly views the connection as a transmission medium. Transmissions media can include a network and/or data links, which can be used to carry desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer. Combinations of the above should also be included within the scope of computer-readable media.</p>
<p id="p-0097" num="0096">Computer-executable instructions comprise, for example, instructions and data which, when executed at a processor, cause a general-purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions. The computer executable instructions may be, for example, binaries, intermediate format instructions such as assembly language, or even source code. Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the described features or acts described above. Rather, the described features and acts are disclosed as example forms of implementing the claims.</p>
<p id="p-0098" num="0097">Those skilled in the art will appreciate that the disclosure may be practiced in network computing environments with many types of computer system configurations, including, an in-dash vehicle computer, personal computers, desktop computers, laptop computers, message processors, hand-held devices, multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, mobile telephones, PDAs, tablets, pagers, routers, switches, various storage devices, and the like. The disclosure may also be practiced in distributed system environments where local and remote computer systems, which are linked (either by hardwired data links, wireless data links, or by a combination of hardwired and wireless data links) through a network, both perform tasks. In a distributed system environment, program modules may be located in both local and remote memory storage devices.</p>
<p id="p-0099" num="0098">Further, where appropriate, functions described herein can be performed in one or more of: hardware, software, firmware, digital components, or analog components. For example, one or more application specific integrated circuits (ASICs) can be programmed to carry out one or more of the systems and procedures described herein. Certain terms are used throughout the description and claims to refer to particular system components. The terms “modules” and “components” are used in the names of certain components to reflect their implementation independence in software, hardware, circuitry, sensors, or the like. As one skilled in the art will appreciate, components may be referred to by different names. This document does not intend to distinguish between components that differ in name, but not function.</p>
<p id="p-0100" num="0099">It should be noted that the sensor embodiments discussed above may comprise computer hardware, software, firmware, or any combination thereof to perform at least a portion of their functions. For example, a sensor may include computer code configured to be executed in one or more processors and may include hardware logic/electrical circuitry controlled by the computer code. These example devices are provided herein purposes of illustration and are not intended to be limiting. Embodiments of the present disclosure may be implemented in further types of devices, as would be known to persons skilled in the relevant art(s).</p>
<p id="p-0101" num="0100">At least some embodiments of the disclosure have been directed to computer program products comprising such logic (e.g., in the form of software) stored on any computer useable medium. Such software, when executed in one or more data processing devices, causes a device to operate as described herein.</p>
<p id="p-0102" num="0101">While various embodiments of the present disclosure have been described above, it should be understood that they have been presented by way of example only, and not limitation. It will be apparent to persons skilled in the relevant art that various changes in form and detail can be made therein without departing from the spirit and scope of the disclosure. Thus, the breadth and scope of the present disclosure should not be limited by any of the above-described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents. The foregoing description has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the disclosure to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. Further, it should be noted that any or all of the aforementioned alternate implementations may be used in any combination desired to form additional hybrid implementations of the disclosure.</p>
<p id="p-0103" num="0102">Further, although specific implementations of the disclosure have been described and illustrated, the disclosure is not to be limited to the specific forms or arrangements of parts so described and illustrated. The scope of the disclosure is to be defined by the claims appended hereto, any future claims submitted here and in different applications, and their equivalents.</p>
<?detailed-description description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text><b>1</b>. A method comprising:
<claim-text>receiving an image from a camera of a vehicle;</claim-text>
<claim-text>providing the image to a variational autoencoder generative adversarial network (VAE-GAN);</claim-text>
<claim-text>receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and</claim-text>
<claim-text>calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map;</claim-text>
<claim-text>wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising training the VAE-GAN, wherein training the VAE-GAN comprises:
<claim-text>providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation of the training image;</claim-text>
<claim-text>providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation of the training pose vector data; and</claim-text>
<claim-text>providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation of the training depth map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of:
<claim-text>the image encoder and a corresponding image decoder;</claim-text>
<claim-text>the pose encoder and a corresponding pose decoder; and</claim-text>
<claim-text>the depth encoder and a corresponding depth decoder are trained in tandem utilizing the latent space of the VAE-GAN.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text><b>5</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the VAE-GAN comprises an encoded latent space vector that is applicable to each of the training image, the training pose vector data, and the training depth map.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text><b>6</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising determining the training pose vector data based on the training image, wherein determining the training pose vector data comprises:
<claim-text>receiving a plurality of stereo images forming a stereo image sequence; and</claim-text>
<claim-text>calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry;</claim-text>
<claim-text>wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the camera of the vehicle comprises a monocular camera configured to capture a sequence of images of an environment of the vehicle, and wherein the image comprises a red-green-blue (RGB) image.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VAE-GAN comprises:
<claim-text>a trained image encoder configured to receive the image;</claim-text>
<claim-text>a trained pose decoder comprising a GAN configured to generate the reconstructed pose vector data based on the image; and</claim-text>
<claim-text>a trained depth decoder comprising a GAN configured to generate the reconstructed depth map based on the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VAE-GAN comprises:
<claim-text>an image encoder configured to map the image to a compressed latent representation;</claim-text>
<claim-text>a pose decoder comprising a GAN generator adversarial to a GAN discriminator;</claim-text>
<claim-text>a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and</claim-text>
<claim-text>a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the latent space of the VAE-GAN comprises an encoded latent space vector utilized for each of the image encoder, the pose decoder, and the depth decoder.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the reconstructed pose vector data comprises six Degree of Freedom pose data pertaining to the camera of the vehicle.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text><b>13</b>. Non-transitory computer-readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to:
<claim-text>receive an image from a camera of a vehicle;</claim-text>
<claim-text>provide the image to a variational autoencoder generative adversarial network (VAE-GAN);</claim-text>
<claim-text>receive from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and</claim-text>
<claim-text>calculate simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map;</claim-text>
<claim-text>wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text><b>14</b>. The non-transitory computer-readable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions further cause the one or more processors to train the VAE-GAN, wherein training the VAE-GAN comprises:
<claim-text>providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation in the latent space;</claim-text>
<claim-text>providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation in the latent space; and</claim-text>
<claim-text>providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation in the latent space.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text><b>15</b>. The non-transitory computer-readable storage media of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the instructions cause the one or more processors to train the VAE-GAN utilizing a plurality of inputs in tandem, such that each of:
<claim-text>the image encoder and a corresponding image decoder;</claim-text>
<claim-text>the pose encoder and a corresponding pose decoder; and</claim-text>
<claim-text>the depth encoder and a corresponding depth decoder are trained in tandem such that each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text><b>16</b>. The non-transitory computer-readable storage media of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the instructions further cause the one or more processors to calculate the training pose vector data based on the training image, wherein calculating the training pose vector data comprises:
<claim-text>receiving a plurality of stereo images forming a stereo image sequence; and</claim-text>
<claim-text>calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry;</claim-text>
<claim-text>wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text><b>17</b>. The non-transitory computer-readable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text><b>18</b>. A system for simultaneous localization and mapping of a vehicle in an environment, the system comprising:
<claim-text>a monocular camera of a vehicle;</claim-text>
<claim-text>a vehicle controller in communication with the monocular camera, wherein the vehicle controller comprises non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to:
<claim-text>receive an image from the monocular camera of the vehicle;</claim-text>
<claim-text>provide the image to a variational autoencoder generative adversarial network (VAE-GAN);</claim-text>
<claim-text>receive from the VAE-GAN reconstructed pose vector data based on the image;</claim-text>
<claim-text>receive from the VAE-GAN a reconstructed depth map based on the image; and</claim-text>
<claim-text>calculate simultaneous localization and mapping for the vehicle based on one or more of the reconstructed pose vector data and the reconstructed depth map;</claim-text>
</claim-text>
<claim-text>wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text><b>19</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the VAE-GAN is trained and training the VAE-GAN comprises:
<claim-text>providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation of the training image;</claim-text>
<claim-text>providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation of the training pose vector data; and</claim-text>
<claim-text>providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation of the training depth map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text><b>20</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the VAE-GAN comprises:
<claim-text>an image encoder configured to map the image to a compressed latent representation;</claim-text>
<claim-text>a pose decoder comprising a GAN generator adversarial to a GAN discriminator;</claim-text>
<claim-text>a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and</claim-text>
<claim-text>a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-application></div><div class="header"><span>This XML file does not appear to have any style information associated with it. The document tree is shown below.</span><br /></div><div class="pretty-print"><div class="folder" id="folder0"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;us-patent-application<span class="html-attribute"> <span class="html-attribute-name">lang</span>="<span class="html-attribute-value">EN</span>"</span><span class="html-attribute"> <span class="html-attribute-name">dtd-version</span>="<span class="html-attribute-value">v4.4 2014-04-03</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206.XML</span>"</span><span class="html-attribute"> <span class="html-attribute-name">status</span>="<span class="html-attribute-value">PRODUCTION</span>"</span><span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">us-patent-application</span>"</span><span class="html-attribute"> <span class="html-attribute-name">country</span>="<span class="html-attribute-value">US</span>"</span><span class="html-attribute"> <span class="html-attribute-name">date-produced</span>="<span class="html-attribute-value">20200122</span>"</span><span class="html-attribute"> <span class="html-attribute-name">date-publ</span>="<span class="html-attribute-value">20200206</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder1"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;us-bibliographic-data-application<span class="html-attribute"> <span class="html-attribute-name">lang</span>="<span class="html-attribute-value">EN</span>"</span><span class="html-attribute"> <span class="html-attribute-name">country</span>="<span class="html-attribute-value">US</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder2"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;publication-reference&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder3"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;document-id&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;doc-number&gt;</span><span>20200041276</span><span class="html-tag">&lt;/doc-number&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;kind&gt;</span><span>A1</span><span class="html-tag">&lt;/kind&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/document-id&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/publication-reference&gt;</span></div></div><span>
</span><div class="folder" id="folder4"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;application-reference<span class="html-attribute"> <span class="html-attribute-name">appl-type</span>="<span class="html-attribute-value">utility</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder5"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;document-id&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;doc-number&gt;</span><span>16054694</span><span class="html-tag">&lt;/doc-number&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20180803</span><span class="html-tag">&lt;/date&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/document-id&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/application-reference&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;us-application-series-code&gt;</span><span>16</span><span class="html-tag">&lt;/us-application-series-code&gt;</span></div><span>
</span><div class="folder" id="folder6"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classifications-ipcr&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder7"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-ipcr&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder8"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;ipc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20060101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/ipc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-level&gt;</span><span>A</span><span class="html-tag">&lt;/classification-level&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>01</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>C</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>21</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>32</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>F</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder9"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder10"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-ipcr&gt;</span></div></div><span>
</span><div class="folder" id="folder11"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-ipcr&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder12"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;ipc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20060101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/ipc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-level&gt;</span><span>A</span><span class="html-tag">&lt;/classification-level&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>F</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>17</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>30</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder13"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder14"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-ipcr&gt;</span></div></div><span>
</span><div class="folder" id="folder15"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-ipcr&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder16"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;ipc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20060101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/ipc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-level&gt;</span><span>A</span><span class="html-tag">&lt;/classification-level&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>N</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>3</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>08</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder17"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder18"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-ipcr&gt;</span></div></div><span>
</span><div class="folder" id="folder19"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-ipcr&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder20"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;ipc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20060101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/ipc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-level&gt;</span><span>A</span><span class="html-tag">&lt;/classification-level&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>N</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>3</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>04</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder21"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder22"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-ipcr&gt;</span></div></div><span>
</span><div class="folder" id="folder23"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-ipcr&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder24"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;ipc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20060101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/ipc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-level&gt;</span><span>A</span><span class="html-tag">&lt;/classification-level&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>T</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>7</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>73</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder25"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder26"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-ipcr&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classifications-ipcr&gt;</span></div></div><span>
</span><div class="folder" id="folder27"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classifications-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder28"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;main-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder29"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder30"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20130101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>01</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>C</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>21</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>32</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>F</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder31"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder32"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/main-cpc&gt;</span></div></div><span>
</span><div class="folder" id="folder33"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;further-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder34"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder35"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20130101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>F</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>17</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>30241</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder36"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder37"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span><div class="folder" id="folder38"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder39"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20130101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>N</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>3</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>088</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder40"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder41"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span><div class="folder" id="folder42"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder43"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20130101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>N</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>3</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>0454</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder44"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder45"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span><div class="folder" id="folder46"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder47"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20130101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>T</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>2207</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>20084</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>A</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder48"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder49"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span><div class="folder" id="folder50"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder51"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20130101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>T</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>2207</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>10028</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>A</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder52"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder53"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span><div class="folder" id="folder54"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder55"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20130101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>T</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>2207</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>30252</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>A</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder56"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder57"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span><div class="folder" id="folder58"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder59"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20130101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>T</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>2207</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>20081</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>A</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder60"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder61"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span><div class="folder" id="folder62"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;classification-cpc&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder63"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;cpc-version-indicator&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20170101</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/cpc-version-indicator&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;section&gt;</span><span>G</span><span class="html-tag">&lt;/section&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;class&gt;</span><span>06</span><span class="html-tag">&lt;/class&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subclass&gt;</span><span>T</span><span class="html-tag">&lt;/subclass&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;main-group&gt;</span><span>7</span><span class="html-tag">&lt;/main-group&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;subgroup&gt;</span><span>74</span><span class="html-tag">&lt;/subgroup&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;symbol-position&gt;</span><span>L</span><span class="html-tag">&lt;/symbol-position&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-value&gt;</span><span>I</span><span class="html-tag">&lt;/classification-value&gt;</span></div><span>
</span><div class="folder" id="folder64"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;action-date&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;date&gt;</span><span>20200206</span><span class="html-tag">&lt;/date&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/action-date&gt;</span></div></div><span>
</span><div class="folder" id="folder65"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;generating-office&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/generating-office&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-status&gt;</span><span>B</span><span class="html-tag">&lt;/classification-status&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;classification-data-source&gt;</span><span>H</span><span class="html-tag">&lt;/classification-data-source&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;scheme-origination-code&gt;</span><span>C</span><span class="html-tag">&lt;/scheme-origination-code&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classification-cpc&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/further-cpc&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/classifications-cpc&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;invention-title<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">d2e43</span>"</span>&gt;</span><span>End-To-End Deep Generative Model For Simultaneous Localization And Mapping</span><span class="html-tag">&lt;/invention-title&gt;</span></div><span>
</span><div class="folder" id="folder66"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;us-parties&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder67"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;us-applicants&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder68"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;us-applicant<span class="html-attribute"> <span class="html-attribute-name">sequence</span>="<span class="html-attribute-value">00</span>"</span><span class="html-attribute"> <span class="html-attribute-name">app-type</span>="<span class="html-attribute-value">applicant</span>"</span><span class="html-attribute"> <span class="html-attribute-name">designation</span>="<span class="html-attribute-value">us-only</span>"</span><span class="html-attribute"> <span class="html-attribute-name">applicant-authority-category</span>="<span class="html-attribute-value">assignee</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder69"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;addressbook&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;orgname&gt;</span><span>Ford Global Technologies, LLC</span><span class="html-tag">&lt;/orgname&gt;</span></div><span>
</span><div class="folder" id="folder70"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;address&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;city&gt;</span><span>Dearborn</span><span class="html-tag">&lt;/city&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;state&gt;</span><span>MI</span><span class="html-tag">&lt;/state&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/address&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/addressbook&gt;</span></div></div><span>
</span><div class="folder" id="folder71"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;residence&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/residence&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/us-applicant&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/us-applicants&gt;</span></div></div><span>
</span><div class="folder" id="folder72"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;inventors&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder73"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;inventor<span class="html-attribute"> <span class="html-attribute-name">sequence</span>="<span class="html-attribute-value">00</span>"</span><span class="html-attribute"> <span class="html-attribute-name">designation</span>="<span class="html-attribute-value">us-only</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder74"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;addressbook&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;last-name&gt;</span><span>Chakravarty</span><span class="html-tag">&lt;/last-name&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;first-name&gt;</span><span>Punarjay</span><span class="html-tag">&lt;/first-name&gt;</span></div><span>
</span><div class="folder" id="folder75"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;address&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;city&gt;</span><span>Mountain View</span><span class="html-tag">&lt;/city&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;state&gt;</span><span>CA</span><span class="html-tag">&lt;/state&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/address&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/addressbook&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/inventor&gt;</span></div></div><span>
</span><div class="folder" id="folder76"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;inventor<span class="html-attribute"> <span class="html-attribute-name">sequence</span>="<span class="html-attribute-value">01</span>"</span><span class="html-attribute"> <span class="html-attribute-name">designation</span>="<span class="html-attribute-value">us-only</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder77"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;addressbook&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;last-name&gt;</span><span>Narayanan</span><span class="html-tag">&lt;/last-name&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;first-name&gt;</span><span>Praveen</span><span class="html-tag">&lt;/first-name&gt;</span></div><span>
</span><div class="folder" id="folder78"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;address&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;city&gt;</span><span>San Jose</span><span class="html-tag">&lt;/city&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;state&gt;</span><span>CA</span><span class="html-tag">&lt;/state&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;country&gt;</span><span>US</span><span class="html-tag">&lt;/country&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/address&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/addressbook&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/inventor&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/inventors&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/us-parties&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/us-bibliographic-data-application&gt;</span></div></div><span>
</span><div class="folder" id="folder79"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;abstract<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">abstract</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0001</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0000</span>"</span>&gt;</span><span>The disclosure relates to systems, methods, and devices for simultaneous localization and mapping of a robot in an environment utilizing a variational autoencoder generative adversarial network (VAE-GAN). A method includes receiving an image from a camera of a vehicle and providing the image to a VAE-GAN. The method includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image. The method includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map. The method is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/abstract&gt;</span></div></div><span>
</span><div class="folder" id="folder80"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;drawings<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder81"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00000</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00000</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00000</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">111.84mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">158.75mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00000.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span><div class="folder" id="folder82"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00001</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00001</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00001</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">183.90mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">136.74mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00001.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span><div class="folder" id="folder83"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00002</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00002</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00002</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">226.23mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">188.47mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">orientation</span>="<span class="html-attribute-value">landscape</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00002.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span><div class="folder" id="folder84"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00003</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00003</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00003</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">234.70mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">177.55mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">orientation</span>="<span class="html-attribute-value">landscape</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00003.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span><div class="folder" id="folder85"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00004</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00004</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00004</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">216.75mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">180.93mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00004.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span><div class="folder" id="folder86"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00005</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00005</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00005</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">172.13mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">172.38mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00005.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span><div class="folder" id="folder87"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00006</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00006</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00006</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">231.73mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">175.43mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00006.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span><div class="folder" id="folder88"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00007</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00007</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00007</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">186.86mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">175.43mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00007.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span><div class="folder" id="folder89"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;figure<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">Fig-EMI-D00008</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00008</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;img<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">EMI-D00008</span>"</span><span class="html-attribute"> <span class="html-attribute-name">he</span>="<span class="html-attribute-value">225.30mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">wi</span>="<span class="html-attribute-value">123.95mm</span>"</span><span class="html-attribute"> <span class="html-attribute-name">file</span>="<span class="html-attribute-value">US20200041276A1-20200206-D00008.TIF</span>"</span><span class="html-attribute"> <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">embedded image</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-content</span>="<span class="html-attribute-value">drawing</span>"</span><span class="html-attribute"> <span class="html-attribute-name">img-format</span>="<span class="html-attribute-value">tif</span>"</span>/&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/figure&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/drawings&gt;</span></div></div><span>
</span><div class="folder" id="folder90"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;description<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">description</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="comment html-comment">&lt;?summary-of-invention description="Summary of Invention" end="lead"?&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;heading<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">h-0001</span>"</span><span class="html-attribute"> <span class="html-attribute-name">level</span>="<span class="html-attribute-value">1</span>"</span>&gt;</span><span>TECHNICAL FIELD</span><span class="html-tag">&lt;/heading&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0002</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0001</span>"</span>&gt;</span><span>The present disclosure relates to methods, systems, and apparatuses for simultaneous localization and mapping of an apparatus in an environment, and particularly relates to simultaneous localization and mapping of a vehicle using a variational autoencoder generative adversarial network.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;heading<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">h-0002</span>"</span><span class="html-attribute"> <span class="html-attribute-name">level</span>="<span class="html-attribute-value">1</span>"</span>&gt;</span><span>BACKGROUND</span><span class="html-tag">&lt;/heading&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0003</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0002</span>"</span>&gt;</span><span>Localization, mapping, and depth perception in real-time are requirements for certain autonomous systems, including autonomous driving systems or mobile robotics systems. Each of localization, mapping, and depth perception are key components for carrying out certain tasks such as obstacle avoidance, route planning, mapping, localization, pedestrian detection, and human-robot interaction. Depth perception and localization are traditionally performed by expensive active sensing systems such as LIDAR sensors or passive sensing systems such as binocular vision or stereo cameras.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0004</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0003</span>"</span>&gt;</span><span>Systems, methods, and devices for computing localization, mapping, and depth perception can be integrated in automobiles such as autonomous vehicles and driving assistance systems. Such systems are currently being developed and deployed to provide safety features, reduce an amount of user input required, or even eliminate user involvement entirely. For example, some driving assistance systems, such as crash avoidance systems, may monitor driving, positions, and a velocity of the vehicle and other objects while a human is driving. When the system detects that a crash or impact is imminent the crash avoidance system may intervene and apply a brake, steer the vehicle, or perform other avoidance or safety maneuvers. As another example, autonomous vehicles may drive, navigate, and/or park a vehicle with little or no user input. However, due to the dangers involved in driving and the costs of vehicles, it is extremely important that autonomous vehicles and driving assistance systems operate safely and are able to accurately navigate roads in a variety of different driving environments.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="comment html-comment">&lt;?summary-of-invention description="Summary of Invention" end="tail"?&gt;</span></div><span>
</span><div class="line"><span class="comment html-comment">&lt;?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?&gt;</span></div><span>
</span><div class="folder" id="folder91"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;description-of-drawings&gt;</span></div><div class="opened"><span>
</span><div class="line"><span class="html-tag">&lt;heading<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">h-0003</span>"</span><span class="html-attribute"> <span class="html-attribute-name">level</span>="<span class="html-attribute-value">1</span>"</span>&gt;</span><span>BRIEF DESCRIPTION OF THE DRAWINGS</span><span class="html-tag">&lt;/heading&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0005</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0004</span>"</span>&gt;</span><span>Non-limiting and non-exhaustive implementations of the present disclosure are described with reference to the following figures, wherein like reference numerals refer to like parts throughout the various views unless otherwise specified. Advantages of the present disclosure will become better understood with regard to the following description and accompanying drawings where:</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="folder" id="folder92"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0006</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0005</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 1</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is a schematic block diagram illustrating an example vehicle control system or autonomous vehicle system, according to one embodiment;</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder93"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0007</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0006</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 2</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is a schematic block diagram of a variational autoencoder generative adversarial network in a training phase, according to one embodiment;</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder94"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0008</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0007</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 3</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is a schematic block diagram of a variational autoencoder generative adversarial network in a computation phase, according to one embodiment;</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder95"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0009</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0008</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 4</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is a schematic block diagram illustrating a process for determining a depth map of an environment, according to one embodiment;</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder96"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0010</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0009</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 5</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is a schematic flow chart diagram of a method for utilizing simultaneous localization and mapping of a vehicle in an environment, according to one embodiment;</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder97"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0011</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0010</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 6</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is a schematic flow chart diagram of a method for utilizing simultaneous localization and mapping of a vehicle in an environment, according to one embodiment;</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder98"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0012</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0011</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 7</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is a schematic flow chart diagram of a method for training a variational autoencoder generative adversarial network, according to one embodiment; and</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder99"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0013</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0012</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 8</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is a schematic block diagram illustrating an example computing system, according to one embodiment.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/description-of-drawings&gt;</span></div></div><span>
</span><div class="line"><span class="comment html-comment">&lt;?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?&gt;</span></div><span>
</span><div class="line"><span class="comment html-comment">&lt;?detailed-description description="Detailed Description" end="lead"?&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;heading<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">h-0004</span>"</span><span class="html-attribute"> <span class="html-attribute-name">level</span>="<span class="html-attribute-value">1</span>"</span>&gt;</span><span>DETAILED DESCRIPTION</span><span class="html-tag">&lt;/heading&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0014</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0013</span>"</span>&gt;</span><span>Localization of a vehicle along with mapping and depth perception of drivable surfaces or regions is an important aspect of allowing for and improving operation of autonomous vehicle or driver assistance features. For example, a vehicle must know precisely where obstacles or drivable surfaces are located to navigate safely around objects.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0015</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0014</span>"</span>&gt;</span><span>Simultaneous Localization and Mapping (SLAM) forms the basis for operational functionality of mobile robots, including autonomous vehicles and other mobile robots. Examples of such robots include an indoor mobile robot configured for delivering items in a warehouse or an autonomous drone configured for traversing a building or other environment in a disaster scenario. SLAM is directed to sensing the robot's environment and building a map of its surroundings as the robot moves through its environment. SLAM is further directed to simultaneously localizing the robot within its environment by extracting pose vector data, including six Degree of Freedom (DoF) poses relative to a starting point of the robot. SLAM thus incrementally generates a map of the robot's environment. In the case of a robot repeating a route that it has previously mapped, the robot can solve for the localization subset of the problem without generating a new map. The generating of building a map of a new area necessitates SLAM.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0016</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0015</span>"</span>&gt;</span><span>SLAM is commonly implemented utilizing a depth sensor, such as a LIDAR sensor or a stereo camera. SLAM normally necessitates such devices for enabling the SLAM process to measure the depth and distance of three-dimensional landmarks and to calculate the robot's position in relation to those landmarks. SLAM may also be implemented using monocular vision, but the depth recovered through triangulation of landmarks from a moving camera over time is up to scale only, such that relative depths of objects in the scene are recovered without absolute depth measurements.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0017</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0016</span>"</span>&gt;</span><span>Applicant recognizes than allied problem in robots is one of obstacle avoidance. Robots must know how far an object is from the robot such that the robot can determine a collision-free path around the object. Robots utilize LIDAR sensors and stereo camera to determine a dense depth-map of obstacles around the robot. Some of the same obstacles determined through this process may be utilized as three-dimensional landmarks in the SLAM implementation.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0018</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0017</span>"</span>&gt;</span><span>Applicant has developed systems, methods, and devices for improving operations in both SLAM and obstacle avoidance. Applicant presents systems, methods, and devices for generating a dense depth map for obstacle avoidance, determining a robot's location, and determining pose vector data as a robot traverses its environment. The systems, methods, and devices of the present disclosure utilize a monocular camera and do not necessitate the use of expensive LIDAR sensors or stereo cameras that further require intensive computing resources. The disclosure presents lightweight, inexpensive, and low-computing methods for sensing a robot's surrounding, localizing a robot within its environment, and enabling the robot to generate obstacle avoidance movement procedures. Such systems, methods, and devices of the present disclosure may be implemented on any suitable robotics system, including for example, an autonomous vehicle, a mobile robot, and/or a drone or smart mobility vehicle.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0019</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0018</span>"</span>&gt;</span><span>Variational autoencoders (VAEs) are a class of latent variable models that provide compressed latent representations of data. A VAE can serve as an autoencoder while further serving as a generative model from which new data can be generated by sampling from a latent manifold. The VAE consists of an encoder, which maps the input to a compressed latent representation. The VAE further includes a decoder configured to decode the latent vector back to an output. The entire VAE system may be trained end to end as a deep neural network.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0020</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0019</span>"</span>&gt;</span><span>The VAE may be configured to encode meaningful information about various data attributes in its latent manifold which can then be exploited to carry out pertinent tasks. In an implementation of the disclosure, Applicant presents utilizing a shared latent space assumption of a VAE between an image, pose vector data of the image, and a depth map of the image, to facilitate the use of SLAM in conjunction with the VAE.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0021</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0020</span>"</span>&gt;</span><span>Generative adversarial networks (GANs) are a class of generative models configured to produce high quality samples from probability distributions of interest. In the image domain, a GAN may generate output samples of stellar artistic quality. The training methodology for a GAN is adversarial, in that the generator (the network that produces samples, often called “fakes”) learns by fooling another network called the discriminator that decides whether the samples produced are real or fake. The generator network and the discriminator network are trained in tandem, with the generator network eventually learning to produce samples that succeed in fooling the discriminator network. At such a point, the GAN is able to generate samples from the probability distribution underlying the generative process.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0022</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0021</span>"</span>&gt;</span><span>Applicant recognizes that VAEs confer advantages in providing latent representations of data for further us. However, one drawback of the VAE is the blurriness of the samples produced. GANs, on the other hand, produce excellent samples but do not have a useful latent representation available. The variational autoencoder generative adversarial network (VAE-GAN) utilizes and combines each system such that one obtains a tractable VAE latent representation while also improving upon the quality of the samples by using a GAN as the generator in the decoder of the VAE. This results in crisper images than a VAE alone.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0023</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0022</span>"</span>&gt;</span><span>The systems, methods, and devices of the present disclosure utilize the VAE-GAN as the central machinery in the SLAM algorithm. Such systems, methods, and devices receiving an input such as a red-green-blue (RGB) image and outputs corresponding depth maps and pose vector data for the camera that captured the RGB image. The system is trained using a regular stereo visual SLAM pipeline, where stereo visual simultaneous localization and mapping (vSLAM) receives a sequence of stereoscopic images, generates the depth maps and corresponding six Degree of Freedom poses as the stereo camera moves through space. Stereo vSLAM trains the VAE-GAN-SLAM algorithm using a sequence of RGB images, the corresponding depth maps for the images, and the corresponding pose vector data for the images. The VAE-GAN is trained to reconstruct the RGB image, the pose vector data for the image, and the depth map for the image while creating a shared latent space representation of the same. The assumption is that the RGB image, depth map of the image, and pose vector data of the image are sampled from places close together in the real world, are close together in the learnt shared latent space as well. After the networks are trained, the VAE-GAN take as its input an RGB image coming from a monocular camera moving through the same environment and produce both a depth map and pose vector data for the monocular camera.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0024</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0023</span>"</span>&gt;</span><span>In an embodiment, the latent space representation of the VAE-GAN also enables disentanglement and latent space arithmetic. An example of such an embodiment would be to isolate a dimension in the latent vector responsible for a certain attribute of interest, such as a pose dimension, and create previously unseen view of a scene by changing the pose vector.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0025</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0024</span>"</span>&gt;</span><span>Applicant recognizes that the systems, methods, and devices disclosed herein enable the use of the system as a SLAM box for facilitating fast and easy single-image inference producing the pose of a robot and the positions of obstacles in the environment around the robot.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0026</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0025</span>"</span>&gt;</span><span>Generative adversarial networks (GANs) have shown that image-to-image transformation, for instance segmentation or labelling tasks, can be achieved with smaller amounts of training data compared to regular convolutional neural networks by training generative networks and discriminative networks in an adversarial manner. Applicant presents systems, methods, and devices for depth estimation of a single image using a GAN. Such systems, methods, and devices improve performance over known depth estimation systems, and further require a smaller number of training images. The use of GAN as opposed to a regular convolutional neural network enables the collection of a small amount of training data in each environment, typically in the hundreds of images as opposed to the hundreds of thousands of images required by convolutional neural networks. Such systems, methods, and devices reduce the burden for data collection by an order of magnitude.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0027</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0026</span>"</span>&gt;</span><span>Applicant further presents systems, methods, and devices for depth estimation utilizing visual simultaneous localization and mapping (vSLAM) methods for ensuring temporal consistency in the generated depth maps produced by the GAN as the camera moves through an environment. The vSLAM module provides pose information of the camera, e.g. how much the camera has moved between successive images. Such pose information is provided to the GAN as a temporal constraint on training the GAN to promote the GAN to generate consistent depth maps for successive images.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0028</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0027</span>"</span>&gt;</span><span>Before the methods, systems, and devices for determining simultaneous localization and mapping for a robot are disclosed and described, it is to be understood that this disclosure is not limited to the configurations, process steps, and materials disclosed herein as such configurations, process steps, and materials may vary somewhat. It is also to be understood that the terminology employed herein is used for describing implementations only and is not intended to be limiting since the scope of the disclosure will be limited only by the appended claims and equivalents thereof.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0029</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0028</span>"</span>&gt;</span><span>In describing and claiming the disclosure, the following terminology will be used in accordance with the definitions set out below.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0030</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0029</span>"</span>&gt;</span><span>It must be noted that, as used in this specification and the appended claims, the singular forms “a,” “an,” and “the” include plural referents unless the context clearly dictates otherwise.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0031</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0030</span>"</span>&gt;</span><span>As used herein, the terms “comprising,” “including,” “containing,” “characterized by,” and grammatical equivalents thereof are inclusive or open-ended terms that do not exclude additional, unrecited elements or method steps.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0032</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0031</span>"</span>&gt;</span><span>In one embodiment, a method for mapping and localizing a robot, such as an autonomous vehicle, in an environment is disclosed. The method includes receiving an image from a camera of a vehicle. The method includes providing the image to a variational autoencoder generative adversarial network (VAE-GAN). The method includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image. The method includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map. The method is such that the VAE-GAN comprises a single latent space for encoding a plurality of inputs.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0033</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0032</span>"</span>&gt;</span><span>Further embodiments and examples will be discussed in relation to the figures below.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="folder" id="folder100"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0034</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0033</span>"</span>&gt;</span></div><div class="opened"><span>Referring now to the figures, </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 1</span><span class="html-tag">&lt;/figref&gt;</span></div><span> illustrates an example vehicle control system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span> that may be used for autonomous or assisted driving. The automated driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be used to automate or control operation of a vehicle or to aid a human driver. For example, the automated driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span> may control one or more of braking, steering, acceleration, lights, alerts, driver notifications, radio, or any other auxiliary systems of the vehicle. In another example, the automated driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span> may not be able to provide any control of the driving (e.g., steering, acceleration, or braking), but may provide notifications and alerts to assist a human driver in driving safely. The automated driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span> may use a neural network, or other model or algorithm to detect or localize objects based on perception data gathered by one or more sensors.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder101"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0035</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0034</span>"</span>&gt;</span></div><div class="opened"><span>The vehicle control system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span> also includes one or more sensor systems/devices for detecting a presence of objects near or within a sensor range of a parent vehicle (e.g., a vehicle that includes the vehicle control system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span>). For example, the vehicle control system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span> may include one or more radar systems </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>106</span><span class="html-tag">&lt;/b&gt;</span></div><span>, one or more LIDAR systems </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>108</span><span class="html-tag">&lt;/b&gt;</span></div><span>, one or more camera systems </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>110</span><span class="html-tag">&lt;/b&gt;</span></div><span>, a global positioning system (GPS) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>112</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and/or one or more ultrasound systems </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>114</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The vehicle control system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span> may include a data store </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>116</span><span class="html-tag">&lt;/b&gt;</span></div><span> for storing relevant or useful data for navigation and safety such as map data, driving history or other data. The vehicle control system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span> may also include a transceiver </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>118</span><span class="html-tag">&lt;/b&gt;</span></div><span> for wireless communication with a mobile or wireless network, other vehicles, infrastructure, or any other communication system.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder102"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0036</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0035</span>"</span>&gt;</span></div><div class="opened"><span>The vehicle control system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span> may include vehicle control actuators </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>120</span><span class="html-tag">&lt;/b&gt;</span></div><span> to control various aspects of the driving of the vehicle such as electric motors, switches or other actuators, to control braking, acceleration, steering or the like. The vehicle control system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span> may also include one or more displays </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>122</span><span class="html-tag">&lt;/b&gt;</span></div><span>, speakers </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>124</span><span class="html-tag">&lt;/b&gt;</span></div><span>, or other devices so that notifications to a human driver or passenger may be provided. A display </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>122</span><span class="html-tag">&lt;/b&gt;</span></div><span> may include a heads-up display, dashboard display or indicator, a display screen, or any other visual indicator which may be seen by a driver or passenger of a vehicle. A heads-up display may be used to provide notifications or indicate locations of detected objects or overlay instructions or driving maneuvers for assisting a driver. The speakers </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>124</span><span class="html-tag">&lt;/b&gt;</span></div><span> may include one or more speakers of a sound system of a vehicle or may include a speaker dedicated to driver notification.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder103"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0037</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0036</span>"</span>&gt;</span></div><div class="opened"><span>It will be appreciated that the embodiment of </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 1</span><span class="html-tag">&lt;/figref&gt;</span></div><span> is given by way of example only. Other embodiments may include fewer or additional components without departing from the scope of the disclosure. Additionally, illustrated components may be combined or included within other components without limitation.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder104"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0038</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0037</span>"</span>&gt;</span></div><div class="opened"><span>In one embodiment, the automated driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to control driving or navigation of a parent vehicle. For example, the automated driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span> may control the vehicle control actuators </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>120</span><span class="html-tag">&lt;/b&gt;</span></div><span> to drive a path on a road, parking lot, driveway or other location. For example, the automated driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span> may determine a path based on information or perception data provided by any of the components </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>106</span><span class="html-tag">&lt;/b&gt;</span></div><span>-</span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>114</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The sensor systems/devices </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>106</span><span class="html-tag">&lt;/b&gt;</span></div><span>-</span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>114</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be used to obtain real-time sensor data so that the automated driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span> can assist a driver or drive a vehicle in real-time.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder105"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0039</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0038</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 2</span><span class="html-tag">&lt;/figref&gt;</span></div><span> illustrates a schematic block diagram of a training phase </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>200</span><span class="html-tag">&lt;/b&gt;</span></div><span> of a variational autoencoder generative adversarial network (VAE-GAN) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes an image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>204</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a corresponding image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>206</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes a pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>212</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a corresponding pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>214</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes a depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>222</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a corresponding depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>224</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Each of the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>206</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>214</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>224</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes a generative adversarial network (GAN) that comprises a GAN generator (see e.g. </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>) and a GAN discriminator (see e.g. </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span>). The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes a latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> that is shared by each of the image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>204</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>206</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>212</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>214</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>222</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>224</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> receives a training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> at the image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>204</span><span class="html-tag">&lt;/b&gt;</span></div><span> and generates a reconstructed image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>208</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> receives training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span> that is based on the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> at the pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>212</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> generates reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>216</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> receives a training depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span> that is based on the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> at the depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>222</span><span class="html-tag">&lt;/b&gt;</span></div><span> and outputs a reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>226</span><span class="html-tag">&lt;/b&gt;</span></div><span> that is based on the training depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder106"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0040</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0039</span>"</span>&gt;</span></div><div class="opened"><span>The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> is the central machinery in the simultaneous localization and mapping (SLAM) algorithm of the present disclosure. In an embodiment the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained utilizing a regular stereo visual SLAM pipeline. In such an embodiment, a stereo visual SLAM takes a sequence of stereoscopic images and generates depth maps and corresponding six Degrees of Freedom poses for the stereo camera as the camera moves through space. Stereo visual SLAM trains the VAE-GAN-SLAM algorithm using a sequence of red-green-blue (RGB) images where only the left image of a stereo pair is used, along with the corresponding depth maps and six Degrees of Freedom pose vector data for the sequence of RGB images. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained under the assumption that the RGB image, the depth map of the image, and the pose vector data of the image are sampled from locations close together in the real world that are also close together in the learnt shared latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> as well. After the networks are trained, the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> can take as its input an RGB image coming from a monocular camera moving through the same environment and produce both a depth map and a six Degree of Freedom pose vector data for the camera.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder107"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0041</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0040</span>"</span>&gt;</span></div><div class="opened"><span>The training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> is provided to the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> for training the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> to generate pose vector data and/or depth map data based on an image. In an embodiment the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> is a red-blue-green (RGB) image captured by a monocular camera. In an embodiment the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> is a single image of a stereo image pair captured by a stereo camera. The reconstructed image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>208</span><span class="html-tag">&lt;/b&gt;</span></div><span> is generated by the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>204</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>206</span><span class="html-tag">&lt;/b&gt;</span></div><span> are adversarial to one another and are configured to generate the reconstructed image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>208</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>204</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to receiving the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> and map the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> to a compress latent representation in the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>206</span><span class="html-tag">&lt;/b&gt;</span></div><span> comprises a GAN having a GAN generator and a GAN discriminator. The image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>206</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to decode the compressed latent representation of the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> from the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The GAN of the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>206</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to generate the reconstructed image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>208</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder108"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0042</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0041</span>"</span>&gt;</span></div><div class="opened"><span>The training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span> is provided to the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> for training the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> to generate pose vector data of an image. In an embodiment, the training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes six Degree of Freedom pose data of a camera that captured the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span>, wherein the six Degree of Freedom pose data indicates a relative pose of the camera when the image was captured as the camera traversed an environment. The reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>216</span><span class="html-tag">&lt;/b&gt;</span></div><span> is generated by the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>212</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to receive the training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span> and map the training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span> to a compressed latent representation in the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> of the VEA-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>214</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to decode the compressed latent representation of the training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span> from the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>214</span><span class="html-tag">&lt;/b&gt;</span></div><span> comprises a GAN that comprises a GAN generator and a GAN discriminator. The GAN of the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>214</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to generate the reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>216</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the training pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>210</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder109"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0043</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0042</span>"</span>&gt;</span></div><div class="opened"><span>The training depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span> is provided to the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> for training the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> to generate a depth map of an image. In an embodiment, the depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span> is based on the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span> and includes depth information for the training image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>202</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>226</span><span class="html-tag">&lt;/b&gt;</span></div><span> is generated by the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the training depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>222</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to receive the training depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span> and map the training depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span> to a compressed latent representation in the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> of the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>224</span><span class="html-tag">&lt;/b&gt;</span></div><span> comprises a GAN that comprises a GAN generator and a GAN discriminator. The depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>224</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to decode the compressed latent representation of the training depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span> from the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The GAN of the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>224</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to generate the reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>226</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the training depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>220</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder110"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0044</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0043</span>"</span>&gt;</span></div><div class="opened"><span>The latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> of the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> is shared by each of the image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>204</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>206</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>212</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>214</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>222</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>224</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Thus, the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained to generate each of the reconstructed image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>208</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>216</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>226</span><span class="html-tag">&lt;/b&gt;</span></div><span> in tandem. In an embodiment, the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes an encoded latent space vector applicable to each of an image, pose vector data of an image, and a depth map of an image. The latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> representation of the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> enables disentanglement and latent space arithmetic. An example of the disentanglement and latent space arithmetic includes isolating a dimension in the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> responsible for a certain attribute of interest, such as a posed dimension. This may enable the creation of a previously unseen view of a scheme by changing the pose vector. In an embodiment, training the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> simultaneously for all three attributes, namely the image, the pose vector data, and the depth map, forces the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>230</span><span class="html-tag">&lt;/b&gt;</span></div><span> to be consistent for each of the attributes. This provides an elegant formulation where the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> is not trained separately for each of an image, pose vector data, and a depth map. Thus, because the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained in tandem, the trained VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span> may receive an input image and generate any outer output such as pose vector data based on the input image or a depth map based on the input image.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder111"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0045</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0044</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 3</span><span class="html-tag">&lt;/figref&gt;</span></div><span> illustrates a schematic block diagram of a computing phase </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>300</span><span class="html-tag">&lt;/b&gt;</span></div><span> (alternatively may be referred to as a generative or execution phase) of a variational autoencoder generative adversarial network (VAE-GAN) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes an image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>304</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a corresponding image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>306</span><span class="html-tag">&lt;/b&gt;</span></div><span>, wherein the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>306</span><span class="html-tag">&lt;/b&gt;</span></div><span> comprises a GAN configured to generate a reconstructed image based on the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In an embodiment as illustrated in </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 3</span><span class="html-tag">&lt;/figref&gt;</span></div><span>, the image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>304</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>306</span><span class="html-tag">&lt;/b&gt;</span></div><span> have been trained (see </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 2</span><span class="html-tag">&lt;/figref&gt;</span></div><span>). The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes a pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>312</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a corresponding pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span>, wherein the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span> comprises a GAN configured to generate the reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>316</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In an embodiment as illustrated in </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 3</span><span class="html-tag">&lt;/figref&gt;</span></div><span>, the pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>312</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span> have been trained (see </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 2</span><span class="html-tag">&lt;/figref&gt;</span></div><span>). The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes a depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>322</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a corresponding depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span>, wherein the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span> comprises a GAN configured to generate the reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In an embodiment as illustrated in </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 3</span><span class="html-tag">&lt;/figref&gt;</span></div><span>, the depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>322</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span> have been trained (see </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 2</span><span class="html-tag">&lt;/figref&gt;</span></div><span>). The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes a latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>330</span><span class="html-tag">&lt;/b&gt;</span></div><span> that is shared by the image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>304</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>306</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>312</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>322</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> receives an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span> at the image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>304</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN outputs reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>316</span><span class="html-tag">&lt;/b&gt;</span></div><span> at the trained pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN outputs a reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span> at the trained depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder112"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0046</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0045</span>"</span>&gt;</span></div><div class="opened"><span>In an embodiment the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span> is a red-green-blue image captured by a monocular camera and provided to the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> after the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> has been trained. In an embodiment, the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span> is captured by a monocular camera of a vehicle, is provided to a vehicle controller, and is provided to the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> in real-time. The RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span> may provide a capture of an environment of the vehicle and may be utilized to determine depth perception for the vehicle surroundings. In such an embodiment the vehicle controller may implement the result of the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> into a SLAM algorithm for computing simultaneous localization and mapping of the vehicle in real-time. The vehicle controller may further provide a notification to a driver, determine a driving maneuver, or execute a driving maneuver based on the results of the SLAM algorithm.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder113"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0047</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0046</span>"</span>&gt;</span></div><div class="opened"><span>The reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>316</span><span class="html-tag">&lt;/b&gt;</span></div><span> is generated by a GAN embedded in the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span> of the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be trained to generate the reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>316</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on a monocular image. In an embodiment as illustrated in </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 3</span><span class="html-tag">&lt;/figref&gt;</span></div><span>, the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes a latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>330</span><span class="html-tag">&lt;/b&gt;</span></div><span> that is shared by each of an image encoder/decoder, a pose encoder/decoder, and a depth encoder/decoder. The shared latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>330</span><span class="html-tag">&lt;/b&gt;</span></div><span> enables the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> to generate any trained output based on an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span> (or non-RGB image) as illustrated. The reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>316</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes six Degree of Freedom pose data for a monocular camera. The reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>316</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be utilized by a vehicle to determine a location of the vehicle in its environment and further utilized for simultaneous localization and mapping of the vehicle as it moves through space by implementing the data in a SLAM algorithm.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder114"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0048</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0047</span>"</span>&gt;</span></div><div class="opened"><span>The reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span> is generated by a GAN embedded in the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span> of the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be trained to generate the reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span> based only on the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span> provides a dense depth map based on the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span> and may provide a dense depth map of a surrounding of a robot or autonomous vehicle. The reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be provided to a SLAM algorithm for calculating simultaneous localization and mapping of a robot as the robot moves through its environment. In an embodiment where the robot is an autonomous vehicle, a vehicle controller may then provide a notification to a driver, determine a driving maneuver, and/or execute a driving maneuver such as an obstacle avoidance maneuver based on the reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the result of the SLAM algorithm.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder115"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0049</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0048</span>"</span>&gt;</span></div><div class="opened"><span>The latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>330</span><span class="html-tag">&lt;/b&gt;</span></div><span> is shared by each of the image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>304</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>306</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>312</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>322</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In an embodiment the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>330</span><span class="html-tag">&lt;/b&gt;</span></div><span> comprises an encoded latent space vector that is utilized for each of an image, pose vector data of an image, and a depth map of an image. In such an embodiment, the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> is capable of determining any suitable output e.g. reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>316</span><span class="html-tag">&lt;/b&gt;</span></div><span> and/or a reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>302</span><span class="html-tag">&lt;/b&gt;</span></div><span> input. Each of the encoders, including the image encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>304</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>312</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the depth encoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>322</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to map an input into a compressed latent representation at the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>330</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Conversely, each of the decoders, including the image decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>306</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span> are configured to decode the compressed latent representation of the input from the latent space </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>330</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The decoders of the VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>301</span><span class="html-tag">&lt;/b&gt;</span></div><span> further include a GAN that is configured to generate an output based on the decoded version of the input.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder116"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0050</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0049</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 4</span><span class="html-tag">&lt;/figref&gt;</span></div><span> illustrates a schematic block diagram of a process </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>400</span><span class="html-tag">&lt;/b&gt;</span></div><span> of determining a depth map of an environment, according to one embodiment. In an embodiment the process </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>400</span><span class="html-tag">&lt;/b&gt;</span></div><span> is implemented in a depth decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>324</span><span class="html-tag">&lt;/b&gt;</span></div><span> that comprises a GAN configured to generate a reconstructed depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>326</span><span class="html-tag">&lt;/b&gt;</span></div><span>. It should be appreciated that a similar process </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>400</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be implemented in a pose decoder </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>314</span><span class="html-tag">&lt;/b&gt;</span></div><span> that comprises a GAN that is configured to generate reconstructed pose vector data </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>316</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The process </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>400</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes receiving an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> and feeding the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> to a generative adversarial network (hereinafter “GAN”) generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> generates a depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span>. A generative adversarial network (“GAN”) discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> receives the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> (i.e. the original image) and the depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> generated by the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to distinguish real and fake image pairs </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>410</span><span class="html-tag">&lt;/b&gt;</span></div><span>, e.g. genuine images received from a camera versus depth map images generated by the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder117"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0051</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0050</span>"</span>&gt;</span></div><div class="opened"><span>In an embodiment, the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> is received from a monocular camera and may be received from the monocular camera in real-time. In an embodiment, the monocular camera is attached to a moving device, such as a vehicle, and each RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> is captured when the monocular camera is in a unique position or is in a unique pose. In an embodiment, the monocular camera is attached to an exterior of a vehicle and provides the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> to a vehicle controller, and the vehicle controller is in communication with the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder118"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0052</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0051</span>"</span>&gt;</span></div><div class="opened"><span>The GAN (i.e. the combination of the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span>) comprises a deep neural network architecture comprising two adversarial nets in a zero-sum game framework. In an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to generate new data instances and the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to evaluate the new data instances for authenticity. In such an embodiment, the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to analyze the new data instances and determine whether each new data instance belongs to the actual training data sets or if it was generated artificially (see </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>410</span><span class="html-tag">&lt;/b&gt;</span></div><span>). The GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is configured to create new images that are passed to the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained to generate images that fool the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> into determining that an artificial new data instance belongs to the actual training data.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder119"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0053</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0052</span>"</span>&gt;</span></div><div class="opened"><span>In an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> receives an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> and returns a depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> based on the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> is fed to the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> alongside a stream of camera images from an actual dataset, and the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> determines a prediction of authenticity for each image, i.e. whether the image is a camera image from the actual dataset or a depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> generated by the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Thus, in such an embodiment, the GAN includes a double feedback loop wherein the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> is in a feedback loop with the ground truth of the images and the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is in a feedback loop with the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In an embodiment, the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> is a convolutional neural network configured to categorize images fed to it and the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is an inverse convolutional neural network. In an embodiment, both the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> are seeking to optimize a different and opposing objective function or loss function. Thus, as the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> changes its behavior, so does the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and vice versa. The losses of the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> push against each other to improve the outputs of the GAN.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder120"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0054</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0053</span>"</span>&gt;</span></div><div class="opened"><span>In an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is pretrained offline before the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> receives an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> from a monocular camera. In an embodiment, the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> is pretrained before the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained and this may provide a clearer gradient. In an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained using a known dataset as the initial training data for the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be seeded with a randomized input that is sampled from a predefined latent space, and thereafter, samples synthesized by the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> are evaluated by the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder121"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0055</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0054</span>"</span>&gt;</span></div><div class="opened"><span>In an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> circumvents the bottleneck for information commonly found in an encoder-decoder network known in the art. In such an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes skip connections between each layer of the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>, wherein each skip connection concatenates all channels of the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is optimized by alternating between one gradient descent step on the adversarial network then one step on the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>. At interference time, the generator net is run in the same manner as during the training phase. In an embodiment, instance normalization is applied to the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>, wherein dropout is applied at test time and batch normalization is applied using statistics of the test batch rather than aggregated statistics of the training batch.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder122"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0056</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0055</span>"</span>&gt;</span></div><div class="opened"><span>In an embodiment, the GAN comprises an encoder-decoder architecture as illustrated in </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 4</span><span class="html-tag">&lt;/figref&gt;</span></div><span>. In such an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> receives the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> and generates the depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> distinguishes between a pair comprising an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> are trained alternatively until the GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> cannot tell the difference between an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span>. This can encourage the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> to generate depth maps that are as close to ground truth as possible.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder123"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0057</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0056</span>"</span>&gt;</span></div><div class="opened"><span>The depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> constitute image-to-image translation that is carried out by the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> and based on the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In generating the depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span>, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> learns a mapping from a random noise vector z to determine the depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> output image. The GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained to produce outputs that cannot be distinguished from real images by an adversarial GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In an embodiment, an adversarial GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> learns to classify between an RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> and a depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> is trained to fool the adversarial GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In such an embodiment, both the adversarial GAN discriminator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>408</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> observe the depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> output images.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder124"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0058</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0057</span>"</span>&gt;</span></div><div class="opened"><span>In an embodiment, the input images, i.e. the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> and the output images, i.e. the depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span> differ in surface appearance but both include a rendering of the same underlying structure. Thus, structure in the RGB image </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>402</span><span class="html-tag">&lt;/b&gt;</span></div><span> is roughly aligned with structure in the depth map </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>406</span><span class="html-tag">&lt;/b&gt;</span></div><span>. In an embodiment, the GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span> architecture is designed around this consideration.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder125"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0059</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0058</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 5</span><span class="html-tag">&lt;/figref&gt;</span></div><span> illustrates a schematic flow chart diagram of a method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>500</span><span class="html-tag">&lt;/b&gt;</span></div><span> for localizing a vehicle in an environment and mapping the environment of the vehicle. The method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>500</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>500</span><span class="html-tag">&lt;/b&gt;</span></div><span> begins and the computing device receives an image from a camera of a vehicle at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>502</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The computing device provides the image to a variational autoencoder generative adversarial network (VAE-GAN) at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>504</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The computing device receives from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>506</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The computing device calculates simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>508</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs (see </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>510</span><span class="html-tag">&lt;/b&gt;</span></div><span>).</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder126"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0060</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0059</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 6</span><span class="html-tag">&lt;/figref&gt;</span></div><span> illustrates a schematic flow chart diagram of a method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>600</span><span class="html-tag">&lt;/b&gt;</span></div><span> for localizing a vehicle in an environment and mapping the environment of the vehicle. The method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>100</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>600</span><span class="html-tag">&lt;/b&gt;</span></div><span> begins and the computing device receives an image from a camera of a vehicle at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>602</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The computing devices provides the image to a variational autoencoder generative adversarial network (VAE-GAN) at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>604</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN is such that the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of an image encoder, an image decoder, a pose encoder, a pose decoder, a depth encoder, and a depth decoder are trained utilizing a single latent space of the VAE-GAN (see </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>606</span><span class="html-tag">&lt;/b&gt;</span></div><span>). The VAE-GAN is such that the VEA-GAN comprises a trained image encoder configured to receive the image, a trained pose decoder comprising a GAN configured to generate reconstructed pose vector data based on the image, and a trained depth decoder comprising a GAN configured to generate a reconstructed depth map based on the image (see </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>608</span><span class="html-tag">&lt;/b&gt;</span></div><span>). The computing device receives from the VAE-GAN the reconstructed pose vector data based on the image at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>610</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The computing device receives from the VAE-GAN the reconstructed depth map based on the image at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>612</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The computing device calculates simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>614</span><span class="html-tag">&lt;/b&gt;</span></div><span>.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder127"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0061</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0060</span>"</span>&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 7</span><span class="html-tag">&lt;/figref&gt;</span></div><span> illustrates a schematic flow chart diagram of a method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>700</span><span class="html-tag">&lt;/b&gt;</span></div><span> for training a VAE-GAN. The method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>700</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>700</span><span class="html-tag">&lt;/b&gt;</span></div><span> begins and the computing device provides a training image to an image encoder of a variational autoencoder generative adversarial network (VAE-GAN) at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>702</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The computing device provides training pose vector data based on the training image to a pose encoder of the VAE-GAN at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>704</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The computing devices provides a training depth map based on the training image to a depth encoder of the VAE-GAN at </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>706</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The VAE-GAN is such that the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of the image encoder, the pose encoder, and the depth encoder are trained in tandem utilizing a latent space of the VAE-GAN (see </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>708</span><span class="html-tag">&lt;/b&gt;</span></div><span>). The VAE-GAN is such that the VAE-GAN comprises an encoded latent space vector applicable to each of the training image, the training pose vector data, and the training depth map (see </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>710</span><span class="html-tag">&lt;/b&gt;</span></div><span>).</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder128"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0062</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0061</span>"</span>&gt;</span></div><div class="opened"><span>Referring now to </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 8</span><span class="html-tag">&lt;/figref&gt;</span></div><span>, a block diagram of an example computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span> is illustrated. Computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span> may be used to perform various procedures, such as those discussed herein. In one embodiment, the computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span> can function as a neural network such as a GAN generator </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>404</span><span class="html-tag">&lt;/b&gt;</span></div><span>, a vehicle controller such as an autonomous driving/assistance system </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>102</span><span class="html-tag">&lt;/b&gt;</span></div><span>, a VAE-GAN </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>201</span><span class="html-tag">&lt;/b&gt;</span></div><span>, a server, and the like. Computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span> can perform various monitoring functions as discussed herein, and can execute one or more application programs, such as the application programs or functionality described herein. Computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span> can be any of a wide variety of computing devices, such as a desktop computer, in-dash computer, vehicle control system, a notebook computer, a server computer, a handheld computer, tablet computer and the like.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder129"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0063</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0062</span>"</span>&gt;</span></div><div class="opened"><span>Computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes one or more processor(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>802</span><span class="html-tag">&lt;/b&gt;</span></div><span>, one or more memory device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>804</span><span class="html-tag">&lt;/b&gt;</span></div><span>, one or more interface(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>806</span><span class="html-tag">&lt;/b&gt;</span></div><span>, one or more mass storage device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>808</span><span class="html-tag">&lt;/b&gt;</span></div><span>, one or more Input/output (I/O) device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>810</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and a display device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>830</span><span class="html-tag">&lt;/b&gt;</span></div><span> all of which are coupled to a bus </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>812</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Processor(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>802</span><span class="html-tag">&lt;/b&gt;</span></div><span> include one or more processors or controllers that execute instructions stored in memory device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>804</span><span class="html-tag">&lt;/b&gt;</span></div><span> and/or mass storage device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>808</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Processor(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>802</span><span class="html-tag">&lt;/b&gt;</span></div><span> may also include various types of computer-readable media, such as cache memory.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder130"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0064</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0063</span>"</span>&gt;</span></div><div class="opened"><span>Memory device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>804</span><span class="html-tag">&lt;/b&gt;</span></div><span> include various computer-readable media, such as volatile memory (e.g., random access memory (RAM) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>814</span><span class="html-tag">&lt;/b&gt;</span></div><span>) and/or nonvolatile memory (e.g., read-only memory (ROM) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>816</span><span class="html-tag">&lt;/b&gt;</span></div><span>). Memory device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>804</span><span class="html-tag">&lt;/b&gt;</span></div><span> may also include rewritable ROM, such as Flash memory.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder131"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0065</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0064</span>"</span>&gt;</span></div><div class="opened"><span>Mass storage device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>808</span><span class="html-tag">&lt;/b&gt;</span></div><span> include various computer readable media, such as magnetic tapes, magnetic disks, optical disks, solid-state memory (e.g., Flash memory), and so forth. As shown in </span><div class="line"><span class="html-tag">&lt;figref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">DRAWINGS</span>"</span>&gt;</span><span>FIG. 8</span><span class="html-tag">&lt;/figref&gt;</span></div><span>, a particular mass storage device is a hard disk drive </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>824</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Various drives may also be included in mass storage device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>808</span><span class="html-tag">&lt;/b&gt;</span></div><span> to enable reading from and/or writing to the various computer readable media. Mass storage device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>808</span><span class="html-tag">&lt;/b&gt;</span></div><span> include removable media </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>826</span><span class="html-tag">&lt;/b&gt;</span></div><span> and/or non-removable media.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder132"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0066</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0065</span>"</span>&gt;</span></div><div class="opened"><span>I/O device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>810</span><span class="html-tag">&lt;/b&gt;</span></div><span> include various devices that allow data and/or other information to be input to or retrieved from computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Example I/O device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>810</span><span class="html-tag">&lt;/b&gt;</span></div><span> include cursor control devices, keyboards, keypads, microphones, monitors or other display devices, speakers, printers, network interface cards, modems, and the like.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder133"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0067</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0066</span>"</span>&gt;</span></div><div class="opened"><span>Display device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>830</span><span class="html-tag">&lt;/b&gt;</span></div><span> includes any type of device capable of displaying information to one or more users of computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Examples of display device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>830</span><span class="html-tag">&lt;/b&gt;</span></div><span> include a monitor, display terminal, video projection device, and the like.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder134"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0068</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0067</span>"</span>&gt;</span></div><div class="opened"><span>Interface(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>806</span><span class="html-tag">&lt;/b&gt;</span></div><span> include various interfaces that allow computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span> to interact with other systems, devices, or computing environments. Example interface(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>806</span><span class="html-tag">&lt;/b&gt;</span></div><span> may include any number of different network interfaces </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>820</span><span class="html-tag">&lt;/b&gt;</span></div><span>, such as interfaces to local area networks (LANs), wide area networks (WANs), wireless networks, and the Internet. Other interface(s) include user interface </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>818</span><span class="html-tag">&lt;/b&gt;</span></div><span> and peripheral device interface </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>822</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The interface(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>806</span><span class="html-tag">&lt;/b&gt;</span></div><span> may also include one or more user interface elements </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>818</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The interface(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>806</span><span class="html-tag">&lt;/b&gt;</span></div><span> may also include one or more peripheral interfaces such as interfaces for printers, pointing devices (mice, track pad, or any suitable user interface now known to those of ordinary skill in the field, or later discovered), keyboards, and the like.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder135"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0069</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0068</span>"</span>&gt;</span></div><div class="opened"><span>Bus </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>812</span><span class="html-tag">&lt;/b&gt;</span></div><span> allows processor(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>802</span><span class="html-tag">&lt;/b&gt;</span></div><span>, memory device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>804</span><span class="html-tag">&lt;/b&gt;</span></div><span>, interface(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>806</span><span class="html-tag">&lt;/b&gt;</span></div><span>, mass storage device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>808</span><span class="html-tag">&lt;/b&gt;</span></div><span>, and I/O device(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>810</span><span class="html-tag">&lt;/b&gt;</span></div><span> to communicate with one another, as well as other devices or components coupled to bus </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>812</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Bus </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>812</span><span class="html-tag">&lt;/b&gt;</span></div><span> represents one or more of several types of bus structures, such as a system bus, PCI bus, IEEE bus, USB bus, and so forth.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="folder" id="folder136"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0070</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0069</span>"</span>&gt;</span></div><div class="opened"><span>For purposes of illustration, programs and other executable program components are shown herein as discrete blocks, although it is understood that such programs and components may reside at various times in different storage components of computing device </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>800</span><span class="html-tag">&lt;/b&gt;</span></div><span> and are executed by processor(s) </span><div class="line"><span class="html-tag">&lt;b&gt;</span><span>802</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Alternatively, the systems and procedures described herein can be implemented in hardware, or a combination of hardware, software, and/or firmware. For example, one or more application specific integrated circuits (ASICs) can be programmed to carry out one or more of the systems and procedures described herein.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/p&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;heading<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">h-0005</span>"</span><span class="html-attribute"> <span class="html-attribute-name">level</span>="<span class="html-attribute-value">1</span>"</span>&gt;</span><span>EXAMPLES</span><span class="html-tag">&lt;/heading&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0071</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0070</span>"</span>&gt;</span><span>The following examples pertain to further embodiments.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0072</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0071</span>"</span>&gt;</span><span>Example 1 is a method for simultaneous localization and mapping of a robot in an environment. The method includes: receiving an image from a camera of a vehicle; providing the image to a variational autoencoder generative adversarial network (VAE-GAN); receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0073</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0072</span>"</span>&gt;</span><span>Example 2 is a method as in Example 1, further comprising training the VAE-GAN, wherein training the VAE-GAN comprises: providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation; providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to the compressed latent representation; and providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to the compressed latent representation.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0074</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0073</span>"</span>&gt;</span><span>Example 3 is a method as in any of Examples 1-2, wherein the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of: the image encoder and the image decoder; the pose encoder and the pose decoder; and the depth encoder and the depth decoder are trained in tandem utilizing the latent space of the VAE-GAN.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0075</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0074</span>"</span>&gt;</span><span>Example 4 is a method as in any of Examples 1-3, wherein each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0076</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0075</span>"</span>&gt;</span><span>Example 5 is a method as in any of Examples 1-4, wherein the VAE-GAN comprises an encoded latent space vector that is applicable to each of the training image, the training pose vector data, and the training depth map.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0077</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0076</span>"</span>&gt;</span><span>Example 6 is a method as in any of Examples 1-5, further comprising determining the training pose vector data based on the training image, wherein determining the training pose vector data comprises: receiving a plurality of stereo images forming a stereo image sequence; and calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry; wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0078</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0077</span>"</span>&gt;</span><span>Example 7 is a method as in any of Examples 1-6, wherein the camera of the vehicle comprises a monocular camera configured to capture a sequence of images of an environment of the vehicle, and wherein the image comprises a red-green-blue (RGB) image.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0079</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0078</span>"</span>&gt;</span><span>Example 8 is a method as in any of Examples 1-7, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0080</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0079</span>"</span>&gt;</span><span>Example 9 is a method as in any of Examples 1-8, wherein the VAE-GAN comprises: a trained image encoder configured to receive the image; a trained pose decoder comprising a GAN configured to generate the reconstructed pose vector data based on the image; and a trained depth decoder comprising a GAN configured to generate the reconstructed depth map based on the image.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0081</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0080</span>"</span>&gt;</span><span>Example 10 is a method as in any of Examples 1-9, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0082</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0081</span>"</span>&gt;</span><span>Example 11 is a method as in any of Examples 1-10, wherein the latent space of the VAE-GAN comprises an encoded latent space vector utilized for each of the image encoder, the pose decoder, and the depth decoder.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0083</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0082</span>"</span>&gt;</span><span>Example 12 is a method as in any of Examples 1-11, wherein the reconstructed pose vector data comprises six Degree of Freedom pose data pertaining to the camera of the vehicle.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0084</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0083</span>"</span>&gt;</span><span>Example 13 is non-transitory computer-readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to: receive an image from a camera of a vehicle; provide the image to a variational autoencoder generative adversarial network (VAE-GAN); receive from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and calculate simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0085</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0084</span>"</span>&gt;</span><span>Example 14 is non-transitory computer-readable storage media as in Example 13, wherein the instructions further cause the one or more processors to train the VAE-GAN, wherein training the VAE-GAN comprises: providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation; providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to the compressed latent representation; and providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to the compressed latent representation.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0086</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0085</span>"</span>&gt;</span><span>Example 15 is non-transitory computer-readable storage media as in any of Examples 13-14, wherein the instructions cause the one or more processors to train the VAE-GAN utilizing a plurality of inputs in tandem, such that each of: the image encoder and the image decoder; the pose encoder and the pose decoder; and the depth encoder and the depth decoder are trained in tandem such that each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0087</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0086</span>"</span>&gt;</span><span>Example 16 is non-transitory computer-readable storage media as in any of Examples 13-15, the instructions further cause the one or more processors to calculate the training pose vector data based on the training image, wherein calculating the training pose vector data comprises: receiving a plurality of stereo images forming a stereo image sequence; and calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry; wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0088</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0087</span>"</span>&gt;</span><span>Example 17 is non-transitory computer-readable storage media as in any of Examples 13-16, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0089</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0088</span>"</span>&gt;</span><span>Example 18 is a system for simultaneous localization and mapping of a vehicle in an environment, the system comprising: a monocular camera of a vehicle; a vehicle controller in communication with the monocular camera, wherein the vehicle controller comprises non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to: receive an image from the monocular camera of the vehicle; provide the image to a variational autoencoder generative adversarial network (VAE-GAN); receive from the VAE-GAN reconstructed pose vector data based on the image; receive from the VAE-GAN a reconstructed depth map based on the image; and calculate simultaneous localization and mapping for the vehicle based on one or more of the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0090</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0089</span>"</span>&gt;</span><span>Example 19 is a system as in Example 18, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0091</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0090</span>"</span>&gt;</span><span>Example 20 is a system as in any of Examples 18-19, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0092</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0091</span>"</span>&gt;</span><span>Example 21 is a system or device that includes means for implementing a method, system, or device as in any of Examples 1-20.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0093</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0092</span>"</span>&gt;</span><span>In the above disclosure, reference has been made to the accompanying drawings, which form a part hereof, and in which is shown by way of illustration specific implementations in which the disclosure may be practiced. It is understood that other implementations may be utilized, and structural changes may be made without departing from the scope of the present disclosure. References in the specification to “one embodiment,” “an embodiment,” “an example embodiment,” etc., indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to affect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0094</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0093</span>"</span>&gt;</span><span>Implementations of the systems, devices, and methods disclosed herein may comprise or utilize a special purpose or general-purpose computer including computer hardware, such as, for example, one or more processors and system memory, as discussed herein. Implementations within the scope of the present disclosure may also include physical and other computer-readable media for carrying or storing computer-executable instructions and/or data structures. Such computer-readable media can be any available media that can be accessed by a general purpose or special purpose computer system. Computer-readable media that store computer-executable instructions are computer storage media (devices). Computer-readable media that carry computer-executable instructions are transmission media. Thus, by way of example, and not limitation, implementations of the disclosure can comprise at least two distinctly different kinds of computer-readable media: computer storage media (devices) and transmission media.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0095</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0094</span>"</span>&gt;</span><span>Computer storage media (devices) includes RAM, ROM, EEPROM, CD-ROM, solid state drives (“SSDs”) (e.g., based on RAM), Flash memory, phase-change memory (“PCM”), other types of memory, other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium, which can be used to store desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0096</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0095</span>"</span>&gt;</span><span>An implementation of the devices, systems, and methods disclosed herein may communicate over a computer network. A “network” is defined as one or more data links that enable the transport of electronic data between computer systems and/or modules and/or other electronic devices. When information is transferred or provided over a network or another communications connection (either hardwired, wireless, or a combination of hardwired or wireless) to a computer, the computer properly views the connection as a transmission medium. Transmissions media can include a network and/or data links, which can be used to carry desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer. Combinations of the above should also be included within the scope of computer-readable media.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0097</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0096</span>"</span>&gt;</span><span>Computer-executable instructions comprise, for example, instructions and data which, when executed at a processor, cause a general-purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions. The computer executable instructions may be, for example, binaries, intermediate format instructions such as assembly language, or even source code. Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the described features or acts described above. Rather, the described features and acts are disclosed as example forms of implementing the claims.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0098</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0097</span>"</span>&gt;</span><span>Those skilled in the art will appreciate that the disclosure may be practiced in network computing environments with many types of computer system configurations, including, an in-dash vehicle computer, personal computers, desktop computers, laptop computers, message processors, hand-held devices, multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, mobile telephones, PDAs, tablets, pagers, routers, switches, various storage devices, and the like. The disclosure may also be practiced in distributed system environments where local and remote computer systems, which are linked (either by hardwired data links, wireless data links, or by a combination of hardwired and wireless data links) through a network, both perform tasks. In a distributed system environment, program modules may be located in both local and remote memory storage devices.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0099</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0098</span>"</span>&gt;</span><span>Further, where appropriate, functions described herein can be performed in one or more of: hardware, software, firmware, digital components, or analog components. For example, one or more application specific integrated circuits (ASICs) can be programmed to carry out one or more of the systems and procedures described herein. Certain terms are used throughout the description and claims to refer to particular system components. The terms “modules” and “components” are used in the names of certain components to reflect their implementation independence in software, hardware, circuitry, sensors, or the like. As one skilled in the art will appreciate, components may be referred to by different names. This document does not intend to distinguish between components that differ in name, but not function.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0100</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0099</span>"</span>&gt;</span><span>It should be noted that the sensor embodiments discussed above may comprise computer hardware, software, firmware, or any combination thereof to perform at least a portion of their functions. For example, a sensor may include computer code configured to be executed in one or more processors and may include hardware logic/electrical circuitry controlled by the computer code. These example devices are provided herein purposes of illustration and are not intended to be limiting. Embodiments of the present disclosure may be implemented in further types of devices, as would be known to persons skilled in the relevant art(s).</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0101</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0100</span>"</span>&gt;</span><span>At least some embodiments of the disclosure have been directed to computer program products comprising such logic (e.g., in the form of software) stored on any computer useable medium. Such software, when executed in one or more data processing devices, causes a device to operate as described herein.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0102</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0101</span>"</span>&gt;</span><span>While various embodiments of the present disclosure have been described above, it should be understood that they have been presented by way of example only, and not limitation. It will be apparent to persons skilled in the relevant art that various changes in form and detail can be made therein without departing from the spirit and scope of the disclosure. Thus, the breadth and scope of the present disclosure should not be limited by any of the above-described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents. The foregoing description has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the disclosure to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. Further, it should be noted that any or all of the aforementioned alternate implementations may be used in any combination desired to form additional hybrid implementations of the disclosure.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;p<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">p-0103</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">0102</span>"</span>&gt;</span><span>Further, although specific implementations of the disclosure have been described and illustrated, the disclosure is not to be limited to the specific forms or arrangements of parts so described and illustrated. The scope of the disclosure is to be defined by the claims appended hereto, any future claims submitted here and in different applications, and their equivalents.</span><span class="html-tag">&lt;/p&gt;</span></div><span>
</span><div class="line"><span class="comment html-comment">&lt;?detailed-description description="Detailed Description" end="tail"?&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/description&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;us-claim-statement&gt;</span><span>What is claimed is:</span><span class="html-tag">&lt;/us-claim-statement&gt;</span></div><span>
</span><div class="folder" id="folder137"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claims<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">claims</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder138"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00001</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00001</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder139"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>1</span><span class="html-tag">&lt;/b&gt;</span></div><span>. A method comprising:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receiving an image from a camera of a vehicle;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing the image to a variational autoencoder generative adversarial network (VAE-GAN);</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder140"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00002</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00002</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder141"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>2</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00001</span>"</span>&gt;</span><span>claim 1</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, further comprising training the VAE-GAN, wherein training the VAE-GAN comprises:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation of the training image;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation of the training pose vector data; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation of the training depth map.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder142"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00003</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00003</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder143"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>3</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00002</span>"</span>&gt;</span><span>claim 2</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>the image encoder and a corresponding image decoder;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>the pose encoder and a corresponding pose decoder; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>the depth encoder and a corresponding depth decoder are trained in tandem utilizing the latent space of the VAE-GAN.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder144"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00004</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00004</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder145"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>4</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00002</span>"</span>&gt;</span><span>claim 2</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder146"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00005</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00005</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder147"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>5</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00002</span>"</span>&gt;</span><span>claim 2</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the VAE-GAN comprises an encoded latent space vector that is applicable to each of the training image, the training pose vector data, and the training depth map.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder148"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00006</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00006</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder149"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>6</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00002</span>"</span>&gt;</span><span>claim 2</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, further comprising determining the training pose vector data based on the training image, wherein determining the training pose vector data comprises:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receiving a plurality of stereo images forming a stereo image sequence; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder150"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00007</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00007</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder151"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>7</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00001</span>"</span>&gt;</span><span>claim 1</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the camera of the vehicle comprises a monocular camera configured to capture a sequence of images of an environment of the vehicle, and wherein the image comprises a red-green-blue (RGB) image.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder152"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00008</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00008</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder153"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>8</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00001</span>"</span>&gt;</span><span>claim 1</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder154"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00009</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00009</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder155"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>9</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00001</span>"</span>&gt;</span><span>claim 1</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the VAE-GAN comprises:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a trained image encoder configured to receive the image;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a trained pose decoder comprising a GAN configured to generate the reconstructed pose vector data based on the image; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a trained depth decoder comprising a GAN configured to generate the reconstructed depth map based on the image.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder156"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00010</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00010</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder157"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>10</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00001</span>"</span>&gt;</span><span>claim 1</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the VAE-GAN comprises:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>an image encoder configured to map the image to a compressed latent representation;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a pose decoder comprising a GAN generator adversarial to a GAN discriminator;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder158"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00011</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00011</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder159"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>11</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00010</span>"</span>&gt;</span><span>claim 10</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the latent space of the VAE-GAN comprises an encoded latent space vector utilized for each of the image encoder, the pose decoder, and the depth decoder.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder160"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00012</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00012</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder161"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>12</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The method of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00001</span>"</span>&gt;</span><span>claim 1</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the reconstructed pose vector data comprises six Degree of Freedom pose data pertaining to the camera of the vehicle.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder162"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00013</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00013</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder163"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>13</span><span class="html-tag">&lt;/b&gt;</span></div><span>. Non-transitory computer-readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receive an image from a camera of a vehicle;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>provide the image to a variational autoencoder generative adversarial network (VAE-GAN);</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receive from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>calculate simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder164"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00014</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00014</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder165"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>14</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The non-transitory computer-readable storage media of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00013</span>"</span>&gt;</span><span>claim 13</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the instructions further cause the one or more processors to train the VAE-GAN, wherein training the VAE-GAN comprises:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation in the latent space;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation in the latent space; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation in the latent space.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder166"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00015</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00015</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder167"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>15</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The non-transitory computer-readable storage media of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00014</span>"</span>&gt;</span><span>claim 14</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the instructions cause the one or more processors to train the VAE-GAN utilizing a plurality of inputs in tandem, such that each of:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>the image encoder and a corresponding image decoder;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>the pose encoder and a corresponding pose decoder; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>the depth encoder and a corresponding depth decoder are trained in tandem such that each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder168"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00016</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00016</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder169"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>16</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The non-transitory computer-readable storage media of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00014</span>"</span>&gt;</span><span>claim 14</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the instructions further cause the one or more processors to calculate the training pose vector data based on the training image, wherein calculating the training pose vector data comprises:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receiving a plurality of stereo images forming a stereo image sequence; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder170"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00017</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00017</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder171"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>17</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The non-transitory computer-readable storage media of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00013</span>"</span>&gt;</span><span>claim 13</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder172"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00018</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00018</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder173"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>18</span><span class="html-tag">&lt;/b&gt;</span></div><span>. A system for simultaneous localization and mapping of a vehicle in an environment, the system comprising:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a monocular camera of a vehicle;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="folder" id="folder174"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><span>a vehicle controller in communication with the monocular camera, wherein the vehicle controller comprises non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receive an image from the monocular camera of the vehicle;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>provide the image to a variational autoencoder generative adversarial network (VAE-GAN);</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receive from the VAE-GAN reconstructed pose vector data based on the image;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>receive from the VAE-GAN a reconstructed depth map based on the image; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>calculate simultaneous localization and mapping for the vehicle based on one or more of the reconstructed pose vector data and the reconstructed depth map;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder175"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00019</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00019</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder176"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>19</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The system of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00018</span>"</span>&gt;</span><span>claim 18</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the VAE-GAN is trained and training the VAE-GAN comprises:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation of the training image;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation of the training pose vector data; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation of the training depth map.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span><div class="folder" id="folder177"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim<span class="html-attribute"> <span class="html-attribute-name">id</span>="<span class="html-attribute-value">CLM-00020</span>"</span><span class="html-attribute"> <span class="html-attribute-name">num</span>="<span class="html-attribute-value">00020</span>"</span>&gt;</span></div><div class="opened"><span>
</span><div class="folder" id="folder178"><div class="line"><span class="folder-button fold"></span><span class="html-tag">&lt;claim-text&gt;</span></div><div class="opened"><div class="line"><span class="html-tag">&lt;b&gt;</span><span>20</span><span class="html-tag">&lt;/b&gt;</span></div><span>. The system of </span><div class="line"><span class="html-tag">&lt;claim-ref<span class="html-attribute"> <span class="html-attribute-name">idref</span>="<span class="html-attribute-value">CLM-00018</span>"</span>&gt;</span><span>claim 18</span><span class="html-tag">&lt;/claim-ref&gt;</span></div><span>, wherein the VAE-GAN comprises:
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>an image encoder configured to map the image to a compressed latent representation;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a pose decoder comprising a GAN generator adversarial to a GAN discriminator;</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span><div class="line"><span class="html-tag">&lt;claim-text&gt;</span><span>a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span><span class="html-tag">&lt;/claim-text&gt;</span></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim-text&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claim&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/claims&gt;</span></div></div><span>
</span></div><span class="folded hidden">...</span><div class="line"><span class="html-tag">&lt;/us-patent-application&gt;</span></div></div></div></body></html>