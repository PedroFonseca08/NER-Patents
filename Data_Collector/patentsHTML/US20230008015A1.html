<!DOCTYPE html>
<html lang="en">
  <head>
    <title>US20230008015A1 - Sensor fusion architecture for low-latency accurate road user detection 
        - Google Patents</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <meta name="referrer" content="origin-when-crossorigin">
    <link rel="canonical" href="https://patents.google.com/patent/US20230008015A1/en">
    <meta name="description" content="
     Aspects described herein provide sensor data stream processing for enabling camera/radar sensor fusion, with application to road user detection in the context of Autonomous Driving/Assisted Driving (ADAS). In particular, a scheme to extract Region-of-Interests (ROI) from a high-resolution, high-dimensional radar data cube that can then be transmitted to a sensor fusion unit is described. The ROI scheme allows to extract relevant information, thus reducing the latency and data transmission rate to the sensor fusion module, without trading-off accuracy and detection rates. The sensor data stream processing comprises receiving a first data stream from a radar sensor, forming a point cloud by extracting 3D points from the 3D data cube, performing clustering on the point cloud in order to identify high-density regions representing one or ROIs, and extracting one or more 3D bounding boxes from the 3D data cube corresponding to the one or more ROIs and classifying each ROI. 
   
   ">
    <meta name="DC.type" content="patent">
    <meta name="DC.title" content="Sensor fusion architecture for low-latency accurate road user detection 
       ">
    <meta name="DC.date" content="2021-07-12" scheme="dateSubmitted">
    <meta name="DC.description" content="
     Aspects described herein provide sensor data stream processing for enabling camera/radar sensor fusion, with application to road user detection in the context of Autonomous Driving/Assisted Driving (ADAS). In particular, a scheme to extract Region-of-Interests (ROI) from a high-resolution, high-dimensional radar data cube that can then be transmitted to a sensor fusion unit is described. The ROI scheme allows to extract relevant information, thus reducing the latency and data transmission rate to the sensor fusion module, without trading-off accuracy and detection rates. The sensor data stream processing comprises receiving a first data stream from a radar sensor, forming a point cloud by extracting 3D points from the 3D data cube, performing clustering on the point cloud in order to identify high-density regions representing one or ROIs, and extracting one or more 3D bounding boxes from the 3D data cube corresponding to the one or more ROIs and classifying each ROI. 
   
   ">
    <meta name="citation_patent_application_number" content="US:17/373,358">
    <meta name="citation_pdf_url" content="https://patentimages.storage.googleapis.com/41/44/80/c4cb905f84409e/US20230008015A1.pdf">
    <meta name="citation_patent_publication_number" content="US:20230008015:A1">
    <meta name="DC.date" content="2023-01-12">
    <meta name="DC.contributor" content="Ecaterina Bodnariuc" scheme="inventor">
    <meta name="DC.contributor" content="Lucas Rencker" scheme="inventor">
    <meta name="DC.contributor" content="Daniel Lampert Richart" scheme="inventor">
    <meta name="DC.contributor" content="Teraki GmbH" scheme="assignee">
    <meta name="DC.relation" content="US:10593042" scheme="references">
    <meta name="DC.relation" content="US:11630197" scheme="references">
    <meta name="DC.relation" content="US:11393097" scheme="references">
    <meta name="DC.relation" content="US:20220308205:A1" scheme="references">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Product+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700">

    <style>
      body { transition: none; }
    </style>

    <script>
      window.version = 'patent-search.search_20240108_RC01';

      function sendFeedback() {
        userfeedback.api.startFeedback({
          'productId': '713680',
          'bucket': 'patent-search-web',
          'productVersion': window.version,
        });
      }

      window.experiments = {};
      window.experiments.patentCountries = "ae,ag,al,am,ao,ap,ar,at,au,aw,az,ba,bb,bd,be,bf,bg,bh,bj,bn,bo,br,bw,bx,by,bz,ca,cf,cg,ch,ci,cl,cm,cn,co,cr,cs,cu,cy,cz,dd,de,dj,dk,dm,do,dz,ea,ec,ee,eg,em,ep,es,fi,fr,ga,gb,gc,gd,ge,gh,gm,gn,gq,gr,gt,gw,hk,hn,hr,hu,ib,id,ie,il,in,ir,is,it,jo,jp,ke,kg,kh,km,kn,kp,kr,kw,kz,la,lc,li,lk,lr,ls,lt,lu,lv,ly,ma,mc,md,me,mg,mk,ml,mn,mo,mr,mt,mw,mx,my,mz,na,ne,ng,ni,nl,no,nz,oa,om,pa,pe,pg,ph,pl,pt,py,qa,ro,rs,ru,rw,sa,sc,sd,se,sg,si,sk,sl,sm,sn,st,su,sv,sy,sz,td,tg,th,tj,tm,tn,tr,tt,tw,tz,ua,ug,us,uy,uz,vc,ve,vn,wo,yu,za,zm,zw";
      
      
      window.experiments.keywordWizard = true;
      
      
      
      window.experiments.definitions = true;

      window.Polymer = {
        dom: 'shady',
        lazyRegister: true,
      };
    </script>

    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/webcomponentsjs/webcomponents-lite.min.js"></script>
    <link rel="import" href="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.html">
  </head>
  <body unresolved>
    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.js"></script>
    <search-app>
      
      

      <article class="result" itemscope itemtype="http://schema.org/ScholarlyArticle">
  <h1 itemprop="pageTitle">US20230008015A1 - Sensor fusion architecture for low-latency accurate road user detection 
        - Google Patents</h1>
  <span itemprop="title">Sensor fusion architecture for low-latency accurate road user detection 
       </span>

  <meta itemprop="type" content="patent">
  <a href="https://patentimages.storage.googleapis.com/41/44/80/c4cb905f84409e/US20230008015A1.pdf" itemprop="pdfLink">Download PDF</a>
  <h2>Info</h2>

  <dl>
    <dt>Publication number</dt>
    <dd itemprop="publicationNumber">US20230008015A1</dd>
    <meta itemprop="numberWithoutCodes" content="20230008015">
    <meta itemprop="kindCode" content="A1">
    <meta itemprop="publicationDescription" content="Patent application publication">
    <span>US20230008015A1</span>
    <span>US17/373,358</span>
    <span>US202117373358A</span>
    <span>US2023008015A1</span>
    <span>US 20230008015 A1</span>
    <span>US20230008015 A1</span>
    <span>US 20230008015A1</span>
    <span>  </span>
    <span> </span>
    <span> </span>
    <span>US 202117373358 A</span>
    <span>US202117373358 A</span>
    <span>US 202117373358A</span>
    <span>US 2023008015 A1</span>
    <span>US2023008015 A1</span>
    <span>US 2023008015A1</span>

    <dt>Authority</dt>
    <dd itemprop="countryCode">US</dd>
    <dd itemprop="countryName">United States</dd>

    <dt>Prior art keywords</dt>
    <dd itemprop="priorArtKeywords" repeat>sensor</dd>
    <dd itemprop="priorArtKeywords" repeat>data</dd>
    <dd itemprop="priorArtKeywords" repeat>bounding boxes</dd>
    <dd itemprop="priorArtKeywords" repeat>data cube</dd>
    <dd itemprop="priorArtKeywords" repeat>radar</dd>

    <dt>Prior art date</dt>
    <dd><time itemprop="priorArtDate" datetime="2021-07-12">2021-07-12</time></dd>

    <dt>Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)</dt>
    <dd itemprop="legalStatusIfi" itemscope>
      <span itemprop="status">Abandoned</span>
    </dd>
  </dl>

  <dt>Application number</dt>
  <dd itemprop="applicationNumber">US17/373,358</dd>

  

  

  <dt>Inventor</dt>
  <dd itemprop="inventor" repeat>Ecaterina Bodnariuc</dd>
  <dd itemprop="inventor" repeat>Lucas Rencker</dd>
  <dd itemprop="inventor" repeat>Daniel Lampert Richart</dd>

  <dt>Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)</dt>
  <dd itemprop="assigneeCurrent" repeat>
    Teraki GmbH
  </dd>

  <dt>Original Assignee</dt>
  <dd itemprop="assigneeOriginal" repeat>Teraki GmbH</dd>

  <dt>Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)</dt>
  <dd><time itemprop="priorityDate" datetime="2021-07-12">2021-07-12</time></dd>

  <dt>Filing date</dt>
  <dd><time itemprop="filingDate" datetime="2021-07-12">2021-07-12</time></dd>

  <dt>Publication date</dt>
  <dd><time itemprop="publicationDate" datetime="2023-01-12">2023-01-12</time></dd>

  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2021-07-12">2021-07-12</time>
    <span itemprop="title">Application filed by Teraki GmbH</span>
    <span itemprop="type">filed</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    <span itemprop="assigneeSearch">Teraki GmbH</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2021-07-12">2021-07-12</time>
    <span itemprop="title">Priority to US17/373,358</span>
    <span itemprop="type">priority</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US20230008015A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2021-08-16">2021-08-16</time>
    <span itemprop="title">Assigned to TERAKI GMBH</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">TERAKI GMBH</span>
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    <span itemprop="description" repeat>Assignors: BODNARIUC, ECATERINA, RICHART, Daniel Lampert, RENCKER, LUCAS</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2022-07-12">2022-07-12</time>
    <span itemprop="title">Priority to EP22184339.4A</span>
    <span itemprop="type">priority</span>
    
    
    
    <span itemprop="documentId">patent/EP4120204A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-01-12">2023-01-12</time>
    <span itemprop="title">Publication of US20230008015A1</span>
    <span itemprop="type">publication</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US20230008015A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date">Status</time>
    <span itemprop="title">Abandoned</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    <span itemprop="current" content="true" bool>Current</span>
    
    
    
  </dd>

  <h2>Links</h2>
  <ul>
    <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoLink">
          <a href="https://appft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&p=1&u=/netahtml/PTO/srchnum.html&r=1&f=G&l=50&d=PG01&s1=20230008015.PGNR." itemprop="url" target="_blank"><span itemprop="text">USPTO</span></a>
        </li>
        
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoPatentCenterLink">
          <a href="https://patentcenter.uspto.gov/applications/17373358" itemprop="url" target="_blank"><span itemprop="text">USPTO PatentCenter</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoAssignmentLink">
          <a href="https://assignment.uspto.gov/patent/index.html#/patent/search/resultFilter?searchInput=20230008015" itemprop="url" target="_blank"><span itemprop="text">USPTO Assignment</span></a>
        </li>

    <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="espacenetLink">
        <a href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=2023008015A1&amp;KC=A1&amp;FT=D" itemprop="url" target="_blank"><span itemprop="text">Espacenet</span></a>
      </li>
      

    

    
      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="globalDossierLink">
        <a href="https://globaldossier.uspto.gov/#/result/publication/US/20230008015/1" itemprop="url" target="_blank"><span itemprop="text">Global Dossier</span></a>
      </li>

      

      

      

      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="stackexchangeLink">
        <a href="https://patents.stackexchange.com/questions/tagged/US20230008015" itemprop="url"><span itemprop="text">Discuss</span></a>
      </li>
      
  </ul>

  <ul itemprop="concept" itemscope>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004927</span>
      <span itemprop="name">fusion</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>title</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">51</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000001514</span>
      <span itemprop="name">detection method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>title</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">28</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012545</span>
      <span itemprop="name">processing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">38</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000000034</span>
      <span itemprop="name">method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">52</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004422</span>
      <span itemprop="name">calculation algorithm</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">21</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012706</span>
      <span itemprop="name">support-vector machine</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">9</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000011159</span>
      <span itemprop="name">matrix material</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">8</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000284</span>
      <span itemprop="name">extract</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000005540</span>
      <span itemprop="name">biological transmission</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">7</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000008569</span>
      <span itemprop="name">process</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">12</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010586</span>
      <span itemprop="name">diagram</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">9</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004891</span>
      <span itemprop="name">communication</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">7</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000008901</span>
      <span itemprop="name">benefit</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000009467</span>
      <span itemprop="name">reduction</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001960</span>
      <span itemprop="name">triggered effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002567</span>
      <span itemprop="name">autonomic effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000007635</span>
      <span itemprop="name">classification algorithm</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013135</span>
      <span itemprop="name">deep learning</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000011161</span>
      <span itemprop="name">development</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000018109</span>
      <span itemprop="name">developmental process</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005516</span>
      <span itemprop="name">engineering process</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000000605</span>
      <span itemprop="name">extraction</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000001914</span>
      <span itemprop="name">filtration</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006870</span>
      <span itemprop="name">function</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000670</span>
      <span itemprop="name">limiting effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010801</span>
      <span itemprop="name">machine learning</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003287</span>
      <span itemprop="name">optical effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000008447</span>
      <span itemprop="name">perception</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000135</span>
      <span itemprop="name">prohibitive effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000011218</span>
      <span itemprop="name">segmentation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012549</span>
      <span itemprop="name">training</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002411</span>
      <span itemprop="name">adverse</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013459</span>
      <span itemprop="name">approach</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003190</span>
      <span itemprop="name">augmentative effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001413</span>
      <span itemprop="name">cellular effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000003795</span>
      <span itemprop="name">chemical substances by application</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004069</span>
      <span itemprop="name">differentiation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">244000144992</span>
      <span itemprop="name">flock</span>
      <span itemprop="domain">Species</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000002372</span>
      <span itemprop="name">labelling</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013507</span>
      <span itemprop="name">mapping</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000003550</span>
      <span itemprop="name">marker</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012986</span>
      <span itemprop="name">modification</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004048</span>
      <span itemprop="name">modification</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006855</span>
      <span itemprop="name">networking</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000644</span>
      <span itemprop="name">propagated effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002829</span>
      <span itemprop="name">reductive effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003068</span>
      <span itemprop="name">static effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000007723</span>
      <span itemprop="name">transport mechanism</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
  </ul>

  <section>
    <h2>Images</h2>
    <ul>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/d5/1e/1a/0b56b130d8fba0/US20230008015A1-20230112-D00000.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/1e/56/37/5b13880a323a01/US20230008015A1-20230112-D00000.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="300">
            <meta itemprop="label" content="3D data cube">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="735">
              <meta itemprop="top" content="11">
              <meta itemprop="right" content="796">
              <meta itemprop="bottom" content="38">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="305">
            <meta itemprop="label" content="point cloud">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1045">
              <meta itemprop="top" content="687">
              <meta itemprop="right" content="1107">
              <meta itemprop="bottom" content="715">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="320">
            <meta itemprop="label" content="tree">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="551">
              <meta itemprop="top" content="236">
              <meta itemprop="right" content="613">
              <meta itemprop="bottom" content="263">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="330">
            <meta itemprop="label" content="street">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="524">
              <meta itemprop="top" content="308">
              <meta itemprop="right" content="588">
              <meta itemprop="bottom" content="337">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="340">
            <meta itemprop="label" content="car">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="146">
              <meta itemprop="top" content="537">
              <meta itemprop="right" content="208">
              <meta itemprop="bottom" content="564">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="350">
            <meta itemprop="label" content="birds">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="63">
              <meta itemprop="top" content="256">
              <meta itemprop="right" content="126">
              <meta itemprop="bottom" content="282">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="370">
            <meta itemprop="label" content="3D bounding boxes">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1810">
              <meta itemprop="top" content="99">
              <meta itemprop="right" content="1872">
              <meta itemprop="bottom" content="127">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="Camera">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="692">
              <meta itemprop="top" content="285">
              <meta itemprop="right" content="742">
              <meta itemprop="bottom" content="319">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="Camera">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="68">
              <meta itemprop="top" content="671">
              <meta itemprop="right" content="117">
              <meta itemprop="bottom" content="705">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/82/34/3d/87dfc50e48b328/US20230008015A1-20230112-D00001.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/08/ff/9d/cb6707ae925d20/US20230008015A1-20230112-D00001.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="100">
            <meta itemprop="label" content="system">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1304">
              <meta itemprop="top" content="1119">
              <meta itemprop="right" content="1389">
              <meta itemprop="bottom" content="1165">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="110">
            <meta itemprop="label" content="vehicle">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="240">
              <meta itemprop="top" content="788">
              <meta itemprop="right" content="323">
              <meta itemprop="bottom" content="832">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="120">
            <meta itemprop="label" content="multiple sensors">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="659">
              <meta itemprop="top" content="527">
              <meta itemprop="right" content="741">
              <meta itemprop="bottom" content="571">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="130">
            <meta itemprop="label" content="other vehicle">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="267">
              <meta itemprop="top" content="184">
              <meta itemprop="right" content="348">
              <meta itemprop="bottom" content="230">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="140">
            <meta itemprop="label" content="cloud environment">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1173">
              <meta itemprop="top" content="72">
              <meta itemprop="right" content="1250">
              <meta itemprop="bottom" content="121">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="environment">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1023">
              <meta itemprop="top" content="2367">
              <meta itemprop="right" content="1105">
              <meta itemprop="bottom" content="2415">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="201">
            <meta itemprop="label" content="traffic sign">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="101">
              <meta itemprop="top" content="1773">
              <meta itemprop="right" content="215">
              <meta itemprop="bottom" content="1819">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="202">
            <meta itemprop="label" content="pedestrian">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="475">
              <meta itemprop="top" content="1368">
              <meta itemprop="right" content="559">
              <meta itemprop="bottom" content="1416">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="203">
            <meta itemprop="label" content="street">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="593">
              <meta itemprop="top" content="1368">
              <meta itemprop="right" content="674">
              <meta itemprop="bottom" content="1416">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="car">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="711">
              <meta itemprop="top" content="1368">
              <meta itemprop="right" content="794">
              <meta itemprop="bottom" content="1416">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="205">
            <meta itemprop="label" content="cyclist">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="952">
              <meta itemprop="top" content="1368">
              <meta itemprop="right" content="1036">
              <meta itemprop="bottom" content="1416">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="206A">
            <meta itemprop="label" content="trees">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1258">
              <meta itemprop="top" content="1621">
              <meta itemprop="right" content="1385">
              <meta itemprop="bottom" content="1668">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="207">
            <meta itemprop="label" content="sky">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1270">
              <meta itemprop="top" content="1497">
              <meta itemprop="right" content="1370">
              <meta itemprop="bottom" content="1544">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/4c/93/e9/c35c27f73137ea/US20230008015A1-20230112-D00002.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/04/2b/bf/aae11f1b89feef/US20230008015A1-20230112-D00002.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="300">
            <meta itemprop="label" content="3D data cube">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="28">
              <meta itemprop="top" content="1652">
              <meta itemprop="right" content="66">
              <meta itemprop="bottom" content="1742">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="305">
            <meta itemprop="label" content="point cloud">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1058">
              <meta itemprop="top" content="1181">
              <meta itemprop="right" content="1096">
              <meta itemprop="bottom" content="1272">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="320">
            <meta itemprop="label" content="tree">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="369">
              <meta itemprop="top" content="1933">
              <meta itemprop="right" content="407">
              <meta itemprop="bottom" content="2025">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="330">
            <meta itemprop="label" content="street">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="481">
              <meta itemprop="top" content="1968">
              <meta itemprop="right" content="521">
              <meta itemprop="bottom" content="2062">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="340">
            <meta itemprop="label" content="car">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="830">
              <meta itemprop="top" content="2546">
              <meta itemprop="right" content="869">
              <meta itemprop="bottom" content="2640">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="350">
            <meta itemprop="label" content="birds">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="401">
              <meta itemprop="top" content="2671">
              <meta itemprop="right" content="440">
              <meta itemprop="bottom" content="2765">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="360">
            <meta itemprop="label" content="3D bounding boxes">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="352">
              <meta itemprop="top" content="394">
              <meta itemprop="right" content="391">
              <meta itemprop="bottom" content="485">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="370">
            <meta itemprop="label" content="3D bounding boxes">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="163">
              <meta itemprop="top" content="17">
              <meta itemprop="right" content="204">
              <meta itemprop="bottom" content="110">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="Camera">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="442">
              <meta itemprop="top" content="1733">
              <meta itemprop="right" content="501">
              <meta itemprop="bottom" content="1809">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="Camera">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1031">
              <meta itemprop="top" content="2685">
              <meta itemprop="right" content="1083">
              <meta itemprop="bottom" content="2757">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/e2/7c/2a/3df2cffbbba422/US20230008015A1-20230112-D00003.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/89/fa/bf/c2bb7c951b2a49/US20230008015A1-20230112-D00003.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="300">
            <meta itemprop="label" content="3D data cube">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="24">
              <meta itemprop="top" content="1778">
              <meta itemprop="right" content="66">
              <meta itemprop="bottom" content="1873">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="300">
            <meta itemprop="label" content="3D data cube">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="951">
              <meta itemprop="top" content="17">
              <meta itemprop="right" content="997">
              <meta itemprop="bottom" content="112">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="300B">
            <meta itemprop="label" content="image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1061">
              <meta itemprop="top" content="1723">
              <meta itemprop="right" content="1101">
              <meta itemprop="bottom" content="1851">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="320">
            <meta itemprop="label" content="tree">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="369">
              <meta itemprop="top" content="2060">
              <meta itemprop="right" content="408">
              <meta itemprop="bottom" content="2154">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="330">
            <meta itemprop="label" content="street">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="482">
              <meta itemprop="top" content="2099">
              <meta itemprop="right" content="520">
              <meta itemprop="bottom" content="2191">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="340">
            <meta itemprop="label" content="car">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="829">
              <meta itemprop="top" content="2674">
              <meta itemprop="right" content="869">
              <meta itemprop="bottom" content="2769">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="350">
            <meta itemprop="label" content="birds">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="399">
              <meta itemprop="top" content="2801">
              <meta itemprop="right" content="440">
              <meta itemprop="bottom" content="2893">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="360">
            <meta itemprop="label" content="3D bounding boxes">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="214">
              <meta itemprop="top" content="797">
              <meta itemprop="right" content="252">
              <meta itemprop="bottom" content="889">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="370">
            <meta itemprop="label" content="3D bounding boxes">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="26">
              <meta itemprop="top" content="422">
              <meta itemprop="right" content="66">
              <meta itemprop="bottom" content="514">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="Camera">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1221">
              <meta itemprop="top" content="1449">
              <meta itemprop="right" content="1260">
              <meta itemprop="bottom" content="1514">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/f6/01/fd/dac7bce7f208c2/US20230008015A1-20230112-D00004.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/9b/2f/17/c45d4d7c87a104/US20230008015A1-20230112-D00004.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="Camera">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="354">
              <meta itemprop="top" content="1276">
              <meta itemprop="right" content="421">
              <meta itemprop="bottom" content="1324">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="400">
            <meta itemprop="label" content="method">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="895">
              <meta itemprop="top" content="33">
              <meta itemprop="right" content="988">
              <meta itemprop="bottom" content="70">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/b5/40/58/8de5884a560992/US20230008015A1-20230112-D00005.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/84/3c/d4/1c6c7b9877ad28/US20230008015A1-20230112-D00005.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="Camera">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="303">
              <meta itemprop="top" content="941">
              <meta itemprop="right" content="366">
              <meta itemprop="bottom" content="989">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="500">
            <meta itemprop="label" content="method">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="898">
              <meta itemprop="top" content="32">
              <meta itemprop="right" content="990">
              <meta itemprop="bottom" content="69">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/59/99/d0/a814c3fc5feb27/US20230008015A1-20230112-D00006.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/65/8b/a7/75a6ff6e3b4db1/US20230008015A1-20230112-D00006.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/bc/90/e3/135aa2a10e3df5/US20230008015A1-20230112-D00007.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/37/ae/cb/7d466924f3f704/US20230008015A1-20230112-D00007.png">
        <ul>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>Classifications</h2>
    <ul>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/50</span>&mdash;<span itemprop="Description">Depth or shape recovery</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V20/00</span>&mdash;<span itemprop="Description">Scenes; Scene-specific elements</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V20/50</span>&mdash;<span itemprop="Description">Context or environment of the image</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V20/56</span>&mdash;<span itemprop="Description">Context or environment of the image exterior to a vehicle by using sensors mounted on the vehicle</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01</span>&mdash;<span itemprop="Description">MEASURING; TESTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S</span>&mdash;<span itemprop="Description">RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/00</span>&mdash;<span itemprop="Description">Systems using the reflection or reradiation of radio waves, e.g. radar systems; Analogous systems using reflection or reradiation of waves whose nature or wavelength is irrelevant or unspecified</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/02</span>&mdash;<span itemprop="Description">Systems using reflection of radio waves, e.g. primary radar systems; Analogous systems</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/50</span>&mdash;<span itemprop="Description">Systems of measurement based on relative movement of target</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/58</span>&mdash;<span itemprop="Description">Velocity or trajectory determination systems; Sense-of-movement determination systems</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/581</span>&mdash;<span itemprop="Description">Velocity or trajectory determination systems; Sense-of-movement determination systems using transmission of interrupted pulse modulated waves and based upon the Doppler effect resulting from movement of targets</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/582</span>&mdash;<span itemprop="Description">Velocity or trajectory determination systems; Sense-of-movement determination systems using transmission of interrupted pulse modulated waves and based upon the Doppler effect resulting from movement of targets adapted for simultaneous range and velocity measurements</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01</span>&mdash;<span itemprop="Description">MEASURING; TESTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S</span>&mdash;<span itemprop="Description">RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/00</span>&mdash;<span itemprop="Description">Systems using the reflection or reradiation of radio waves, e.g. radar systems; Analogous systems using reflection or reradiation of waves whose nature or wavelength is irrelevant or unspecified</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/02</span>&mdash;<span itemprop="Description">Systems using reflection of radio waves, e.g. primary radar systems; Analogous systems</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/50</span>&mdash;<span itemprop="Description">Systems of measurement based on relative movement of target</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/58</span>&mdash;<span itemprop="Description">Velocity or trajectory determination systems; Sense-of-movement determination systems</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/585</span>&mdash;<span itemprop="Description">Velocity or trajectory determination systems; Sense-of-movement determination systems processing the video signal in order to evaluate or display the velocity value</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01S13/587</span>&mdash;<span itemprop="Description">Velocity or trajectory determination systems; Sense-of-movement determination systems processing the video signal in order to evaluate or display the velocity value using optical means</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N20/00</span>&mdash;<span itemprop="Description">Machine learning</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N20/10</span>&mdash;<span itemprop="Description">Machine learning using kernel methods, e.g. support vector machines [SVM]</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T17/00</span>&mdash;<span itemprop="Description">Three dimensional [3D] modelling, e.g. data description of 3D objects</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/10</span>&mdash;<span itemprop="Description">Segmentation; Edge detection</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/11</span>&mdash;<span itemprop="Description">Region-based segmentation</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/20</span>&mdash;<span itemprop="Description">Image preprocessing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/25</span>&mdash;<span itemprop="Description">Determination of region of interest [ROI] or a volume of interest [VOI]</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/70</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding using pattern recognition or machine learning</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/77</span>&mdash;<span itemprop="Description">Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/80</span>&mdash;<span itemprop="Description">Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10</span>&mdash;<span itemprop="Description">Image acquisition modality</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10028</span>&mdash;<span itemprop="Description">Range image; Depth image; 3D point clouds</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10</span>&mdash;<span itemprop="Description">Image acquisition modality</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10032</span>&mdash;<span itemprop="Description">Satellite or aerial image; Remote sensing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10044</span>&mdash;<span itemprop="Description">Radar image</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20</span>&mdash;<span itemprop="Description">Special algorithmic details</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20212</span>&mdash;<span itemprop="Description">Image combination</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20221</span>&mdash;<span itemprop="Description">Image fusion; Image merging</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30</span>&mdash;<span itemprop="Description">Subject of image; Context of image processing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30248</span>&mdash;<span itemprop="Description">Vehicle exterior or interior</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30252</span>&mdash;<span itemprop="Description">Vehicle exterior; Vicinity of vehicle</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2210/00</span>&mdash;<span itemprop="Description">Indexing scheme for image generation or computer graphics</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2210/12</span>&mdash;<span itemprop="Description">Bounding box</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
    </ul>
  </section>

  

  

  <section>
    <h2>Definitions</h2>
    <ul>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a computer-implemented method and system described herein</span>
        <span itemprop="definition">provide sensor data processing.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">methods of enabling low level sensor fusion by lightweight semantic segmentation on sensors generating point cloud as generated from LIDAR, radar, cameras and Time-of-Flight sensors</span>
        <span itemprop="definition">are described.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a method for camera-radar sensor fusion</span>
        <span itemprop="definition">with application to road user detection in the context of Autonomous Driving/Assisted Driving (ADAS) is provided. More specifically, a scheme to extract Region-of-Interests (ROI) from a high-resolution, high-dimensional radar data cube is provided, that can then be transmitted to a sensor fusion unit.</span>
        <meta itemprop="num_attr" content="0004">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ROI</span>
        <span itemprop="definition">Region-of-Interests</span>
        <meta itemprop="num_attr" content="0004">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a computer-implemented method for sensor data stream processing</span>
        <span itemprop="definition">comprises receiving a first data stream from a radar sensor, forming a point cloud by extracting 3D points from the 3D data cube, and extracting one or more 3D bounding boxes from the 3D data cube corresponding to the one or more ROIs and classifying each ROI.</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the first data stream</span>
        <span itemprop="definition">comprises a 3D data cube including azimuth, range and velocity dimensions. Data of the 3D data cube that is not included in the one or more 3D bounding boxes is discarded after classification.</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">further comprises sensor fusion with a camera sensor, comprising receiving a second data stream from the camera sensor, detecting objects within the image and determining one or more 2D bounding boxes for each detected object, projecting the one or more 3D bounding boxes from the 3D data cube onto the image, and matching said one or more 3D bounding boxes with said one or more 2D bounding boxes.</span>
        <meta itemprop="num_attr" content="0006">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the second data stream</span>
        <span itemprop="definition">comprises an image including pixel values from the camera sensor.</span>
        <meta itemprop="num_attr" content="0006">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">further comprises classifying a fused object once a matched pair of 2D and 3D bounding boxes is identified. Classifying uses features computed from the radar sensor and the camera sensor.</span>
        <meta itemprop="num_attr" content="0007">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">matching</span>
        <span itemprop="definition">comprises computing an Intersection-Over-Union (IOU) between each 2D/3D bounding box pair, resulting in a matrix of pairwise IOUs, wherein a pair is a match if the IOU corresponding to the pair is greater than a threshold.</span>
        <meta itemprop="num_attr" content="0008">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">IOU</span>
        <span itemprop="definition">Intersection-Over-Union</span>
        <meta itemprop="num_attr" content="0008">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">forming of the point cloud by extracting 3D points from the 3D data cube</span>
        <span itemprop="definition">is performed by using a lightweight local maximum detector comprising a custom 3D Constant-False-Alarm-Rate, CFAR, algorithm that directly returns 3D points from the 3D data cube.</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a lightweight local maximum detector</span>
        <span itemprop="definition">comprising a custom 3D Constant-False-Alarm-Rate, CFAR, algorithm that directly returns 3D points from the 3D data cube.</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">classifying</span>
        <span itemprop="definition">comprises for each 3D bounding box, computing custom features utilizing a support vector machine, SVM, wherein said custom features comprise at least one of spatial shape, radar cross section, RCS, mean velocity and variance of velocity.</span>
        <meta itemprop="num_attr" content="0010">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">information from sensor fusion</span>
        <span itemprop="definition">is used for running time of interest, TOI, models relying on low latency sensor data processing.</span>
        <meta itemprop="num_attr" content="0011">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the above described embodiments</span>
        <span itemprop="definition">can be combined with each other.</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the above described embodiments</span>
        <span itemprop="definition">may also be implemented on a computer-readable medium comprising computer-readable instructions, that, when executed by a processor, cause the processor to perform the above described steps.</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">is a diagram illustrating a system comprising a vehicle, a corresponding camera, another vehicle and a cloud environment;</span>
        <meta itemprop="num_attr" content="0016">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">illustrates an exemplary environment that may be dealt with a vehicle implementing the herein described concepts.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the environment</span>
        <span itemprop="definition">comprises an exemplary scene including a traffic sign, a pedestrian, a street, a vehicle, a cyclist, trees and sky;</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3 A</span>
        <span itemprop="definition">is an exemplary concept diagram for extracting 3D points from a 3D data cube and clustering the point cloud;</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3 B</span>
        <span itemprop="definition">is an exemplary concept diagram for sensor fusion</span>
        <meta itemprop="num_attr" content="0019">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">is a flow diagram for a method of performing sensor data processing</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a flow diagram for a method of performing sensor fusion.</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">shows various detection metrics for a radar object detector.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">is a block diagram illustrating example physical components of a computing device with which aspects of the disclosure may be practiced.</span>
        <meta itemprop="num_attr" content="0023">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">aspects of the disclosure</span>
        <span itemprop="definition">are described more fully below with reference to the accompanying drawings, which from a part hereof, and which show specific example aspects.</span>
        <meta itemprop="num_attr" content="0024">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">different aspects of the disclosure</span>
        <span itemprop="definition">may be implemented in many different ways and should not be construed as limited to the aspects set forth herein; rather, these aspects are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the aspects to those skilled in the art.</span>
        <meta itemprop="num_attr" content="0024">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">aspects</span>
        <span itemprop="definition">may be practiced as methods, systems or devices. Accordingly, aspects may take the form of a hardware implementation, an entirely software implementation or an implementation combining software and hardware aspects. The following detailed description is, therefore, not to be taken in a limiting sense.</span>
        <meta itemprop="num_attr" content="0024">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">aspects of the present disclosure</span>
        <span itemprop="definition">relate to systems and methods for processing data streams from different sensors of a vehicle.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">processing that is performed on images that are recorded by a radar and camera of a vehicle</span>
        <span itemprop="definition">With the rise of remote and autonomic driving, the amount of image data which is streamed is ever increasing. In many cases, recording images by physical sensors, such as LIDAR, radar, camera, etc. which are integrated into vehicles (or which can be removably attached to vehicles) is valuable.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">sensors</span>
        <span itemprop="definition">examples include cameras, radars or LIDARs, each of those sensors having different advantages and limitations know to one of skill in the art.</span>
        <meta itemprop="num_attr" content="0027">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Cameras</span>
        <span itemprop="definition">for example, provide good lateral resolution and are able to sense color and texture information, which makes it ideal for object detection tasks.</span>
        <meta itemprop="num_attr" content="0027">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">cameras</span>
        <span itemprop="definition">only provide a 2D projection of a 3D world, and as such do not provide any range or distance information, and are not able to detect occluded objects.</span>
        <meta itemprop="num_attr" content="0027">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">illustrates a system 100 including a vehicle 110 , a set of multiple sensors 120 of the vehicle 110 , another vehicle 130 , and a cloud environment 140 .</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the set of multiple sensors 120</span>
        <span itemprop="definition">may include a camera, a LIDAR, a radar, a time-of-flight device and other sensors and devices that may be used for observing the environment of the vehicle 110 .</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle 110</span>
        <span itemprop="definition">may further comprise a processor configured to receive data from the multiple sensors 120 and to process the data before encoding the data. In one embodiment this data may be data from the radar sensor.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle 110</span>
        <span itemprop="definition">may further comprise a memory for saving the encoded image.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle 110</span>
        <span itemprop="definition">may further comprise an autonomous driving system that may be communicatively coupled to the processor of the vehicle and that may receive the encoded image.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the autonomous driving system</span>
        <span itemprop="definition">may use the encoded data for autonomously driving the vehicle 110 .</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle 110</span>
        <span itemprop="definition">may comprise one or more further sensors, such as a distance sensor and a temperature sensor.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle 110</span>
        <span itemprop="definition">may be further communicatively coupled to another vehicle 130 and/or a cloud environment 140 .</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the multiple sensors 120</span>
        <span itemprop="definition">may be integrated anywhere in the vehicle 110 (e.g., next to a headlight, a rearview mirror, etc.) or may comprise sensors that can be attached and removed from the vehicle 110 .</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the other vehicle 130</span>
        <span itemprop="definition">may also comprise different sensors (not shown) for observing the environment, a processor, a memory, and/or an autonomous driving system. Likewise, the processor of the other vehicle 130 may also be configured to process an image by filtering the image before encoding the image, as described herein.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud environment 140</span>
        <span itemprop="definition">may include a cloud storage for storing the encoded image. The cloud environment 140 , may be communicatively coupled to a remote driving system that may be used to control the vehicle 110 from remote by a remote driver.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">illustrates an exemplary environment 200 that may exist around the vehicle 110 .</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the environment 200</span>
        <span itemprop="definition">may comprise one or more objects.</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the exemplary environment 200 of FIG. 2</span>
        <span itemprop="definition">illustrates several regions 201 - 207 that display several objects including a traffic sign 201 , a pedestrian 202 , a street 203 , a car 204 , a cyclist 205 , two trees 206 A, 206 B and sky 207 . It is apparent that it may be possible to define more regions comprising further objects such as cyclist way, lane marker, or cloud which are also present in environment 200 .</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the environment 200</span>
        <span itemprop="definition">may be a representative environment with which aspects of the present disclosure may be confronted.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle 110</span>
        <span itemprop="definition">is depicted as a car, however, aspects of the present disclosure is not limited to be implemented by cars, but can be employed with other systems, vehicles and devices may be used for implementing the herein disclosed concepts.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Other examples of vehicles 110</span>
        <span itemprop="definition">may be a drone or a delivery robot.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the environment 200</span>
        <span itemprop="definition">may look quite different based on the vehicle 110 implementing the herein disclosed concepts.</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the environment 200</span>
        <span itemprop="definition">may comprise other drones and obstacles in the air, such as birds, wind turbines, buildings, aircrafts, etc.</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the herein disclosed techniques</span>
        <span itemprop="definition">pertain to a concept of enabling low level sensor fusion by lightweight semantic segmentation on sensors generating point clouds as generated from LIDAR, radar, camera, and/or Time-of-Flight sensors when capturing/observing the environment 200 .</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Regions of Interest (ROIs) within the environment 200</span>
        <span itemprop="definition">are detected by the sensors 120 , generating point clouds as data points in space and then using the point clouds to fuse with other sensor modalities such as cameras to enable real-time sensor fusion.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ROIs</span>
        <span itemprop="definition">Regions of Interest</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a camera-radar based sensor fusion scheme</span>
        <span itemprop="definition">is described herein, using high-dimensional camera and radars, while retaining the latency low enough to match safety requirements. Sensor fusion even in early warning system on a chip (SoC) (e.g., 1-5 ms timeframe) and not only in the centralized device fusing the results from various camera/Radar detections along the car is currently an important topic of interest.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SoC</span>
        <span itemprop="definition">system on a chip</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">aspects disclosed herein</span>
        <span itemprop="definition">can provide an advantage over existing centralized sensor fusion schemes that require the fusion, for example, within a 100 ms timeframe.</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an earlier trigger</span>
        <span itemprop="definition">could already be triggered at the camera/radar SoC level tuning the required frame-rates of all camera/radar data streams.</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">transmission of 3D cubes for a radar/camera cube and performing centralized fusion of the 3D cubes</span>
        <span itemprop="definition">results in a reduction in computational requirements when centralized fusion is triggered.</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a further aspect</span>
        <span itemprop="definition">is the ability to obtain outputs of the camera/radar sensor fusion stack and the corresponding locations of particular objects and thus the fusion of these (x,y,z) object tuples in the centralized sensor fusion unit.</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a low-level sensor fusion stack</span>
        <span itemprop="definition">enables a 2-level redundancy level for implementing a functional safety aspect for the operation of the car.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">redundancy system</span>
        <span itemprop="definition">differentiates from schemes implementing fully separate processing pipelines for Radar and a Camera data streams processed using, for example, separate processing units, nevertheless utilizing a centralized fusion output at least at 100 ms versus other early sensor fusion schemes.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">state-of-the-art sensor fusion schemes</span>
        <span itemprop="definition">often rely on a low-level sensor fusion, e.g., fusing raw unprocessed camera-radar data at an early stage. Since the frame-rate of radar is often lower than the frame-rate of the camera, the fused data overall has lower frame-rate which leads to an increased latency in detecting objects. Moreover the computational complexity of processing both high-dimensional radar and high-dimensional camera on the same device is often prohibitive.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">each sensor data</span>
        <span itemprop="definition">may be individually processed on separate devices. That way the camera detector (which has higher frame rate/lower latency) may act has an early-warning system, which is then complemented by the radar detector, which provides higher accuracy as well as range and velocity information.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such a fusion scheme</span>
        <span itemprop="definition">results in increased mean average precision (mAP) for particular relevant object classification and further early-warning tasks without requiring multiple 10 s of TOPs (trillion operations per second) of operation with expensive SoCs.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">mAP</span>
        <span itemprop="definition">mean average precision</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Average precision</span>
        <span itemprop="definition">refers to a metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Average precision</span>
        <span itemprop="definition">computes the average precision value for recall value over 0 to 1.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the sensor fusion scheme</span>
        <span itemprop="definition">is a combination of supervised and unsupervised machine learning algorithms.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">supervised learning</span>
        <span itemprop="definition">the system learns under supervision, where the supervision signal is named as target value or label.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a goal</span>
        <span itemprop="definition">may be to learn the mapping function, which refers to being able to understand how the input should be matched with output using available data.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">unsupervised learning</span>
        <span itemprop="definition">such signal is not available.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">clustering</span>
        <span itemprop="definition">is an unsupervised technique where the goal is to find natural groups or clusters in a feature space and interpret the input data.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Another example for unsupervised machine learning</span>
        <span itemprop="definition">may refer to dimensionality reduction where the goal is to reduce the number of random variables under consideration.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the radar sensor used here</span>
        <span itemprop="definition">may be a high-resolution radar, which provides a whole 3D (azimuth-range-velocity) data cube. Since processing this high-dimensional cube can be expensive, radar data may be processed first independently from image data.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the radar processing pipeline</span>
        <span itemprop="definition">extracts relevant information from the 3D cube, and outputs one or more 3D bounding boxes, as well as radar features.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a 3D data cube 300 output from a radar sensor stream</span>
        <span itemprop="definition">is shown.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the radar cube 300</span>
        <span itemprop="definition">is processed in order to extract ROIs. Detecting ROIs on the high-dimensional 3D cube can be an extremely computationally expensive step. To avoid any computational bottleneck, relevant 3D points are extracted from the 3D data cube, which acts as a data reduction step.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the 3D data cube 300</span>
        <span itemprop="definition">includes multiple objects, including a tree 320 , a street 330 , a car 340 and a flock of birds 350 . These objects are merely examples and other objects may be in the 3D data cube, as discussed with regard to FIGS. 1 and 2 .</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a preselection</span>
        <span itemprop="definition">occurs and a subset of objections is added to the point cloud 305 .</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the street 330</span>
        <span itemprop="definition">is not added to the point cloud, however, one of skill in the art will appreciate that other objects may be added or omitted by transferring data to the point cloud 305 .</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the point cloud 305 extraction operation</span>
        <span itemprop="definition">may be performed using a lightweight local maxima detector.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Local maxima detector in radar data</span>
        <span itemprop="definition">may be done using a Constant-False-Alarm-Rate (CFAR) algorithm, or other 2D or 3D filter-based algorithms using different kind of kernels, which can be prohibitively expensive.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">CFAR</span>
        <span itemprop="definition">Constant-False-Alarm-Rate</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DOA</span>
        <span itemprop="definition">Direction-of-Arrival</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the aspects of the present disclosure</span>
        <span itemprop="definition">may use a custom lightweight 3D CFAR algorithm that directly returns 3D points from the 3D data cube 300 .</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the algorithm</span>
        <span itemprop="definition">compares one or more cells with the statistics of all cells in each dimension, without relying on an expensive 3D filtering operation.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the resulting point cloud 305</span>
        <span itemprop="definition">may then be clustered as can be seen on the right side of FIG. 3 A , in order to identify high-density regions which are identified as regions of interest (ROIs).</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ROIs</span>
        <span itemprop="definition">regions of interest</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">aspects disclosed herein</span>
        <span itemprop="definition">may use DBSCAN as a clustering scheme, since it is fast and efficient, can cluster objects of arbitrarily large shapes, and is robust to outliers (i.e. noise and/or clutter points).</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Clusters</span>
        <span itemprop="definition">may then be identified as a ROI, and outliers are discarded.</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">corresponding small 3D cubes</span>
        <span itemprop="definition">such as 3D cubes 360 and 370 for the car 340 and the tree 320 , respectively, are extracted (not just the point cloud 305 ).</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the rest of the (non-ROI) data</span>
        <span itemprop="definition">may discarded, as can be seen in FIG. 3 B .</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ROI cubes 360 and 370</span>
        <span itemprop="definition">may be classified as either car, pedestrian, truck, bicycle, or background, where background are objects that have been detected but do not correspond to road-users (such as walls, trees or other static objects).</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a support vector machine</span>
        <span itemprop="definition">SVM</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">RCS</span>
        <span itemprop="definition">Radar Cross-Section</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">mean velocity</span>
        <span itemprop="definition">mean velocity</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">variance of velocity</span>
        <span itemprop="definition">etc.</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processed radar sensor data</span>
        <span itemprop="definition">may then be used for fusion with camera sensor data.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Yolo</span>
        <span itemprop="definition">a deep learning-based image detection algorithm may be used to detect objects on an image captured by a camera.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the Yolo algorithm</span>
        <span itemprop="definition">is known for relatively low computational time compared to other algorithms.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the algorithm</span>
        <span itemprop="definition">returns a list of 2D image bounding boxes, as well as a detection confidence for each bounding box, and a classification confidence for each class.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">each projected radar bounding box</span>
        <span itemprop="definition">may be estimated using the ROI azimuth width.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the pixel height</span>
        <span itemprop="definition">may be estimated by assuming that each object may be at ground level and may have a fixed height of 1.8 m.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the first step of the fusion pipeline</span>
        <span itemprop="definition">may be to match radar bounding boxes with camera bounding boxes.</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the Intersection-Over-Union (IOU)</span>
        <span itemprop="definition">may be computed between each radar/image bounding box pair, resulting in a matrix of pairwise IOUs.</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a pair</span>
        <span itemprop="definition">is a match, e.g., is coming from the same object, if the IOU is greater than a certain threshold. If a pair matching is ambiguous (e.g., a radar object being a potential match with many different image objects), a custom greedy algorithm may be used to identify the combination of pairs leading to the maximum sum of IOUs.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the algorithm</span>
        <span itemprop="definition">takes the pairwise IOU matrix as input and finds the optimal permutation matrix, e.g., the matrix such that each row/column has at most 1 non-zero element, and the sum of all elements is maximized.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the resulting fused object</span>
        <span itemprop="definition">may be classified using features computed from both sensors.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the output from the last layer of the image detector</span>
        <span itemprop="definition">may be used as image feature, which is concatenated with the radar features as described above.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the features from radar</span>
        <span itemprop="definition">(such as range and velocity) help resolve some cases where the camera-only system struggles to classify a target, for example for (partially) occluded objects, or in general by using the additional range and velocity information.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">some micro-doppler features</span>
        <span itemprop="definition">may help to identify and classify objects with many moving parts such as pedestrians or bicycles.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An SVM</span>
        <span itemprop="definition">may be used as a lightweight classification algorithm.</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the overall fusion algorithm</span>
        <span itemprop="definition">results in increased detection and classification performance, compared with camera-only algorithms, as well as providing essential range and velocity information for each detected object.</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">TOI models</span>
        <span itemprop="definition">utilizing an early fusion scheme outputting ROI coefficients can be run on combined information from various devices. For example, information of combined radar/camera features may be used to distinguish a person from a car in an overtake maneuver, as such relying on such a low-level and low-latency stack for proper operation.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such a TOI model</span>
        <span itemprop="definition">can be further trained based on reduced coefficients obtained from both the Radar and Camera 3D cubes coefficients and the output of an SVM based fusion scheme while maintaining both low-latency and low RAM requirements.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">This sensor fusion model</span>
        <span itemprop="definition">can also be used as a pre-labeling stack for radar and/or camera data, which is highly demanded for the development of safe and reliable autonomous systems.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ADAS</span>
        <span itemprop="definition">Advance Driver Assistance Systems</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AD</span>
        <span itemprop="definition">Autonomous Driving</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AR</span>
        <span itemprop="definition">Augmented Reality</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">UAV</span>
        <span itemprop="definition">Unmanned Aerial Vehicles</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ROI scheme</span>
        <span itemprop="definition">allows to extract relevant information, thus reducing the latency and data transmission rate to the sensor fusion module, without trading-off accuracy and detection rates.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">various early alert functions</span>
        <span itemprop="definition">such as overtake detectors rely on features captured by Radar and Camera data streams, as such rely on an early and reliable identification of agents on the road such as a pedestrian to avoid and to trigger an overtake scheme valid for a pedestrian.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a centralized fusion scheme with access to the raw Radar stream</span>
        <span itemprop="definition">is prohibitive as the incurred bandwidth requirements can easily surpass the 10 Gbit requirements, as such disabling the access to the data streams required for improved precision/recall values, thus requiring on the described low-level radar/camera fusion scheme.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">is a flow diagram for an exemplary method 400 of performing sensor data processing.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a general order of the operations for the method 400</span>
        <span itemprop="definition">is shown in FIG. 4 .</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 400</span>
        <span itemprop="definition">may include more or fewer steps or may arrange the order of the steps differently than those shown in FIG. 4 .</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 400</span>
        <span itemprop="definition">can be executed as a set of computer-executable instructions executed by a computer system and encoded or stored on a computer readable medium. Further, the method 400 can be performed by gates or circuits associated with a processor, an ASIC, an FPGA, a SOC or other hardware device.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processing</span>
        <span itemprop="definition">may be performed by a computer.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a first data stream</span>
        <span itemprop="definition">is received from a radar sensor.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the first data stream</span>
        <span itemprop="definition">comprises a 3D data cube, where a point in the 3D data cube 300 may include azimuth, range and velocity dimensions, as described above.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a point cloud</span>
        <span itemprop="definition">may be formed by extracting 3D points from the 3D data cube.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">forming of the point cloud by extracting 3D points from the 3D data cube</span>
        <span itemprop="definition">may be performed by using a lightweight local maximum detector comprising a 3D Constant-False-Alarm-Rate, CFAR, algorithm that directly returns 3D points from the 3D data cube.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">step S 430</span>
        <span itemprop="definition">clustering is performed in step S 430 .</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the clustering</span>
        <span itemprop="definition">is performed on the point cloud in order to identify high-density regions representing one or more ROIs.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Various processes for performing clustering</span>
        <span itemprop="definition">have been disclosed herein, however, one of skill in the art will appreciate that other processes for performing clustering can be employed without departing from the scope of this disclosure.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">one or more 3D bounding boxes</span>
        <span itemprop="definition">such as bounding boxes 360 and 370 are extracted from the 3D data cube 300 corresponding to the one or more ROIs and each ROI is classified. Data of the 3D data cube that is not included in the one or more 3D bounding boxes is discarded in this step.</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Various processes for identifying, generating, and/or extracting bounding boxes</span>
        <span itemprop="definition">have been disclosed herein, however, one of skill in the art will appreciate that other processes for can be employed without departing from the scope of this disclosure.</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a flow diagram for a method of performing sensor fusion with another sensor. This method may be performed following the method 400 as described in FIG. 4 .</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a general order of the operations for the method 500</span>
        <span itemprop="definition">is shown in FIG. 5 .</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 500</span>
        <span itemprop="definition">may include more or fewer steps or may arrange the order of the steps differently than those shown in FIG. 5 .</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 500</span>
        <span itemprop="definition">can be executed as a set of computer-executable instructions executed by a computer system and encoded or stored on a computer readable medium. Further, the method 500 can be performed by gates or circuits associated with a processor, an ASIC, an FPGA, a SOC or other hardware device.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 500</span>
        <span itemprop="definition">starts with receiving a second data stream from the camera sensor in step S 510 .</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the second data stream</span>
        <span itemprop="definition">comprises an image including pixel values from the camera sensor.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">step S 520</span>
        <span itemprop="definition">objects within the image are detected one or more 2D bounding boxes for one or more detected objects are determined.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An example of the object detection</span>
        <span itemprop="definition">is shown in FIG. 3 B , where multiple objects in the image 300 B are provided with 2D bounding boxes. While specific examples of object detection have been disclosed herein, one of skill in the art will appreciate that other processes for object detection may be employed with the aspects disclosed herein.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">one or more 3D bounding boxes from the 3D data cube</span>
        <span itemprop="definition">are projected onto the image, as discussed above and can be seen in FIG. 3 B . While specific processes for projecting bounding boxes have been disclosed herein, one of skill in the art will appreciate that other processes can be employed without departing from the scope of the disclosure.</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">step S 540</span>
        <span itemprop="definition">the 3D bounding boxes are matched with the 2D bounding boxes in step S 540 .</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an intersection over union (IOU) algorithm</span>
        <span itemprop="definition">may be used. While specific processes for matching have been disclosed herein, one of skill in the art will appreciate that other processes can be employed without departing from the scope of the disclosure.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">aspects disclosed herein</span>
        <span itemprop="definition">provides an improved camera/radar sensor fusion, with application to road user detection in the context of Autonomous Driving/Assisted Driving (ADAS), reducing the latency and data transmission rate to the sensor fusion module, without trading-off accuracy and detection rates.</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ADAS</span>
        <span itemprop="definition">Autonomous Driving/Assisted Driving</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">shows various detection metrics for the proposed radar object detector according to embodiments, such as accuracy, precision, recall and f1 score.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">These radar detections</span>
        <span itemprop="definition">may be fed to the sensor fusion module to further improve the performance.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">feeding these radar detections into the sensor fusion module</span>
        <span itemprop="definition">may be part of a feedback loop for training the model. For example, objects with low detection or classification confidence may be sent to a remote server for model re-training, in order to improve the model performance. That way, future occurrences of similar objects may have higher detection or classification accuracy.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">illustrates a simplified block diagram of a device with which aspects of the present disclosure may be practiced in accordance with aspects of the present disclosure.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the device</span>
        <span itemprop="definition">may be a server computer, a mobile computing device, or a set-top-box, for example.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more of the present embodiments</span>
        <span itemprop="definition">may be implemented in an operating environment 600 . This is only one example of a suitable operating environment and is not intended to suggest any limitation as to the scope of use or functionality.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operating environment 600</span>
        <span itemprop="definition">typically includes at least one processing unit 602 and memory 604 .</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">memory 604</span>
        <span itemprop="definition">instructions for processing data streams from one or more sensors as disclosed herein</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">memory 604</span>
        <span itemprop="definition">may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.), or some combination of the two.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">This most basic configuration</span>
        <span itemprop="definition">is illustrated in FIG. 6 by dashed line 606 .</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operating environment 600</span>
        <span itemprop="definition">may also include storage devices (removable, 608 , and/or non-removable, 610 ) including, but not limited to, magnetic or optical disks or tape.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the removable storage 608</span>
        <span itemprop="definition">includes a subscriber card (e.g., a smart card and a subscriber identification module (SIM) card).</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operating environment 600</span>
        <span itemprop="definition">may also have input device(s) 614 such as remote controller, keyboard, mouse, pen, voice input, on-board sensors, etc. and/or output device(s) 616 such as a display, speakers, printer, motors, etc.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">input device(s) 614</span>
        <span itemprop="definition">such as remote controller, keyboard, mouse, pen, voice input, on-board sensors, etc.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">output device(s) 616</span>
        <span itemprop="definition">such as a display, speakers, printer, motors, etc.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Also included in the environment</span>
        <span itemprop="definition">may be one or more communication connections, 612 , such as LAN, WAN, a near-field communications network, a cellular broadband network, point to point, etc.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Operating environment 600</span>
        <span itemprop="definition">typically includes at least some form of computer readable media.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computer readable media</span>
        <span itemprop="definition">can be any available media that can be accessed by processing unit 602 or other devices comprising the operating environment.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computer readable media</span>
        <span itemprop="definition">may comprise computer storage media and communication media.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computer storage media</span>
        <span itemprop="definition">includes volatile and nonvolatile, removable and non-removable non-transitory media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computer storage media</span>
        <span itemprop="definition">includes, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other tangible, non-transitory medium which can be used to store the desired information.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computer storage media</span>
        <span itemprop="definition">does not include communication media.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computer storage media</span>
        <span itemprop="definition">does not include a carrier wave or other propagated or modulated data signal.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Communication media</span>
        <span itemprop="definition">embodies computer readable instructions, data structures, program modules, or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">modulated data signal</span>
        <span itemprop="definition">means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">communication media</span>
        <span itemprop="definition">includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operating environment 600</span>
        <span itemprop="definition">may be a single computer operating in a networked environment using logical connections to one or more remote computers.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the remote computer</span>
        <span itemprop="definition">may be a personal computer, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above as well as others not so mentioned.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the logical connections</span>
        <span itemprop="definition">may include any method supported by available communications media.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such networking environments</span>
        <span itemprop="definition">are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
    </ul>
  </section>

  


  <section itemprop="abstract" itemscope>
    <h2>Abstract</h2>
    
    <div itemprop="content" html><abstract mxw-id="PA556420069" lang="EN" source="national office" load-source="docdb">
    <div class="abstract">Aspects described herein provide sensor data stream processing for enabling camera/radar sensor fusion, with application to road user detection in the context of Autonomous Driving/Assisted Driving (ADAS). In particular, a scheme to extract Region-of-Interests (ROI) from a high-resolution, high-dimensional radar data cube that can then be transmitted to a sensor fusion unit is described. The ROI scheme allows to extract relevant information, thus reducing the latency and data transmission rate to the sensor fusion module, without trading-off accuracy and detection rates. The sensor data stream processing comprises receiving a first data stream from a radar sensor, forming a point cloud by extracting 3D points from the 3D data cube, performing clustering on the point cloud in order to identify high-density regions representing one or ROIs, and extracting one or more 3D bounding boxes from the 3D data cube corresponding to the one or more ROIs and classifying each ROI.</div>
  </abstract>
  </div>
  </section>

  <section itemprop="description" itemscope>
    <h2>Description</h2>
    
    <div itemprop="content" html><ul mxw-id="PDES375877136" lang="EN" load-source="patent-office" class="description">
    
    <heading id="h-0001">BACKGROUND</heading>
    <li> <para-num num="[0001]"> </para-num> <div id="p-0002" num="0001" class="description-line">With the rise of remote and autonomic driving, the amount of image data which is streamed is ever increasing. In many cases, recording images by physical sensors, such as LIDAR, radar, camera, etc. is indispensable.</div>
    </li> <li> <para-num num="[0002]"> </para-num> <div id="p-0003" num="0002" class="description-line">It is with respect to these and other general considerations that the aspects disclosed herein have been made. Also, although relatively specific problems may be discussed, it should be understood that the examples should not be limited to solving the specific problems identified in the background or elsewhere in this disclosure.</div>
    </li> <heading id="h-0002">SUMMARY</heading>
    <li> <para-num num="[0003]"> </para-num> <div id="p-0004" num="0003" class="description-line">A computer-implemented method and system described herein provide sensor data processing. In exemplary embodiments, methods of enabling low level sensor fusion by lightweight semantic segmentation on sensors generating point cloud as generated from LIDAR, radar, cameras and Time-of-Flight sensors are described.</div>
    </li> <li> <para-num num="[0004]"> </para-num> <div id="p-0005" num="0004" class="description-line">For example, a method for camera-radar sensor fusion, with application to road user detection in the context of Autonomous Driving/Assisted Driving (ADAS) is provided. More specifically, a scheme to extract Region-of-Interests (ROI) from a high-resolution, high-dimensional radar data cube is provided, that can then be transmitted to a sensor fusion unit.</div>
    </li> <li> <para-num num="[0005]"> </para-num> <div id="p-0006" num="0005" class="description-line">According to the aspects disclosed herein, a computer-implemented method for sensor data stream processing comprises receiving a first data stream from a radar sensor, forming a point cloud by extracting 3D points from the 3D data cube, and extracting one or more 3D bounding boxes from the 3D data cube corresponding to the one or more ROIs and classifying each ROI. The first data stream comprises a 3D data cube including azimuth, range and velocity dimensions. Data of the 3D data cube that is not included in the one or more 3D bounding boxes is discarded after classification.</div>
    </li> <li> <para-num num="[0006]"> </para-num> <div id="p-0007" num="0006" class="description-line">According to an embodiment the method further comprises sensor fusion with a camera sensor, comprising receiving a second data stream from the camera sensor, detecting objects within the image and determining one or more 2D bounding boxes for each detected object, projecting the one or more 3D bounding boxes from the 3D data cube onto the image, and matching said one or more 3D bounding boxes with said one or more 2D bounding boxes. The second data stream comprises an image including pixel values from the camera sensor.</div>
    </li> <li> <para-num num="[0007]"> </para-num> <div id="p-0008" num="0007" class="description-line">According to an embodiment the method further comprises classifying a fused object once a matched pair of 2D and 3D bounding boxes is identified. Classifying uses features computed from the radar sensor and the camera sensor.</div>
    </li> <li> <para-num num="[0008]"> </para-num> <div id="p-0009" num="0008" class="description-line">According to an embodiment, matching comprises computing an Intersection-Over-Union (IOU) between each 2D/3D bounding box pair, resulting in a matrix of pairwise IOUs, wherein a pair is a match if the IOU corresponding to the pair is greater than a threshold.</div>
    </li> <li> <para-num num="[0009]"> </para-num> <div id="p-0010" num="0009" class="description-line">According to an embodiment, forming of the point cloud by extracting 3D points from the 3D data cube is performed by using a lightweight local maximum detector comprising a custom 3D Constant-False-Alarm-Rate, CFAR, algorithm that directly returns 3D points from the 3D data cube.</div>
    </li> <li> <para-num num="[0010]"> </para-num> <div id="p-0011" num="0010" class="description-line">According to an embodiment, classifying comprises for each 3D bounding box, computing custom features utilizing a support vector machine, SVM, wherein said custom features comprise at least one of spatial shape, radar cross section, RCS, mean velocity and variance of velocity.</div>
    </li> <li> <para-num num="[0011]"> </para-num> <div id="p-0012" num="0011" class="description-line">According to an embodiment, information from sensor fusion is used for running time of interest, TOI, models relying on low latency sensor data processing.</div>
    </li> <li> <para-num num="[0012]"> </para-num> <div id="p-0013" num="0012" class="description-line">In further examples, a computer-readable medium and a sensor data processing system corresponding to the above embodiments are provided.</div>
    </li> <li> <para-num num="[0013]"> </para-num> <div id="p-0014" num="0013" class="description-line">The above described embodiments can be combined with each other. The above described embodiments may also be implemented on a computer-readable medium comprising computer-readable instructions, that, when executed by a processor, cause the processor to perform the above described steps.</div>
    </li> <li> <para-num num="[0014]"> </para-num> <div id="p-0015" num="0014" class="description-line">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.</div>
    
    
    </li> <description-of-drawings>
      <heading id="h-0003">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <li> <para-num num="[0015]"> </para-num> <div id="p-0016" num="0015" class="description-line">Non-limiting and non-exhaustive examples are described with reference to the following figures.</div>
      </li> <li> <para-num num="[0016]"> </para-num> <div id="p-0017" num="0016" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> is a diagram illustrating a system comprising a vehicle, a corresponding camera, another vehicle and a cloud environment;</div>
      </li> <li> <para-num num="[0017]"> </para-num> <div id="p-0018" num="0017" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> illustrates an exemplary environment that may be dealt with a vehicle implementing the herein described concepts. The environment comprises an exemplary scene including a traffic sign, a pedestrian, a street, a vehicle, a cyclist, trees and sky;</div>
      </li> <li> <para-num num="[0018]"> </para-num> <div id="p-0019" num="0018" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>3</b>A</figref> is an exemplary concept diagram for extracting 3D points from a 3D data cube and clustering the point cloud;</div>
      </li> <li> <para-num num="[0019]"> </para-num> <div id="p-0020" num="0019" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>3</b>B</figref> is an exemplary concept diagram for sensor fusion;</div>
      </li> <li> <para-num num="[0020]"> </para-num> <div id="p-0021" num="0020" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref> is a flow diagram for a method of performing sensor data processing;</div>
      </li> <li> <para-num num="[0021]"> </para-num> <div id="p-0022" num="0021" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref> is a flow diagram for a method of performing sensor fusion; and</div>
      </li> <li> <para-num num="[0022]"> </para-num> <div id="p-0023" num="0022" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> shows various detection metrics for a radar object detector.</div>
      </li> <li> <para-num num="[0023]"> </para-num> <div id="p-0024" num="0023" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref> is a block diagram illustrating example physical components of a computing device with which aspects of the disclosure may be practiced.</div>
    </li> </description-of-drawings>
    
    
    <heading id="h-0004">DETAILED DESCRIPTION</heading>
    <li> <para-num num="[0024]"> </para-num> <div id="p-0025" num="0024" class="description-line">Various aspects of the disclosure are described more fully below with reference to the accompanying drawings, which from a part hereof, and which show specific example aspects. However, different aspects of the disclosure may be implemented in many different ways and should not be construed as limited to the aspects set forth herein; rather, these aspects are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the aspects to those skilled in the art. Aspects may be practiced as methods, systems or devices. Accordingly, aspects may take the form of a hardware implementation, an entirely software implementation or an implementation combining software and hardware aspects. The following detailed description is, therefore, not to be taken in a limiting sense.</div>
    </li> <li> <para-num num="[0025]"> </para-num> <div id="p-0026" num="0025" class="description-line">Aspects of the present disclosure relate to systems and methods for processing data streams from different sensors of a vehicle. In particular, processing that is performed on images that are recorded by a radar and camera of a vehicle. With the rise of remote and autonomic driving, the amount of image data which is streamed is ever increasing. In many cases, recording images by physical sensors, such as LIDAR, radar, camera, etc. which are integrated into vehicles (or which can be removably attached to vehicles) is valuable.</div>
    </li> <li> <para-num num="[0026]"> </para-num> <div id="p-0027" num="0026" class="description-line">The development of smart-mobility/ADAS systems require accurate and robust perception schemes, in order to sense the surrounding environment. In particular, detecting other road users such as cars, pedestrians, bicycles and others is of the utmost importance for many applications such as path planning, warning systems, emergency breaking etc.</div>
    </li> <li> <para-num num="[0027]"> </para-num> <div id="p-0028" num="0027" class="description-line">Examples of such sensors include cameras, radars or LIDARs, each of those sensors having different advantages and limitations know to one of skill in the art. Cameras, for example, provide good lateral resolution and are able to sense color and texture information, which makes it ideal for object detection tasks. However, cameras only provide a 2D projection of a 3D world, and as such do not provide any range or distance information, and are not able to detect occluded objects.</div>
    </li> <li> <para-num num="[0028]"> </para-num> <div id="p-0029" num="0028" class="description-line">Moreover, camera-based perception systems are not robust in bad weather conditions (fog, rain, snow), or low-light conditions (dark or night time). Radar systems on the other hand, work well in adverse environments (night, rain, fog), and provide range and velocity environments. However, radars suffer from a lower spatial resolution, and clutter/noise, which makes it more challenging to detect or classify small objects.</div>
    </li> <li> <para-num num="[0029]"> </para-num> <div id="p-0030" num="0029" class="description-line">For increased performance, recent radar sensors now provide a high-resolution, high-dimensional 3D data cube, which allows more advanced object detection/classification algorithms to be run. The challenge with high-dimensional radars however is the increased data transmission and processing cost, and thus the increased latency.</div>
    </li> <li> <para-num num="[0030]"> </para-num> <div id="p-0031" num="0030" class="description-line">These sensors produce high data rates. For example, high resolution cameras with 1080p and 4 k resolution that produce large amounts of image data are commonly used. However, data transmission and especially data processing is limited by available bandwidth and processing power. This can render applications impossible, which rely on near or real-time image transmission/processing and reliable object detection.</div>
    </li> <li> <para-num num="[0031]"> </para-num> <div id="p-0032" num="0031" class="description-line">Reference will now be made in detail to the exemplary embodiments, examples of which are illustrated in the accompanying drawings, therein like reference numerals reference to like elements throughout.</div>
    </li> <li> <para-num num="[0032]"> </para-num> <div id="p-0033" num="0032" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> illustrates a <figure-callout id="100" label="system" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">system</figure-callout> <b>100</b> including a <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b>, a set of <figure-callout id="120" label="multiple sensors" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">multiple sensors</figure-callout> <b>120</b> of the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b>, another <figure-callout id="130" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>130</b>, and a <figure-callout id="140" label="cloud environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">cloud environment</figure-callout> <b>140</b>. The set of <figure-callout id="120" label="multiple sensors" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">multiple sensors</figure-callout> <b>120</b> may include a camera, a LIDAR, a radar, a time-of-flight device and other sensors and devices that may be used for observing the environment of the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b>.</div>
    </li> <li> <para-num num="[0033]"> </para-num> <div id="p-0034" num="0033" class="description-line">The <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b> may further comprise a processor configured to receive data from the <figure-callout id="120" label="multiple sensors" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">multiple sensors</figure-callout> <b>120</b> and to process the data before encoding the data. In one embodiment this data may be data from the radar sensor. The <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b> may further comprise a memory for saving the encoded image. In addition, the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b> may further comprise an autonomous driving system that may be communicatively coupled to the processor of the vehicle and that may receive the encoded image.</div>
    </li> <li> <para-num num="[0034]"> </para-num> <div id="p-0035" num="0034" class="description-line">The autonomous driving system may use the encoded data for autonomously driving the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b>. The <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b> may comprise one or more further sensors, such as a distance sensor and a temperature sensor. The <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b> may be further communicatively coupled to another <figure-callout id="130" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>130</b> and/or a <figure-callout id="140" label="cloud environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">cloud environment</figure-callout> <b>140</b>. The <figure-callout id="120" label="multiple sensors" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">multiple sensors</figure-callout> <b>120</b> may be integrated anywhere in the vehicle <b>110</b> (e.g., next to a headlight, a rearview mirror, etc.) or may comprise sensors that can be attached and removed from the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b>.</div>
    </li> <li> <para-num num="[0035]"> </para-num> <div id="p-0036" num="0035" class="description-line">The <figure-callout id="130" label="other vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">other vehicle</figure-callout> <b>130</b> may also comprise different sensors (not shown) for observing the environment, a processor, a memory, and/or an autonomous driving system. Likewise, the processor of the <figure-callout id="130" label="other vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">other vehicle</figure-callout> <b>130</b> may also be configured to process an image by filtering the image before encoding the image, as described herein. The <figure-callout id="140" label="cloud environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">cloud environment</figure-callout> <b>140</b> may include a cloud storage for storing the encoded image. The <figure-callout id="140" label="cloud environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">cloud environment</figure-callout> <b>140</b>, may be communicatively coupled to a remote driving system that may be used to control the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b> from remote by a remote driver.</div>
    </li> <li> <para-num num="[0036]"> </para-num> <div id="p-0037" num="0036" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> illustrates an <figure-callout id="200" label="exemplary environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">exemplary environment</figure-callout> <b>200</b> that may exist around the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b>. The <figure-callout id="200" label="environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">environment</figure-callout> <b>200</b> may comprise one or more objects. The <figure-callout id="200" label="exemplary environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">exemplary environment</figure-callout> <b>200</b> of <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> illustrates several regions <b>201</b>-<b>207</b> that display several objects including a <figure-callout id="201" label="traffic sign" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">traffic sign</figure-callout> <b>201</b>, a <figure-callout id="202" label="pedestrian" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">pedestrian</figure-callout> <b>202</b>, a <figure-callout id="203" label="street" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">street</figure-callout> <b>203</b>, a <figure-callout id="204" label="car" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">car</figure-callout> <b>204</b>, a <figure-callout id="205" label="cyclist" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">cyclist</figure-callout> <b>205</b>, two <figure-callout id="206A" label="trees" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">trees</figure-callout> <b>206</b>A, <b>206</b>B and <figure-callout id="207" label="sky" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">sky</figure-callout> <b>207</b>. It is apparent that it may be possible to define more regions comprising further objects such as cyclist way, lane marker, or cloud which are also present in <figure-callout id="200" label="environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">environment</figure-callout> <b>200</b>.</div>
    </li> <li> <para-num num="[0037]"> </para-num> <div id="p-0038" num="0037" class="description-line">The <figure-callout id="200" label="environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">environment</figure-callout> <b>200</b> may be a representative environment with which aspects of the present disclosure may be confronted. In exemplary environment, the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b> is depicted as a car, however, aspects of the present disclosure is not limited to be implemented by cars, but can be employed with other systems, vehicles and devices may be used for implementing the herein disclosed concepts. Other examples of <figure-callout id="110" label="vehicles" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicles</figure-callout> <b>110</b> may be a drone or a delivery robot.</div>
    </li> <li> <para-num num="[0038]"> </para-num> <div id="p-0039" num="0038" class="description-line">Consequently, the <figure-callout id="200" label="environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">environment</figure-callout> <b>200</b> may look quite different based on the <figure-callout id="110" label="vehicle" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">vehicle</figure-callout> <b>110</b> implementing the herein disclosed concepts. For example, in case of a drone, the <figure-callout id="200" label="environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">environment</figure-callout> <b>200</b> may comprise other drones and obstacles in the air, such as birds, wind turbines, buildings, aircrafts, etc.</div>
    </li> <li> <para-num num="[0039]"> </para-num> <div id="p-0040" num="0039" class="description-line">The herein disclosed techniques pertain to a concept of enabling low level sensor fusion by lightweight semantic segmentation on sensors generating point clouds as generated from LIDAR, radar, camera, and/or Time-of-Flight sensors when capturing/observing the <figure-callout id="200" label="environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">environment</figure-callout> <b>200</b>. In this regard, Regions of Interest (ROIs) within the <figure-callout id="200" label="environment" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">environment</figure-callout> <b>200</b> are detected by the <figure-callout id="120" label="sensors" filenames="US20230008015A1-20230112-D00001.png" state="{{state}}">sensors</figure-callout> <b>120</b>, generating point clouds as data points in space and then using the point clouds to fuse with other sensor modalities such as cameras to enable real-time sensor fusion.</div>
    </li> <li> <para-num num="[0040]"> </para-num> <div id="p-0041" num="0040" class="description-line">As previously described, different sensors have certain shortcomings. To overcome the shortcomings of individual sensors, a camera-radar based sensor fusion scheme is described herein, using high-dimensional camera and radars, while retaining the latency low enough to match safety requirements. Sensor fusion even in early warning system on a chip (SoC) (e.g., 1-5 ms timeframe) and not only in the centralized device fusing the results from various camera/Radar detections along the car is currently an important topic of interest.</div>
    </li> <li> <para-num num="[0041]"> </para-num> <div id="p-0042" num="0041" class="description-line">Aspects disclosed herein can provide an advantage over existing centralized sensor fusion schemes that require the fusion, for example, within a 100 ms timeframe. According to a non-limiting example, an earlier trigger could already be triggered at the camera/radar SoC level tuning the required frame-rates of all camera/radar data streams.</div>
    </li> <li> <para-num num="[0042]"> </para-num> <div id="p-0043" num="0042" class="description-line">According to the aspects described herein, transmission of 3D cubes for a radar/camera cube and performing centralized fusion of the 3D cubes, results in a reduction in computational requirements when centralized fusion is triggered. A further aspect is the ability to obtain outputs of the camera/radar sensor fusion stack and the corresponding locations of particular objects and thus the fusion of these (x,y,z) object tuples in the centralized sensor fusion unit.</div>
    </li> <li> <para-num num="[0043]"> </para-num> <div id="p-0044" num="0043" class="description-line">This enables to make use of a multiple redundancy system between the camera/radar SoC triggering an early frame-rate adjustment+x,y,z, object fusion scheme or an equally redundant camera/radar cube ROIs in the centralized sensor fusion unit in cases of low differentiation capabilities at the SoC between e.g. a person and a car (e.g. triggered by a &lt;90% confidence trigger). As such, a low-level sensor fusion stack enables a 2-level redundancy level for implementing a functional safety aspect for the operation of the car. To note that such a redundancy system differentiates from schemes implementing fully separate processing pipelines for Radar and a Camera data streams processed using, for example, separate processing units, nevertheless utilizing a centralized fusion output at least at 100 ms versus other early sensor fusion schemes.</div>
    </li> <li> <para-num num="[0044]"> </para-num> <div id="p-0045" num="0044" class="description-line">For example, state-of-the-art sensor fusion schemes often rely on a low-level sensor fusion, e.g., fusing raw unprocessed camera-radar data at an early stage. Since the frame-rate of radar is often lower than the frame-rate of the camera, the fused data overall has lower frame-rate which leads to an increased latency in detecting objects. Moreover the computational complexity of processing both high-dimensional radar and high-dimensional camera on the same device is often prohibitive.</div>
    </li> <li> <para-num num="[0045]"> </para-num> <div id="p-0046" num="0045" class="description-line">An advantage of the aspects disclosed here is, among other benefits, that each sensor data may be individually processed on separate devices. That way the camera detector (which has higher frame rate/lower latency) may act has an early-warning system, which is then complemented by the radar detector, which provides higher accuracy as well as range and velocity information. Such a fusion scheme results in increased mean average precision (mAP) for particular relevant object classification and further early-warning tasks without requiring multiple 10 s of TOPs (trillion operations per second) of operation with expensive SoCs. Average precision refers to a metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision computes the average precision value for recall value over 0 to 1.</div>
    </li> <li> <para-num num="[0046]"> </para-num> <div id="p-0047" num="0046" class="description-line">The sensor fusion scheme according to aspects of the present disclosure is a combination of supervised and unsupervised machine learning algorithms. In supervised learning, the system learns under supervision, where the supervision signal is named as target value or label. For example, in supervised learning, a goal may be to learn the mapping function, which refers to being able to understand how the input should be matched with output using available data. In unsupervised learning, such signal is not available. For example, clustering is an unsupervised technique where the goal is to find natural groups or clusters in a feature space and interpret the input data. Another example for unsupervised machine learning may refer to dimensionality reduction where the goal is to reduce the number of random variables under consideration.</div>
    </li> <li> <para-num num="[0047]"> </para-num> <div id="p-0048" num="0047" class="description-line">The radar sensor used here may be a high-resolution radar, which provides a whole 3D (azimuth-range-velocity) data cube. Since processing this high-dimensional cube can be expensive, radar data may be processed first independently from image data. The radar processing pipeline extracts relevant information from the 3D cube, and outputs one or more 3D bounding boxes, as well as radar features.</div>
    </li> <li> <para-num num="[0048]"> </para-num> <div id="p-0049" num="0048" class="description-line">An example of this process is described with regard to <figref idrefs="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>.</div>
    </li> <li> <para-num num="[0049]"> </para-num> <div id="p-0050" num="0049" class="description-line">According to <figref idrefs="DRAWINGS">FIG. <b>3</b>A</figref>, a <figure-callout id="300" label="3D data cube" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">3D data cube</figure-callout> <b>300</b> output from a radar sensor stream is shown. The <figure-callout id="300" label="radar cube" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">radar cube</figure-callout> <b>300</b> is processed in order to extract ROIs. Detecting ROIs on the high-dimensional 3D cube can be an extremely computationally expensive step. To avoid any computational bottleneck, relevant 3D points are extracted from the 3D data cube, which acts as a data reduction step.</div>
    </li> <li> <para-num num="[0050]"> </para-num> <div id="p-0051" num="0050" class="description-line">As can be seen in <figref idrefs="DRAWINGS">FIG. <b>3</b>A</figref>, the <figure-callout id="300" label="3D data cube" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">3D data cube</figure-callout> <b>300</b> includes multiple objects, including a <figure-callout id="320" label="tree" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">tree</figure-callout> <b>320</b>, a <figure-callout id="330" label="street" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">street</figure-callout> <b>330</b>, a <figure-callout id="340" label="car" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">car</figure-callout> <b>340</b> and a flock of <figure-callout id="350" label="birds" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">birds</figure-callout> <b>350</b>. These objects are merely examples and other objects may be in the 3D data cube, as discussed with regard to <figref idrefs="DRAWINGS">FIGS. <b>1</b> and <b>2</b> </figref>.</div>
    </li> <li> <para-num num="[0051]"> </para-num> <div id="p-0052" num="0051" class="description-line">After extraction of the <figure-callout id="305" label="point cloud" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">point cloud</figure-callout> <b>305</b>, a preselection occurs and a subset of objections is added to the <figure-callout id="305" label="point cloud" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">point cloud</figure-callout> <b>305</b>. As can be seen, the <figure-callout id="330" label="street" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">street</figure-callout> <b>330</b> is not added to the point cloud, however, one of skill in the art will appreciate that other objects may be added or omitted by transferring data to the <figure-callout id="305" label="point cloud" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">point cloud</figure-callout> <b>305</b>.</div>
    </li> <li> <para-num num="[0052]"> </para-num> <div id="p-0053" num="0052" class="description-line">The <figure-callout id="305" label="point cloud" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">point cloud</figure-callout> <b>305</b> extraction operation may be performed using a lightweight local maxima detector. Local maxima detector in radar data may be done using a Constant-False-Alarm-Rate (CFAR) algorithm, or other 2D or 3D filter-based algorithms using different kind of kernels, which can be prohibitively expensive. CFAR is typically done on 2D range-Doppler maps, the azimuth information being then computed using an additional Direction-of-Arrival (DOA) estimation step. However, this may lead to missed or incorrect detections when several targets are in the same range-Doppler bin.</div>
    </li> <li> <para-num num="[0053]"> </para-num> <div id="p-0054" num="0053" class="description-line">To overcome the limitations of 2D-CFAR, the aspects of the present disclosure may use a custom lightweight 3D CFAR algorithm that directly returns 3D points from the <figure-callout id="300" label="3D data cube" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">3D data cube</figure-callout> <b>300</b>. The algorithm compares one or more cells with the statistics of all cells in each dimension, without relying on an expensive 3D filtering operation.</div>
    </li> <li> <para-num num="[0054]"> </para-num> <div id="p-0055" num="0054" class="description-line">The resulting <figure-callout id="305" label="point cloud" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">point cloud</figure-callout> <b>305</b> may then be clustered as can be seen on the right side of <figref idrefs="DRAWINGS">FIG. <b>3</b>A</figref>, in order to identify high-density regions which are identified as regions of interest (ROIs). According to an aspect, aspects disclosed herein may use DBSCAN as a clustering scheme, since it is fast and efficient, can cluster objects of arbitrarily large shapes, and is robust to outliers (i.e. noise and/or clutter points).</div>
    </li> <li> <para-num num="[0055]"> </para-num> <div id="p-0056" num="0055" class="description-line">Clusters may then be identified as a ROI, and outliers are discarded. Once an ROI has been detected, corresponding small 3D cubes, such as <figure-callout id="360" label="3D cubes" filenames="US20230008015A1-20230112-D00002.png,US20230008015A1-20230112-D00003.png" state="{{state}}"> <figure-callout id="370" label="3D cubes" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">3D cubes</figure-callout> </figure-callout> <b>360</b> and <b>370</b> for the <figure-callout id="340" label="car" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">car</figure-callout> <b>340</b> and the <figure-callout id="320" label="tree" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">tree</figure-callout> <b>320</b>, respectively, are extracted (not just the point cloud <b>305</b>). The rest of the (non-ROI) data may discarded, as can be seen in <figref idrefs="DRAWINGS">FIG. <b>3</b>B</figref>.</div>
    </li> <li> <para-num num="[0056]"> </para-num> <div id="p-0057" num="0056" class="description-line">Using 3D cubes rather than point clouds provides, among other benefits, significant improvements for road-user detection and classification. However, unlike existing systems that use a deep convolutional-based approach for object classification after the ROI detection operation, which is not suitable for real-time inference, aspects disclosed herein provide a lightweight 3D cube classification. For example, <figure-callout id="360" label="ROI cubes" filenames="US20230008015A1-20230112-D00002.png,US20230008015A1-20230112-D00003.png" state="{{state}}"> <figure-callout id="370" label="ROI cubes" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">ROI cubes</figure-callout> </figure-callout> <b>360</b> and <b>370</b> may be classified as either car, pedestrian, truck, bicycle, or background, where background are objects that have been detected but do not correspond to road-users (such as walls, trees or other static objects).</div>
    </li> <li> <para-num num="[0057]"> </para-num> <div id="p-0058" num="0057" class="description-line">This allows for the removal of false positives that have been detected in the ROI detection operation. For classification, a support vector machine (SVM), a supervised learning algorithm which is computationally less expensive than deep learning algorithms may be used. For each <figure-callout id="360" label="ROI cube" filenames="US20230008015A1-20230112-D00002.png,US20230008015A1-20230112-D00003.png" state="{{state}}"> <figure-callout id="370" label="ROI cube" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">ROI cube</figure-callout> </figure-callout> <b>360</b>, <b>370</b>, custom features such as spatial shape, Radar Cross-Section (RCS), mean velocity, variance of velocity etc. may be computed. The classifier then returns a label corresponding to each class.</div>
    </li> <li> <para-num num="[0058]"> </para-num> <div id="p-0059" num="0058" class="description-line">The processed radar sensor data may then be used for fusion with camera sensor data. For image-based detection, Yolo, a deep learning-based image detection algorithm may be used to detect objects on an image captured by a camera. The Yolo algorithm is known for relatively low computational time compared to other algorithms. The algorithm returns a list of 2D image bounding boxes, as well as a detection confidence for each bounding box, and a classification confidence for each class.</div>
    </li> <li> <para-num num="[0059]"> </para-num> <div id="p-0060" num="0059" class="description-line">This is illustrated in <figref idrefs="DRAWINGS">FIG. <b>3</b>B</figref>, where the same scenario as from the radar sensor is captured by a camera sensor and output as <figure-callout id="300B" label="image" filenames="US20230008015A1-20230112-D00003.png" state="{{state}}">image</figure-callout> <b>300</b>B. To fuse radar objects, such as <figure-callout id="360" label="objects" filenames="US20230008015A1-20230112-D00002.png,US20230008015A1-20230112-D00003.png" state="{{state}}"> <figure-callout id="370" label="objects" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">objects</figure-callout> </figure-callout> <b>360</b> and <b>370</b> with image objects, the <figure-callout id="3D" label="radar" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">radar</figure-callout> <figure-callout id="360" label="3D bounding boxes" filenames="US20230008015A1-20230112-D00002.png,US20230008015A1-20230112-D00003.png" state="{{state}}"> <figure-callout id="370" label="3D bounding boxes" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">3D bounding boxes</figure-callout> </figure-callout> <b>360</b> and <b>370</b> are projected onto the <figure-callout id="300B" label="image" filenames="US20230008015A1-20230112-D00003.png" state="{{state}}">image</figure-callout> <b>300</b>B. This may be done after careful calibration of the camera/radar system, and estimation of the camera intrinsic and extrinsic. The pixel width of each projected radar bounding box may be estimated using the ROI azimuth width. The pixel height may be estimated by assuming that each object may be at ground level and may have a fixed height of 1.8 m.</div>
    </li> <li> <para-num num="[0060]"> </para-num> <div id="p-0061" num="0060" class="description-line">The first step of the fusion pipeline may be to match radar bounding boxes with camera bounding boxes. For that, the Intersection-Over-Union (IOU) may be computed between each radar/image bounding box pair, resulting in a matrix of pairwise IOUs.</div>
    </li> <li> <para-num num="[0061]"> </para-num> <div id="p-0062" num="0061" class="description-line">It may be determined that a pair is a match, e.g., is coming from the same object, if the IOU is greater than a certain threshold. If a pair matching is ambiguous (e.g., a radar object being a potential match with many different image objects), a custom greedy algorithm may be used to identify the combination of pairs leading to the maximum sum of IOUs. The algorithm takes the pairwise IOU matrix as input and finds the optimal permutation matrix, e.g., the matrix such that each row/column has at most 1 non-zero element, and the sum of all elements is maximized.</div>
    </li> <li> <para-num num="[0062]"> </para-num> <div id="p-0063" num="0062" class="description-line">Once a matched pair of radar/image bounding boxes is identified, the resulting fused object may be classified using features computed from both sensors. In this regard, the output from the last layer of the image detector may be used as image feature, which is concatenated with the radar features as described above. In particular, the features from radar (such as range and velocity) help resolve some cases where the camera-only system struggles to classify a target, for example for (partially) occluded objects, or in general by using the additional range and velocity information. For example, some micro-doppler features may help to identify and classify objects with many moving parts such as pedestrians or bicycles.</div>
    </li> <li> <para-num num="[0063]"> </para-num> <div id="p-0064" num="0063" class="description-line">An SVM may be used as a lightweight classification algorithm. The overall fusion algorithm results in increased detection and classification performance, compared with camera-only algorithms, as well as providing essential range and velocity information for each detected object.</div>
    </li> <li> <para-num num="[0064]"> </para-num> <div id="p-0065" num="0064" class="description-line">Further time-of-interest (TOI) models utilizing an early fusion scheme outputting ROI coefficients can be run on combined information from various devices. For example, information of combined radar/camera features may be used to distinguish a person from a car in an overtake maneuver, as such relying on such a low-level and low-latency stack for proper operation. Such a TOI model can be further trained based on reduced coefficients obtained from both the Radar and <figure-callout id="3D" label="Camera" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">Camera</figure-callout> 3D cubes coefficients and the output of an SVM based fusion scheme while maintaining both low-latency and low RAM requirements. This sensor fusion model can also be used as a pre-labeling stack for radar and/or camera data, which is highly demanded for the development of safe and reliable autonomous systems.</div>
    </li> <li> <para-num num="[0065]"> </para-num> <div id="p-0066" num="0065" class="description-line">The aspects disclosed herein may be deployed in a variety of multi-sensor applications such as Advance Driver Assistance Systems (ADAS), Autonomous Driving (AD), Robotics, Augmented Reality (AR) or Unmanned Aerial Vehicles (UAV).</div>
    </li> <li> <para-num num="[0066]"> </para-num> <div id="p-0067" num="0066" class="description-line">In examples, the ROI scheme allows to extract relevant information, thus reducing the latency and data transmission rate to the sensor fusion module, without trading-off accuracy and detection rates. Furthermore, various early alert functions such as overtake detectors rely on features captured by Radar and Camera data streams, as such rely on an early and reliable identification of agents on the road such as a pedestrian to avoid and to trigger an overtake scheme valid for a pedestrian. Moreover, a centralized fusion scheme with access to the raw Radar stream is prohibitive as the incurred bandwidth requirements can easily surpass the 10 Gbit requirements, as such disabling the access to the data streams required for improved precision/recall values, thus requiring on the described low-level radar/camera fusion scheme.</div>
    </li> <li> <para-num num="[0067]"> </para-num> <div id="p-0068" num="0067" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref> is a flow diagram for an <figure-callout id="400" label="exemplary method" filenames="US20230008015A1-20230112-D00004.png" state="{{state}}">exemplary method</figure-callout> <b>400</b> of performing sensor data processing. A general order of the operations for the <figure-callout id="400" label="method" filenames="US20230008015A1-20230112-D00004.png" state="{{state}}">method</figure-callout> <b>400</b> is shown in <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref>. The <figure-callout id="400" label="method" filenames="US20230008015A1-20230112-D00004.png" state="{{state}}">method</figure-callout> <b>400</b> may include more or fewer steps or may arrange the order of the steps differently than those shown in <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref>. The <figure-callout id="400" label="method" filenames="US20230008015A1-20230112-D00004.png" state="{{state}}">method</figure-callout> <b>400</b> can be executed as a set of computer-executable instructions executed by a computer system and encoded or stored on a computer readable medium. Further, the <figure-callout id="400" label="method" filenames="US20230008015A1-20230112-D00004.png" state="{{state}}">method</figure-callout> <b>400</b> can be performed by gates or circuits associated with a processor, an ASIC, an FPGA, a SOC or other hardware device. The processing may be performed by a computer. In step S<b>410</b>, a first data stream is received from a radar sensor. The first data stream comprises a 3D data cube, where a point in the <figure-callout id="300" label="3D data cube" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">3D data cube</figure-callout> <b>300</b> may include azimuth, range and velocity dimensions, as described above.</div>
    </li> <li> <para-num num="[0068]"> </para-num> <div id="p-0069" num="0068" class="description-line">In step S<b>420</b>, a point cloud may be formed by extracting 3D points from the 3D data cube. As discussed herein, forming of the point cloud by extracting 3D points from the 3D data cube may be performed by using a lightweight local maximum detector comprising a 3D Constant-False-Alarm-Rate, CFAR, algorithm that directly returns 3D points from the 3D data cube.</div>
    </li> <li> <para-num num="[0069]"> </para-num> <div id="p-0070" num="0069" class="description-line">After the point cloud is extracted, clustering is performed in step S<b>430</b>. The clustering is performed on the point cloud in order to identify high-density regions representing one or more ROIs. Various processes for performing clustering have been disclosed herein, however, one of skill in the art will appreciate that other processes for performing clustering can be employed without departing from the scope of this disclosure.</div>
    </li> <li> <para-num num="[0070]"> </para-num> <div id="p-0071" num="0070" class="description-line">At step S<b>440</b> one or more 3D bounding boxes, such as bounding <figure-callout id="360" label="boxes" filenames="US20230008015A1-20230112-D00002.png,US20230008015A1-20230112-D00003.png" state="{{state}}"> <figure-callout id="370" label="boxes" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">boxes</figure-callout> </figure-callout> <b>360</b> and <b>370</b> are extracted from the <figure-callout id="300" label="3D data cube" filenames="US20230008015A1-20230112-D00000.png,US20230008015A1-20230112-D00002.png" state="{{state}}">3D data cube</figure-callout> <b>300</b> corresponding to the one or more ROIs and each ROI is classified. Data of the 3D data cube that is not included in the one or more 3D bounding boxes is discarded in this step. Various processes for identifying, generating, and/or extracting bounding boxes have been disclosed herein, however, one of skill in the art will appreciate that other processes for can be employed without departing from the scope of this disclosure.</div>
    </li> <li> <para-num num="[0071]"> </para-num> <div id="p-0072" num="0071" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref> is a flow diagram for a method of performing sensor fusion with another sensor. This method may be performed following the <figure-callout id="400" label="method" filenames="US20230008015A1-20230112-D00004.png" state="{{state}}">method</figure-callout> <b>400</b> as described in <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref>. A general order of the operations for the <figure-callout id="500" label="method" filenames="US20230008015A1-20230112-D00005.png" state="{{state}}">method</figure-callout> <b>500</b> is shown in <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref>. The <figure-callout id="500" label="method" filenames="US20230008015A1-20230112-D00005.png" state="{{state}}">method</figure-callout> <b>500</b> may include more or fewer steps or may arrange the order of the steps differently than those shown in <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref>. The <figure-callout id="500" label="method" filenames="US20230008015A1-20230112-D00005.png" state="{{state}}">method</figure-callout> <b>500</b> can be executed as a set of computer-executable instructions executed by a computer system and encoded or stored on a computer readable medium. Further, the <figure-callout id="500" label="method" filenames="US20230008015A1-20230112-D00005.png" state="{{state}}">method</figure-callout> <b>500</b> can be performed by gates or circuits associated with a processor, an ASIC, an FPGA, a SOC or other hardware device. The <figure-callout id="500" label="method" filenames="US20230008015A1-20230112-D00005.png" state="{{state}}">method</figure-callout> <b>500</b> starts with receiving a second data stream from the camera sensor in step S<b>510</b>. The second data stream comprises an image including pixel values from the camera sensor.</div>
    </li> <li> <para-num num="[0072]"> </para-num> <div id="p-0073" num="0072" class="description-line">At step S<b>520</b> objects within the image are detected one or more 2D bounding boxes for one or more detected objects are determined. An example of the object detection is shown in <figref idrefs="DRAWINGS">FIG. <b>3</b>B</figref>, where multiple objects in the <figure-callout id="300B" label="image" filenames="US20230008015A1-20230112-D00003.png" state="{{state}}">image</figure-callout> <b>300</b>B are provided with 2D bounding boxes. While specific examples of object detection have been disclosed herein, one of skill in the art will appreciate that other processes for object detection may be employed with the aspects disclosed herein.</div>
    </li> <li> <para-num num="[0073]"> </para-num> <div id="p-0074" num="0073" class="description-line">At step S<b>530</b>, one or more 3D bounding boxes from the 3D data cube are projected onto the image, as discussed above and can be seen in <figref idrefs="DRAWINGS">FIG. <b>3</b>B</figref>. While specific processes for projecting bounding boxes have been disclosed herein, one of skill in the art will appreciate that other processes can be employed without departing from the scope of the disclosure.</div>
    </li> <li> <para-num num="[0074]"> </para-num> <div id="p-0075" num="0074" class="description-line">Then, the 3D bounding boxes are matched with the 2D bounding boxes in step S<b>540</b>. For this matching, an intersection over union (IOU) algorithm may be used. While specific processes for matching have been disclosed herein, one of skill in the art will appreciate that other processes can be employed without departing from the scope of the disclosure.</div>
    </li> <li> <para-num num="[0075]"> </para-num> <div id="p-0076" num="0075" class="description-line">Aspects disclosed herein provides an improved camera/radar sensor fusion, with application to road user detection in the context of Autonomous Driving/Assisted Driving (ADAS), reducing the latency and data transmission rate to the sensor fusion module, without trading-off accuracy and detection rates.</div>
    </li> <li> <para-num num="[0076]"> </para-num> <div id="p-0077" num="0076" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> shows various detection metrics for the proposed radar object detector according to embodiments, such as accuracy, precision, recall and f1 score. These radar detections may be fed to the sensor fusion module to further improve the performance. According to aspects disclosed herein, feeding these radar detections into the sensor fusion module may be part of a feedback loop for training the model. For example, objects with low detection or classification confidence may be sent to a remote server for model re-training, in order to improve the model performance. That way, future occurrences of similar objects may have higher detection or classification accuracy.</div>
    </li> <li> <para-num num="[0077]"> </para-num> <div id="p-0078" num="0077" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> illustrates a simplified block diagram of a device with which aspects of the present disclosure may be practiced in accordance with aspects of the present disclosure. The device may be a server computer, a mobile computing device, or a set-top-box, for example. One or more of the present embodiments may be implemented in an operating environment <b>600</b>. This is only one example of a suitable operating environment and is not intended to suggest any limitation as to the scope of use or functionality. Other well-known computing systems, environments, and/or configurations that may be suitable for use include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, programmable consumer electronics such as smartphones, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.</div>
    </li> <li> <para-num num="[0078]"> </para-num> <div id="p-0079" num="0078" class="description-line">In its most basic configuration, the operating environment <b>600</b> typically includes at least one processing unit <b>602</b> and memory <b>604</b>. Depending on the exact configuration and type of computing device, memory <b>604</b> (instructions for processing data streams from one or more sensors as disclosed herein) may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.), or some combination of the two. This most basic configuration is illustrated in <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> by dashed line <b>606</b>. Further, the operating environment <b>600</b> may also include storage devices (removable, <b>608</b>, and/or non-removable, <b>610</b>) including, but not limited to, magnetic or optical disks or tape. In some aspects, the removable storage <b>608</b> includes a subscriber card (e.g., a smart card and a subscriber identification module (SIM) card). Similarly, the operating environment <b>600</b> may also have input device(s) <b>614</b> such as remote controller, keyboard, mouse, pen, voice input, on-board sensors, etc. and/or output device(s) <b>616</b> such as a display, speakers, printer, motors, etc. Also included in the environment may be one or more communication connections, <b>612</b>, such as LAN, WAN, a near-field communications network, a cellular broadband network, point to point, etc.</div>
    </li> <li> <para-num num="[0079]"> </para-num> <div id="p-0080" num="0079" class="description-line">Operating environment <b>600</b> typically includes at least some form of computer readable media. Computer readable media can be any available media that can be accessed by processing unit <b>602</b> or other devices comprising the operating environment. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes volatile and nonvolatile, removable and non-removable non-transitory media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other tangible, non-transitory medium which can be used to store the desired information. Computer storage media does not include communication media. Computer storage media does not include a carrier wave or other propagated or modulated data signal.</div>
    </li> <li> <para-num num="[0080]"> </para-num> <div id="p-0081" num="0080" class="description-line">Communication media embodies computer readable instructions, data structures, program modules, or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.</div>
    </li> <li> <para-num num="[0081]"> </para-num> <div id="p-0082" num="0081" class="description-line">The operating environment <b>600</b> may be a single computer operating in a networked environment using logical connections to one or more remote computers. The remote computer may be a personal computer, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above as well as others not so mentioned. The logical connections may include any method supported by available communications media. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.</div>
    </li> <li> <para-num num="[0082]"> </para-num> <div id="p-0083" num="0082" class="description-line">The description and illustration of one or more aspects provided in this application are not intended to limit or restrict the scope of the disclosure as claimed in any way. The aspects, examples, and details provided in this application are considered sufficient to convey possession and enable others to make and use the best mode of claimed disclosure. The claimed disclosure should not be construed as being limited to any aspect, for example, or detail provided in this application. Regardless of whether shown and described in combination or separately, the various features (both structural and methodological) are intended to be selectively included or omitted to produce an embodiment with a particular set of features. Having been provided with the description and illustration of the present application, one skilled in the art may envision variations, modifications, and alternate aspects falling within the spirit of the broader aspects of the general inventive concept embodied in this application that do not depart from the broader scope of the claimed disclosure.</div>
    
  </li> </ul>
  </div>
  </section>

  <section itemprop="claims" itemscope>
    <h2>Claims (<span itemprop="count">20</span>)</h2>
    
    <div itemprop="content" html><div mxw-id="PCLM371389370" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement>
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text"> <b>1</b>. A computer-implemented method for sensor data stream processing, the method comprising:
<div class="claim-text">receiving a first data stream from a radar sensor, wherein the first data stream comprises a 3D data cube including azimuth, range and velocity dimensions;</div> <div class="claim-text">forming a point cloud by extracting 3D points from the 3D data cube;</div> <div class="claim-text">performing clustering on the point cloud in order to identify high-density regions representing one or more regions of interest, ROIs; and</div> <div class="claim-text">extracting one or more 3D bounding boxes from the 3D data cube corresponding to the one or more ROIs and classifying the one or more ROIs, wherein data of the 3D data cube that is not included in the one or more 3D bounding boxes is discarded.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text"> <b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises performing sensor fusion with a camera sensor, and wherein the method further comprises:
<div class="claim-text">receiving a second data stream from the camera sensor, wherein the second data stream comprises an image including pixel values from the camera sensor;</div> <div class="claim-text">detecting objects within the image and determining one or more 2D bounding boxes for each detected object;</div> <div class="claim-text">projecting the one or more 3D bounding boxes from the 3D data cube onto the image; and</div> <div class="claim-text">matching said one or more 3D bounding boxes with said one or more 2D bounding boxes.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text"> <b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:
<div class="claim-text">classifying a fused object once a matched pair of 2D and 3D bounding boxes is identified, wherein classifying uses features computed from the radar sensor and the camera sensor.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text"> <b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein matching comprises computing an Intersection-Over-Union, IOU, between each 2D/3D bounding box pair, resulting in a matrix of pairwise IOUs, wherein a pair is a match if the IOU corresponding to the pair is greater than a threshold.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text"> <b>5</b>. The computer-implemented method of any of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein forming of the point cloud by extracting 3D points from the 3D data cube is performed by using a lightweight local maximum detector comprising a custom 3D Constant-False-Alarm-Rate, CFAR, algorithm that directly returns 3D points from the 3D data cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text"> <b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein classifying the one or more ROIs comprises:
<div class="claim-text">for the one or more 3D bounding boxes, computing custom features utilizing a support vector machine, SVM, wherein said custom features comprise at least one of spatial shape, radar cross section, RCS, mean velocity and variance of velocity.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text"> <b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein information from sensor fusion is used for running time of interest (TOI) models relying on low latency sensor data processing.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text"> <b>8</b>. A non-transitory computer-readable medium comprising computer-readable instructions, that, when executed by a processor, cause the processor to perform a method comprising:
<div class="claim-text">receiving a first data stream from a radar sensor, wherein the first data stream comprises a 3D data cube including azimuth, range and velocity dimensions;</div> <div class="claim-text">forming a point cloud by extracting 3D points from the 3D data cube;</div> <div class="claim-text">performing clustering on the point cloud in order to identify high-density regions representing one or more regions of interest, ROIs; and</div> <div class="claim-text">extracting one or more 3D bounding boxes from the 3D data cube corresponding to the one or more ROIs and classifying the one or more ROIs, wherein data of the 3D data cube that is not included in the one or more 3D bounding boxes is discarded.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text"> <b>9</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the method further comprises performing sensor fusion with a camera sensor, and wherein the method further comprises:
<div class="claim-text">receiving a second data stream from the camera sensor, wherein the second data stream comprises an image including pixel values from the camera sensor;</div> <div class="claim-text">detecting objects within the image and determining one or more 2D bounding boxes for each detected object;</div> <div class="claim-text">projecting the one or more 3D bounding boxes from the 3D data cube onto the image; and</div> <div class="claim-text">matching said one or more 3D bounding boxes with said one or more 2D bounding boxes.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text"> <b>10</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein matching comprises computing an Intersection-Over-Union, IOU, between each 2D/3D bounding box pair, resulting in a matrix of pairwise IOUs, wherein a pair is a match if the IOU corresponding to the pair is greater than a threshold.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text"> <b>11</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein information from sensor fusion is used for running time of interest (TOI) models relying on low latency sensor data processing.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text"> <b>12</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein forming of the point cloud by extracting 3D points from the 3D data cube is performed by using a lightweight local maximum detector comprising a custom 3D Constant-False-Alarm-Rate, CFAR, algorithm that directly returns 3D points from the 3D data cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text"> <b>13</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein classifying the one or more ROIs comprises:
<div class="claim-text">for the one or more 3D bounding boxes, computing custom features utilizing a support vector machine, SVM, wherein said custom features comprise at least one of spatial shape, radar cross section, RCS, mean velocity and variance of velocity.</div> </div>
    </div>
    </div> <div class="claim"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text"> <b>14</b>. A sensor data processing system comprising a processing unit and a radar sensor, the processing unit being configured to:
<div class="claim-text">receive a first data stream from a radar sensor, wherein the first data stream comprises a 3D data cube including azimuth, range and velocity dimensions;</div> <div class="claim-text">form a point cloud by extracting 3D points from the 3D data cube;</div> <div class="claim-text">perform clustering on the point cloud in order to identify high-density regions representing one or more regions of interest, ROIs; and</div> <div class="claim-text">extract one or more 3D bounding boxes from the 3D data cube corresponding to the one or more ROIs and classify each ROI, wherein data of the 3D data cube that is not included in the one or more 3D bounding boxes is discarded.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text"> <b>15</b>. The sensor data processing system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the processing unit being further configured to perform sensor fusion with a camera sensor, by:
<div class="claim-text">receiving a second data stream from the camera sensor, wherein the second data stream comprises an image including pixel values from the camera sensor;</div> <div class="claim-text">detecting objects within the image and determining one or more 2D bounding boxes for each detected object;</div> <div class="claim-text">projecting the one or more 3D bounding boxes from the 3D data cube onto the image; and</div> <div class="claim-text">matching said one or more 3D bounding boxes with said one or more 2D bounding boxes.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text"> <b>16</b>. The sensor data processing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the processing unit being further configured to:
<div class="claim-text">classify a fused object once a matched pair of 2D and 3D bounding boxes is identified, wherein classifying uses features computed from the radar sensor and the camera sensor.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text"> <b>17</b>. The sensor data processing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein matching comprises computing an Intersection-Over-Union, IOU, between each 2D/3D bounding box pair, resulting in a matrix of pairwise IOUs, wherein a pair is a match if the IOU corresponding to the pair is greater than a threshold.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text"> <b>18</b>. The sensor data processing system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein forming of the point cloud by extracting 3D points from the 3D data cube is performed by using a lightweight local maximum detector comprising a custom 3D Constant-False-Alarm-Rate, CFAR, algorithm that directly returns 3D points from the 3D data cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text"> <b>19</b>. The sensor data processing system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein classifying the one or more ROIs comprises:
<div class="claim-text">for the one or more 3D bounding boxes, computing custom features utilizing a support vector machine, SVM, wherein said custom features comprise at least one of spatial shape, radar cross section (RCS) mean velocity and variance of velocity.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text"> <b>20</b>. The sensor data processing system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein information from sensor fusion is used for running time of interest (TOI) models relying on low latency sensor data processing.</div>
    </div>
  </div> </div>
  </div>
  </section>

  <section itemprop="application" itemscope>

    <section itemprop="metadata" itemscope>
      <span itemprop="applicationNumber">US17/373,358</span>
      <span itemprop="priorityDate">2021-07-12</span>
      <span itemprop="filingDate">2021-07-12</span>
      <span itemprop="title">Sensor fusion architecture for low-latency accurate road user detection 
       </span>
      <span itemprop="ifiStatus">Abandoned</span>
      
      <a href="/patent/US20230008015A1/en">
        <span itemprop="representativePublication">US20230008015A1</span>
        (<span itemprop="primaryLanguage">en</span>)
      </a>
    </section>

    <h2>Priority Applications (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="priorityApps" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US17/373,358</span>
            
            <a href="/patent/US20230008015A1/en">
              <span itemprop="representativePublication">US20230008015A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2021-07-12</td>
          <td itemprop="filingDate">2021-07-12</td>
          <td itemprop="title">Sensor fusion architecture for low-latency accurate road user detection 
       </td>
        </tr>
        <tr itemprop="priorityApps" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">EP22184339.4A</span>
            
            <a href="/patent/EP4120204A1/en">
              <span itemprop="representativePublication">EP4120204A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2021-07-12</td>
          <td itemprop="filingDate">2022-07-12</td>
          <td itemprop="title">Sensor fusion architecture for low-latency accurate road user detection 
       </td>
        </tr>
      </tbody>
    </table>

    <h2>Applications Claiming Priority (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US17/373,358</span>
            <a href="/patent/US20230008015A1/en">
              <span itemprop="representativePublication">US20230008015A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2021-07-12</td>
          <td itemprop="filingDate">2021-07-12</td>
          <td itemprop="title">Sensor fusion architecture for low-latency accurate road user detection 
       </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Publications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Publication Number</th>
          <th>Publication Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="pubs" itemscope repeat>
          <td>
            <span itemprop="publicationNumber">US20230008015A1</span>
            
            <span itemprop="thisPatent">true</span>
            <a href="/patent/US20230008015A1/en">
              US20230008015A1
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-01-12</td>
        </tr>
      </tbody>
    </table>

  </section>

  <section itemprop="family" itemscope>
    <h1>Family</h1>
    <h2>ID=82846533</h2>

    <h2>Family Applications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Title</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="applications" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US17/373,358</span>
            <span itemprop="ifiStatus">Abandoned</span>
            
            <a href="/patent/US20230008015A1/en">
              <span itemprop="representativePublication">US20230008015A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2021-07-12</td>
          <td itemprop="filingDate">2021-07-12</td>
          <td itemprop="title">Sensor fusion architecture for low-latency accurate road user detection 
       </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Country Status (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Country</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">US</span>
            (<span itemprop="num">1</span>)
            <meta itemprop="thisCountry" content="true">
          </td>
          <td>
            <a href="/patent/US20230008015A1/en">
              <span itemprop="representativePublication">US20230008015A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">EP</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/EP4120204A1/en">
              <span itemprop="representativePublication">EP4120204A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Citations (4)</h2>
    <table>
      <caption>* Cited by examiner,  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US10593042B1/en">
              <span itemprop="publicationNumber">US10593042B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-04-11</td>
          <td itemprop="publicationDate">2020-03-17</td>
          <td>
            <span itemprop="assigneeOriginal">Zoox, Inc.</span>
          </td>
          <td itemprop="title">Perspective conversion for multi-dimensional data analysis 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11393097B2/en">
              <span itemprop="publicationNumber">US11393097B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-08</td>
          <td itemprop="publicationDate">2022-07-19</td>
          <td>
            <span itemprop="assigneeOriginal">Qualcomm Incorporated</span>
          </td>
          <td itemprop="title">Using light detection and ranging (LIDAR) to train camera and imaging radar deep learning networks 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20220308205A1/en">
              <span itemprop="publicationNumber">US20220308205A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-03-25</td>
          <td itemprop="publicationDate">2022-09-29</td>
          <td>
            <span itemprop="assigneeOriginal">Aptiv Technologies Limited</span>
          </td>
          <td itemprop="title">Partially-Learned Model for Speed Estimates in Radar Tracking 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11630197B2/en">
              <span itemprop="publicationNumber">US11630197B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-04</td>
          <td itemprop="publicationDate">2023-04-18</td>
          <td>
            <span itemprop="assigneeOriginal">Qualcomm Incorporated</span>
          </td>
          <td itemprop="title">Determining a motion state of a target object 
       </td>
        </tr>
      </tbody>
    </table>

    <h2>Family Cites Families (1)</h2>
    <table>
      <caption>* Cited by examiner,  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/US11468582B2/en">
              <span itemprop="publicationNumber">US11468582B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-03-16</td>
          <td itemprop="publicationDate">2022-10-11</td>
          <td><span itemprop="assigneeOriginal">Nvidia Corporation</span></td>
          <td itemprop="title">Leveraging multidimensional sensor data for computationally efficient object detection for autonomous machine applications 
       </td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2021</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2021-07-12</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US17/373,358</span>
            <a href="/patent/US20230008015A1/en"><span itemprop="documentId">patent/US20230008015A1/en</span></a>
            <span itemprop="legalStatusCat">not_active</span>
            <span itemprop="legalStatus">Abandoned</span>
            <span itemprop="thisApp" content="true" bool></span>
          </li>
        </ul>
      </li>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2022</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2022-07-12</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP22184339.4A</span>
            <a href="/patent/EP4120204A1/en"><span itemprop="documentId">patent/EP4120204A1/en</span></a>
            <span itemprop="legalStatusCat">not_active</span>
            <span itemprop="legalStatus">Withdrawn</span>
            
          </li>
        </ul>
      </li>
    </ul>

    </section>

  <section>
    <h2>Patent Citations (4)</h2>
    <table>
      <caption>* Cited by examiner,  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US10593042B1/en">
              <span itemprop="publicationNumber">US10593042B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-04-11</td>
          <td itemprop="publicationDate">2020-03-17</td>
          <td><span itemprop="assigneeOriginal">Zoox, Inc.</span></td>
          <td itemprop="title">Perspective conversion for multi-dimensional data analysis 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US11630197B2/en">
              <span itemprop="publicationNumber">US11630197B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-04</td>
          <td itemprop="publicationDate">2023-04-18</td>
          <td><span itemprop="assigneeOriginal">Qualcomm Incorporated</span></td>
          <td itemprop="title">Determining a motion state of a target object 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US11393097B2/en">
              <span itemprop="publicationNumber">US11393097B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-08</td>
          <td itemprop="publicationDate">2022-07-19</td>
          <td><span itemprop="assigneeOriginal">Qualcomm Incorporated</span></td>
          <td itemprop="title">Using light detection and ranging (LIDAR) to train camera and imaging radar deep learning networks 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20220308205A1/en">
              <span itemprop="publicationNumber">US20220308205A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-03-25</td>
          <td itemprop="publicationDate">2022-09-29</td>
          <td><span itemprop="assigneeOriginal">Aptiv Technologies Limited</span></td>
          <td itemprop="title">Partially-Learned Model for Speed Estimates in Radar Tracking 
       </td>
        </tr>
      </tbody>
    </table>
  </section>

  

  

  <section>
    <h2>Also Published As</h2>
    <table>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Publication date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/EP4120204A1/en">
              <span itemprop="publicationNumber">EP4120204A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-01-18</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Similar Documents</h2>
    <table>
      <thead>
        <tr>
          <th>Publication</th>
          <th>Publication Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN111626217B/en">
                <span itemprop="publicationNumber">CN111626217B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-08-22">2023-08-22</time>
            
            
          </td>
          <td itemprop="title">Target detection and tracking method based on two-dimensional picture and three-dimensional point cloud fusion 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="10969647565527159492">
              <a href="/scholar/10969647565527159492"><span itemprop="scholarAuthors">Nabati et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2019">2019</time>
            
          </td>
          <td itemprop="title">Rrpn: Radar region proposal network for object detection in autonomous vehicles</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11113959B2/en">
                <span itemprop="publicationNumber">US11113959B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-09-07">2021-09-07</time>
            
            
          </td>
          <td itemprop="title">Crowdsourced detection, identification and sharing of hazardous road objects in HD maps 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN109086788B/en">
                <span itemprop="publicationNumber">CN109086788B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-09-20">2022-09-20</time>
            
            
          </td>
          <td itemprop="title">Apparatus, method and system for multi-mode fusion processing of data in multiple different formats sensed from heterogeneous devices 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10929713B2/en">
                <span itemprop="publicationNumber">US10929713B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-02-23">2021-02-23</time>
            
            
          </td>
          <td itemprop="title">Semantic visual landmarks for navigation 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/EP3651064B1/en">
                <span itemprop="publicationNumber">EP3651064B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2024-02-07">2024-02-07</time>
            
            
          </td>
          <td itemprop="title">Deep learning for object detection using pillars 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="15122472290265937583">
              <a href="/scholar/15122472290265937583"><span itemprop="scholarAuthors">Behley et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2013">2013</time>
            
          </td>
          <td itemprop="title">Laser-based segment classification using a mixture of bag-of-words</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11747444B2/en">
                <span itemprop="publicationNumber">US11747444B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-09-05">2023-09-05</time>
            
            
          </td>
          <td itemprop="title">LiDAR-based object detection and classification 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20220309685A1/en">
                <span itemprop="publicationNumber">US20220309685A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-09-29">2022-09-29</time>
            
            
          </td>
          <td itemprop="title">Neural network for object detection and tracking 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10325169B2/en">
                <span itemprop="publicationNumber">US10325169B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2019-06-18">2019-06-18</time>
            
            
          </td>
          <td itemprop="title">Spatio-temporal awareness engine for priority tree based region selection across multiple input cameras and multimodal sensor empowered awareness engine for target recovery and object path prediction 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/TW202244529A/en">
                <span itemprop="publicationNumber">TW202244529A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-11-16">2022-11-16</time>
            
            
          </td>
          <td itemprop="title">Object size estimation using camera map and/or radar information 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11830253B2/en">
                <span itemprop="publicationNumber">US11830253B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-11-28">2023-11-28</time>
            
            
          </td>
          <td itemprop="title">Semantically aware keypoint matching 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="14947749276055998305">
              <a href="/scholar/14947749276055998305"><span itemprop="scholarAuthors">Magnier et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2017">2017</time>
            
          </td>
          <td itemprop="title">Automotive LIDAR objects detection and classification algorithm using the belief theory</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN110781927B/en">
                <span itemprop="publicationNumber">CN110781927B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-05-23">2023-05-23</time>
            
            
          </td>
          <td itemprop="title">Target detection and classification method based on deep learning under vehicle-road cooperation 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20220269900A1/en">
                <span itemprop="publicationNumber">US20220269900A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-08-25">2022-08-25</time>
            
            
          </td>
          <td itemprop="title">Low level sensor fusion based on lightweight semantic segmentation of 3d point clouds 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN106599918B/en">
                <span itemprop="publicationNumber">CN106599918B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2019-12-17">2019-12-17</time>
            
            
          </td>
          <td itemprop="title">vehicle tracking method and system 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="11139919961148222799">
              <a href="/scholar/11139919961148222799"><span itemprop="scholarAuthors">Mekala et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2022">2022</time>
            
          </td>
          <td itemprop="title">Deep learning inspired object consolidation approaches using lidar data for autonomous driving: a review</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/EP4064127A1/en">
                <span itemprop="publicationNumber">EP4064127A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-09-28">2022-09-28</time>
            
            
          </td>
          <td itemprop="title">Methods and electronic devices for detecting objects in surroundings of a self-driving car 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="15089171818609672960">
              <a href="/scholar/15089171818609672960"><span itemprop="scholarAuthors">Fu et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2018">2018</time>
            
          </td>
          <td itemprop="title">Camera-based semantic enhanced vehicle segmentation for planar lidar</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="9795563962911645556">
              <a href="/scholar/9795563962911645556"><span itemprop="scholarAuthors">Luo et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2022">2022</time>
            
          </td>
          <td itemprop="title">Dynamic multitarget detection algorithm of voxel point cloud fusion based on pointrcnn</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="491230197443748625">
              <a href="/scholar/491230197443748625"><span itemprop="scholarAuthors">Kim et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2018">2018</time>
            
          </td>
          <td itemprop="title">MOD: Multi-camera based local position estimation for moving objects detection</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230008015A1/en">
                <span itemprop="publicationNumber">US20230008015A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-01-12">2023-01-12</time>
            
            
          </td>
          <td itemprop="title">Sensor fusion architecture for low-latency accurate road user detection 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="7400835382424314576">
              <a href="/scholar/7400835382424314576"><span itemprop="scholarAuthors">Saleh et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2020">2020</time>
            
          </td>
          <td itemprop="title">Towards robust perception depth information for collision avoidance</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="7533154221204431411">
              <a href="/scholar/7533154221204431411"><span itemprop="scholarAuthors">Zhang et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2018">2018</time>
            
          </td>
          <td itemprop="title">Spatial and temporal context information fusion based flying objects detection for autonomous sense and avoid</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="14368004781420316398">
              <a href="/scholar/14368004781420316398"><span itemprop="scholarAuthors">Madhumitha et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2020">2020</time>
            
          </td>
          <td itemprop="title">Estimation of collision priority on traffic videos using deep learning</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Legal Events</h2>
    <table>
      <thead>
        <tr>
          <th>Date</th>
          <th>Code</th>
          <th>Title</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-08-16">2021-08-16</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">TERAKI GMBH, GERMANY</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BODNARIUC, ECATERINA;RENCKER, LUCAS;RICHART, DANIEL LAMPERT;SIGNING DATES FROM 20210715 TO 20210716;REEL/FRAME:057186/0721</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-10-29">2021-10-29</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">DOCKETED NEW CASE - READY FOR EXAMINATION</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-04-21">2023-04-21</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NON FINAL ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-11-04">2023-11-04</time></td>
          <td itemprop="code">STCB</td>
          <td itemprop="title">Information on status: application discontinuation</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ABANDONED  --  FAILURE TO RESPOND TO AN OFFICE ACTION</span>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </section>

</article>

    </search-app>
    <script type="text/javascript" src="//www.gstatic.com/feedback/api.js"></script>
    <script type="text/javascript" src="//www.gstatic.com/feedback/js/help/prod/service/lazy.min.js"></script>
    <script type="text/javascript">
      if (window.help && window.help.service) {
        helpApi = window.help.service.Lazy.create(0, {apiKey: 'AIzaSyDTEI_0tLX4varJ7bwK8aT-eOI5qr3BmyI', locale: 'en-US'});
        window.requestedSurveys = new Set();
        window.requestSurvey = function(triggerId) {
          if (window.requestedSurveys.has(triggerId)) {
            return;
          }
          window.requestedSurveys.add(triggerId);
          helpApi.requestSurvey({
            triggerId: triggerId,
            enableTestingMode: false,
            callback: (requestSurveyCallbackParam) => {
              if (!requestSurveyCallbackParam.surveyData) {
                return;
              }
              helpApi.presentSurvey({
                productData: {
                  productVersion: window.version,
                  customData: {
                    "experiments": "72459301,72474719",
                  },
                },
                surveyData: requestSurveyCallbackParam.surveyData,
                colorScheme: 1,
                customZIndex: 10000,
              });
            }
          });
        };

        window.requestSurvey('YXTwAsvoW0kedxbuTdH0RArc9VhT');
      }
    </script>
    <script src="/sw/null_loader.js"></script>
  </body>
</html>
