<!DOCTYPE html>
<html lang="en">
  <head>
    <title>US11861853B1 - System and method of vehicle speed estimation using moving camera and time series neural network 
        - Google Patents</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <meta name="referrer" content="origin-when-crossorigin">
    <link rel="canonical" href="https://patents.google.com/patent/US11861853B1/en">
    <meta name="description" content="
     A system, apparatus and method of embedded ego vehicle speed estimation apparatus, including a car-mounted monocular camera for capturing a sequence of video frames of an outdoor scene from a moving car, where the outdoor scene includes a road, as a camera channel, and processing circuitry. The processing circuitry is configured with a single-shot network and a 3D convolutional neural network (3D-CNN), the single-shot network segments features of the road in the video frame sequence and generates a masked-attention map for the segmented road features, a concatenation circuit concatenates the masked-attention map as an additional channel to the camera channel to generate a masked-attention input, and the 3D-CNN network receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of the segmented road features in the video sequences. 
   
   ">
    <meta name="DC.type" content="patent">
    <meta name="DC.title" content="System and method of vehicle speed estimation using moving camera and time series neural network 
       ">
    <meta name="DC.date" content="2023-06-02" scheme="dateSubmitted">
    <meta name="DC.description" content="
     A system, apparatus and method of embedded ego vehicle speed estimation apparatus, including a car-mounted monocular camera for capturing a sequence of video frames of an outdoor scene from a moving car, where the outdoor scene includes a road, as a camera channel, and processing circuitry. The processing circuitry is configured with a single-shot network and a 3D convolutional neural network (3D-CNN), the single-shot network segments features of the road in the video frame sequence and generates a masked-attention map for the segmented road features, a concatenation circuit concatenates the masked-attention map as an additional channel to the camera channel to generate a masked-attention input, and the 3D-CNN network receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of the segmented road features in the video sequences. 
   
   ">
    <meta name="citation_patent_application_number" content="US:18/328,441">
    <meta name="citation_pdf_url" content="https://patentimages.storage.googleapis.com/be/3f/ce/e6e9363d889a67/US11861853.pdf">
    <meta name="citation_patent_number" content="US:11861853">
    <meta name="DC.date" content="2024-01-02" scheme="issue">
    <meta name="DC.contributor" content="Athul Mathew" scheme="inventor">
    <meta name="DC.contributor" content="Thariq Khalid" scheme="inventor">
    <meta name="DC.contributor" content="Riad Souissi" scheme="inventor">
    <meta name="DC.contributor" content="ELM" scheme="assignee">
    <meta name="DC.relation" content="US:20180024562:A1" scheme="references">
    <meta name="DC.relation" content="US:20190244366:A1" scheme="references">
    <meta name="DC.relation" content="US:11608083" scheme="references">
    <meta name="DC.relation" content="US:20210278852:A1" scheme="references">
    <meta name="DC.relation" content="US:20200324794:A1" scheme="references">
    <meta name="DC.relation" content="US:20220292291:A1" scheme="references">
    <meta name="citation_reference" content="Athul M. Mathew, et al., âEgo Vehicle Speed Estimation Using 3D Convolution With Masked Attentionâ, Computer Science &gt; Computer Vision and Pattern Recognition (cs.CV), arXiv:2212.05432v1 [cs.CV] Dec. 11, 2022, Dec. 13, 2022, pp. 1-13." scheme="references">
    <meta name="citation_reference" content="Bandari, Hitesh Linganna, and Binoy B. Nair. âAn End to End Learning based Ego Vehicle Speed Estimation System.â 2021 IEEE International Power and Renewable Energy Conference (IPRECON). IEEE, 2021." scheme="references">
    <meta name="citation_reference" content="Du Tran, et al., âLearning Spatiotemporal Features with 3D Convolutional Networksâ, IEEE International Conference On Computer Vision, Dec. 7-13, 2015, pp. 4489-4497." scheme="references">
    <meta name="citation_reference" content="Hayakawa, Jun, and Behzad Dariush. âEgo-motion and surrounding vehicle state estimation using a monocular camera.â 2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2019." scheme="references">
    <meta name="citation_reference" content="Wu, Chao-Yuan, et al. âLong-term feature banks for detailed video understanding.â Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019." scheme="references">
    <meta name="citation_reference" content="Yeon, Kyuhwan, et al. âEgo-vehicle speed prediction using a long short-term memory based recurrent neural network.â International Journal of Automotive Technology 20 (2019): 713-722." scheme="references">
    <meta name="citation_reference" content="Yi Zhou, et al., âTowards Deep Radar Perception for Autonomous Driving: Datasets, Methods, and Challengesâ, SENSORS, vol. 22, Issue 11, May 31, 2022, pp. 1-45." scheme="references">
    <meta name="citation_reference" content="Zhao, Baigan, et al. âEgo-motion estimation using recurrent convolutional neural networks through optical flow learning.â Electronics 10.3 (2021): 222." scheme="references">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Product+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700">

    <style>
      body { transition: none; }
    </style>

    <script>
      window.version = 'patent-search.search_20240108_RC01';

      function sendFeedback() {
        userfeedback.api.startFeedback({
          'productId': '713680',
          'bucket': 'patent-search-web',
          'productVersion': window.version,
        });
      }

      window.experiments = {};
      window.experiments.patentCountries = "ae,ag,al,am,ao,ap,ar,at,au,aw,az,ba,bb,bd,be,bf,bg,bh,bj,bn,bo,br,bw,bx,by,bz,ca,cf,cg,ch,ci,cl,cm,cn,co,cr,cs,cu,cy,cz,dd,de,dj,dk,dm,do,dz,ea,ec,ee,eg,em,ep,es,fi,fr,ga,gb,gc,gd,ge,gh,gm,gn,gq,gr,gt,gw,hk,hn,hr,hu,ib,id,ie,il,in,ir,is,it,jo,jp,ke,kg,kh,km,kn,kp,kr,kw,kz,la,lc,li,lk,lr,ls,lt,lu,lv,ly,ma,mc,md,me,mg,mk,ml,mn,mo,mr,mt,mw,mx,my,mz,na,ne,ng,ni,nl,no,nz,oa,om,pa,pe,pg,ph,pl,pt,py,qa,ro,rs,ru,rw,sa,sc,sd,se,sg,si,sk,sl,sm,sn,st,su,sv,sy,sz,td,tg,th,tj,tm,tn,tr,tt,tw,tz,ua,ug,us,uy,uz,vc,ve,vn,wo,yu,za,zm,zw";
      
      
      window.experiments.keywordWizard = true;
      
      
      
      window.experiments.definitions = true;

      window.Polymer = {
        dom: 'shady',
        lazyRegister: true,
      };
    </script>

    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/webcomponentsjs/webcomponents-lite.min.js"></script>
    <link rel="import" href="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.html">
  </head>
  <body unresolved>
    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.js"></script>
    <search-app>
      
      

      <article class="result" itemscope itemtype="http://schema.org/ScholarlyArticle">
  <h1 itemprop="pageTitle">US11861853B1 - System and method of vehicle speed estimation using moving camera and time series neural network 
        - Google Patents</h1>
  <span itemprop="title">System and method of vehicle speed estimation using moving camera and time series neural network 
       </span>

  <meta itemprop="type" content="patent">
  <a href="https://patentimages.storage.googleapis.com/be/3f/ce/e6e9363d889a67/US11861853.pdf" itemprop="pdfLink">Download PDF</a>
  <h2>Info</h2>

  <dl>
    <dt>Publication number</dt>
    <dd itemprop="publicationNumber">US11861853B1</dd>
    <meta itemprop="numberWithoutCodes" content="11861853">
    <meta itemprop="kindCode" content="B1">
    <meta itemprop="publicationDescription" content="Patent ( no pre-grant publication)">
    <span>US11861853B1</span>
    <span>US18/328,441</span>
    <span>US202318328441A</span>
    <span>US11861853B1</span>
    <span>US 11861853 B1</span>
    <span>US11861853 B1</span>
    <span>US 11861853B1</span>
    <span>  </span>
    <span> </span>
    <span> </span>
    <span>US 202318328441 A</span>
    <span>US202318328441 A</span>
    <span>US 202318328441A</span>
    <span>US 11861853 B1</span>
    <span>US11861853 B1</span>
    <span>US 11861853B1</span>

    <dt>Authority</dt>
    <dd itemprop="countryCode">US</dd>
    <dd itemprop="countryName">United States</dd>

    <dt>Prior art keywords</dt>
    <dd itemprop="priorArtKeywords" repeat>masked</dd>
    <dd itemprop="priorArtKeywords" repeat>features</dd>
    <dd itemprop="priorArtKeywords" repeat>attention</dd>
    <dd itemprop="priorArtKeywords" repeat>neural network</dd>
    <dd itemprop="priorArtKeywords" repeat>ego vehicle</dd>

    <dt>Prior art date</dt>
    <dd><time itemprop="priorArtDate" datetime="2022-11-17">2022-11-17</time></dd>

    <dt>Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)</dt>
    <dd itemprop="legalStatusIfi" itemscope>
      <span itemprop="status">Active</span>
    </dd>
  </dl>

  <dt>Application number</dt>
  <dd itemprop="applicationNumber">US18/328,441</dd>

  

  

  <dt>Inventor</dt>
  <dd itemprop="inventor" repeat>Athul Mathew</dd>
  <dd itemprop="inventor" repeat>Thariq Khalid</dd>
  <dd itemprop="inventor" repeat>Riad Souissi</dd>

  <dt>Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)</dt>
  <dd itemprop="assigneeCurrent" repeat>
    ELM
  </dd>

  <dt>Original Assignee</dt>
  <dd itemprop="assigneeOriginal" repeat>ELM</dd>

  <dt>Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)</dt>
  <dd><time itemprop="priorityDate" datetime="2022-11-17">2022-11-17</time></dd>

  <dt>Filing date</dt>
  <dd><time itemprop="filingDate" datetime="2023-06-02">2023-06-02</time></dd>

  <dt>Publication date</dt>
  <dd><time itemprop="publicationDate" datetime="2024-01-02">2024-01-02</time></dd>

  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-06-02">2023-06-02</time>
    <span itemprop="title">Application filed by ELM</span>
    <span itemprop="type">filed</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    <span itemprop="assigneeSearch">ELM</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-06-02">2023-06-02</time>
    <span itemprop="title">Priority to US18/328,441</span>
    <span itemprop="type">priority</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US11861853B1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-06-02">2023-06-02</time>
    <span itemprop="title">Assigned to ELM</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">ELM</span>
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    <span itemprop="description" repeat>Assignors: KHALID, THARIQ, SOUISSI, RIAD, MATHEW, ATHUL</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2024-01-02">2024-01-02</time>
    <span itemprop="title">Application granted</span>
    <span itemprop="type">granted</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2024-01-02">2024-01-02</time>
    <span itemprop="title">Publication of US11861853B1</span>
    <span itemprop="type">publication</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US11861853B1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date">Status</time>
    <span itemprop="title">Active</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    <span itemprop="current" content="true" bool>Current</span>
    
    
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2043-06-02">2043-06-02</time>
    <span itemprop="title">Anticipated expiration</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
  </dd>

  <h2>Links</h2>
  <ul>
    
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoLink">
          <a href="https://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&p=1&u=/netahtml/PTO/srchnum.html&r=1&f=G&l=50&d=PALL&s1=11861853.PN." itemprop="url" target="_blank"><span itemprop="text">USPTO</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoPatentCenterLink">
          <a href="https://patentcenter.uspto.gov/applications/18328441" itemprop="url" target="_blank"><span itemprop="text">USPTO PatentCenter</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoAssignmentLink">
          <a href="https://assignment.uspto.gov/patent/index.html#/patent/search/resultFilter?searchInput=11861853" itemprop="url" target="_blank"><span itemprop="text">USPTO Assignment</span></a>
        </li>

    <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="espacenetLink">
        <a href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=11861853B1&amp;KC=B1&amp;FT=D" itemprop="url" target="_blank"><span itemprop="text">Espacenet</span></a>
      </li>
      

    

    
      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="globalDossierLink">
        <a href="https://globaldossier.uspto.gov/#/result/patent/US/11861853/1" itemprop="url" target="_blank"><span itemprop="text">Global Dossier</span></a>
      </li>
      

      

      

      

      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="stackexchangeLink">
        <a href="https://patents.stackexchange.com/questions/tagged/US11861853" itemprop="url"><span itemprop="text">Discuss</span></a>
      </li>
      
  </ul>

  <ul itemprop="concept" itemscope>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000000034</span>
      <span itemprop="name">method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>title</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">20</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013528</span>
      <span itemprop="name">artificial neural network</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>title</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">31</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013527</span>
      <span itemprop="name">convolutional neural network</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">57</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012545</span>
      <span itemprop="name">processing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">37</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000006073</span>
      <span itemprop="name">displacement reaction</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">10</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000011218</span>
      <span itemprop="name">segmentation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">19</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000001514</span>
      <span itemprop="name">detection method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">11</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000011176</span>
      <span itemprop="name">pooling</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">9</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003860</span>
      <span itemprop="name">storage</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000006243</span>
      <span itemprop="name">chemical reaction</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001537</span>
      <span itemprop="name">neural effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000011156</span>
      <span itemprop="name">evaluation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">29</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002123</span>
      <span itemprop="name">temporal effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">19</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012360</span>
      <span itemprop="name">testing method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">17</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010586</span>
      <span itemprop="name">diagram</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">11</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000015654</span>
      <span itemprop="name">memory</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">10</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003909</span>
      <span itemprop="name">pattern recognition</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">10</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000033001</span>
      <span itemprop="name">locomotion</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">9</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004891</span>
      <span itemprop="name">communication</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">6</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005516</span>
      <span itemprop="name">engineering process</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">6</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006870</span>
      <span itemprop="name">function</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010801</span>
      <span itemprop="name">machine learning</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003287</span>
      <span itemprop="name">optical effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000008569</span>
      <span itemprop="name">process</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012549</span>
      <span itemprop="name">training</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013459</span>
      <span itemprop="name">approach</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">4</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000002679</span>
      <span itemprop="name">ablation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000009471</span>
      <span itemprop="name">action</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000009826</span>
      <span itemprop="name">distribution</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000007613</span>
      <span itemprop="name">environmental effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000002474</span>
      <span itemprop="name">experimental method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000011160</span>
      <span itemprop="name">research</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000007</span>
      <span itemprop="name">visual effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000008901</span>
      <span itemprop="name">benefit</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000284</span>
      <span itemprop="name">extract</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006872</span>
      <span itemprop="name">improvement</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000007726</span>
      <span itemprop="name">management method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010606</span>
      <span itemprop="name">normalization</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000737</span>
      <span itemprop="name">periodic effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002093</span>
      <span itemprop="name">peripheral effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004513</span>
      <span itemprop="name">sizing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001360</span>
      <span itemprop="name">synchronised effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">HPTJABJPZMULFH-UHFFFAOYSA-N</span>
      <span itemprop="name">12-[(Cyclohexylcarbamoyl)amino]dodecanoic acid</span>
      <span itemprop="domain">Chemical group</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles">OC(=O)CCCCCCCCCCCNC(=O)NC1CCCCC1</span>
      <span itemprop="inchi_key">HPTJABJPZMULFH-UHFFFAOYSA-N</span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">ORILYTVJVMAKLC-UHFFFAOYSA-N</span>
      <span itemprop="name">Adamantane</span>
      <span itemprop="domain">Natural products</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles">C1C(C2)CC3CC1CC2C3</span>
      <span itemprop="inchi_key">ORILYTVJVMAKLC-UHFFFAOYSA-N</span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">OKTJSMMVPCPJKN-UHFFFAOYSA-N</span>
      <span itemprop="name">Carbon</span>
      <span itemprop="domain">Chemical compound</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles">[C]</span>
      <span itemprop="inchi_key">OKTJSMMVPCPJKN-UHFFFAOYSA-N</span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">241001465754</span>
      <span itemprop="name">Metazoa</span>
      <span itemprop="domain">Species</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004913</span>
      <span itemprop="name">activation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003044</span>
      <span itemprop="name">adaptive effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004458</span>
      <span itemprop="name">analytical method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004888</span>
      <span itemprop="name">barrier function</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010923</span>
      <span itemprop="name">batch production</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006399</span>
      <span itemprop="name">behavior</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">229910052799</span>
      <span itemprop="name">carbon</span>
      <span itemprop="domain">Inorganic materials</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000008859</span>
      <span itemprop="name">change</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000007796</span>
      <span itemprop="name">conventional method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013500</span>
      <span itemprop="name">data storage</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013135</span>
      <span itemprop="name">deep learning</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000007123</span>
      <span itemprop="name">defense</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013461</span>
      <span itemprop="name">design</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004069</span>
      <span itemprop="name">differentiation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000446</span>
      <span itemprop="name">fuel</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010191</span>
      <span itemprop="name">image analysis</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003384</span>
      <span itemprop="name">imaging method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001939</span>
      <span itemprop="name">inductive effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000002347</span>
      <span itemprop="name">injection</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000007924</span>
      <span itemprop="name">injection</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003993</span>
      <span itemprop="name">interaction</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004807</span>
      <span itemprop="name">localization</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000007774</span>
      <span itemprop="name">longterm</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000003550</span>
      <span itemprop="name">marker</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000011159</span>
      <span itemprop="name">matrix material</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000007246</span>
      <span itemprop="name">mechanism</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000116</span>
      <span itemprop="name">mitigating effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010295</span>
      <span itemprop="name">mobile communication</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012986</span>
      <span itemprop="name">modification</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004048</span>
      <span itemprop="name">modification</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003058</span>
      <span itemprop="name">natural language processing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000037361</span>
      <span itemprop="name">pathway</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012805</span>
      <span itemprop="name">post-processing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000644</span>
      <span itemprop="name">propagated effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000306</span>
      <span itemprop="name">recurrent effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006403</span>
      <span itemprop="name">short-term memory</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012546</span>
      <span itemprop="name">transfer</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000007704</span>
      <span itemprop="name">transition</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013519</span>
      <span itemprop="name">translation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010200</span>
      <span itemprop="name">validation analysis</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012800</span>
      <span itemprop="name">visualization</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
  </ul>

  <section>
    <h2>Images</h2>
    <ul>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/02/0e/af/c3e8d946e16fc7/US11861853-20240102-D00000.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/87/99/22/52f00c7b72fbf0/US11861853-20240102-D00000.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="500">
            <meta itemprop="label" content="CNN architecture">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1704">
              <meta itemprop="top" content="4">
              <meta itemprop="right" content="1757">
              <meta itemprop="bottom" content="30">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="502">
            <meta itemprop="label" content="input video sequences">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="114">
              <meta itemprop="top" content="95">
              <meta itemprop="right" content="163">
              <meta itemprop="bottom" content="117">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="504">
            <meta itemprop="label" content="line segmentation branch">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="770">
              <meta itemprop="top" content="62">
              <meta itemprop="right" content="822">
              <meta itemprop="bottom" content="89">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="506">
            <meta itemprop="label" content="attention map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1440">
              <meta itemprop="top" content="80">
              <meta itemprop="right" content="1493">
              <meta itemprop="bottom" content="106">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="510">
            <meta itemprop="label" content="network">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1651">
              <meta itemprop="top" content="503">
              <meta itemprop="right" content="1703">
              <meta itemprop="bottom" content="528">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="512">
            <meta itemprop="label" content="concatenation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="113">
              <meta itemprop="top" content="1023">
              <meta itemprop="right" content="166">
              <meta itemprop="bottom" content="1048">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="518">
            <meta itemprop="label" content="initial pooling layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="934">
              <meta itemprop="top" content="1069">
              <meta itemprop="right" content="986">
              <meta itemprop="bottom" content="1092">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="524">
            <meta itemprop="label" content="subsequent pooling layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1190">
              <meta itemprop="top" content="1050">
              <meta itemprop="right" content="1244">
              <meta itemprop="bottom" content="1073">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="532">
            <meta itemprop="label" content="connected layers">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1509">
              <meta itemprop="top" content="1173">
              <meta itemprop="right" content="1561">
              <meta itemprop="bottom" content="1197">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/c6/11/9b/1cbd96c0cc292f/US11861853-20240102-D00001.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/b2/e1/1f/83e84abde7c027/US11861853-20240102-D00001.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="1">
            <meta itemprop="label" content="layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="613">
              <meta itemprop="top" content="933">
              <meta itemprop="right" content="640">
              <meta itemprop="bottom" content="956">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="104">
            <meta itemprop="label" content="road lane lines">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="25">
              <meta itemprop="top" content="1852">
              <meta itemprop="right" content="62">
              <meta itemprop="bottom" content="1927">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="112">
            <meta itemprop="label" content="speed">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="825">
              <meta itemprop="top" content="327">
              <meta itemprop="right" content="857">
              <meta itemprop="bottom" content="400">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/b7/0b/f6/f3d8d49bc630af/US11861853-20240102-D00002.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/fe/c4/af/54715729670ee9/US11861853-20240102-D00002.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="ego vehicle">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1349">
              <meta itemprop="top" content="21">
              <meta itemprop="right" content="1424">
              <meta itemprop="bottom" content="52">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="exterior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="25">
              <meta itemprop="top" content="376">
              <meta itemprop="right" content="101">
              <meta itemprop="bottom" content="408">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="exterior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="27">
              <meta itemprop="top" content="1897">
              <meta itemprop="right" content="103">
              <meta itemprop="bottom" content="1927">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="exterior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="254">
              <meta itemprop="top" content="74">
              <meta itemprop="right" content="329">
              <meta itemprop="bottom" content="109">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="exterior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="449">
              <meta itemprop="top" content="2232">
              <meta itemprop="right" content="526">
              <meta itemprop="bottom" content="2264">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="exterior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="955">
              <meta itemprop="top" content="80">
              <meta itemprop="right" content="1029">
              <meta itemprop="bottom" content="113">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="exterior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="904">
              <meta itemprop="top" content="2232">
              <meta itemprop="right" content="981">
              <meta itemprop="bottom" content="2263">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="exterior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1218">
              <meta itemprop="top" content="301">
              <meta itemprop="right" content="1295">
              <meta itemprop="bottom" content="334">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="exterior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1272">
              <meta itemprop="top" content="1891">
              <meta itemprop="right" content="1346">
              <meta itemprop="bottom" content="1925">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="210">
            <meta itemprop="label" content="interior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="18">
              <meta itemprop="top" content="638">
              <meta itemprop="right" content="94">
              <meta itemprop="bottom" content="669">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="210">
            <meta itemprop="label" content="interior cameras">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="18">
              <meta itemprop="top" content="1003">
              <meta itemprop="right" content="94">
              <meta itemprop="bottom" content="1033">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/cf/2f/78/9c25cc819a2d0e/US11861853-20240102-D00003.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/87/96/b0/14f797377dd873/US11861853-20240102-D00003.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="310">
            <meta itemprop="label" content="video camera">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1056">
              <meta itemprop="top" content="22">
              <meta itemprop="right" content="1174">
              <meta itemprop="bottom" content="80">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/1d/ca/e9/57c20983792295/US11861853-20240102-D00004.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/85/e7/09/417afa5de5f43c/US11861853-20240102-D00004.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="400">
            <meta itemprop="label" content="hardware implementation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="21">
              <meta itemprop="top" content="72">
              <meta itemprop="right" content="54">
              <meta itemprop="bottom" content="150">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="412">
            <meta itemprop="label" content="supervisory unit">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1090">
              <meta itemprop="top" content="1310">
              <meta itemprop="right" content="1122">
              <meta itemprop="bottom" content="1391">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="414">
            <meta itemprop="label" content="input interface">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1088">
              <meta itemprop="top" content="309">
              <meta itemprop="right" content="1123">
              <meta itemprop="bottom" content="392">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="416">
            <meta itemprop="label" content="power management component">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1088">
              <meta itemprop="top" content="2157">
              <meta itemprop="right" content="1122">
              <meta itemprop="bottom" content="2239">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="420">
            <meta itemprop="label" content="edge computing device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="238">
              <meta itemprop="top" content="259">
              <meta itemprop="right" content="274">
              <meta itemprop="bottom" content="343">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="422">
            <meta itemprop="label" content="CPU">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="636">
              <meta itemprop="top" content="1309">
              <meta itemprop="right" content="670">
              <meta itemprop="bottom" content="1390">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/af/f5/c4/b1a12bb3c2b69b/US11861853-20240102-D00005.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/02/34/8a/579ae0f98abf39/US11861853-20240102-D00005.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="500">
            <meta itemprop="label" content="CNN architecture">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="19">
              <meta itemprop="top" content="180">
              <meta itemprop="right" content="54">
              <meta itemprop="bottom" content="257">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="502">
            <meta itemprop="label" content="input video sequences">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="146">
              <meta itemprop="top" content="2450">
              <meta itemprop="right" content="178">
              <meta itemprop="bottom" content="2523">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="504">
            <meta itemprop="label" content="line segmentation branch">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="104">
              <meta itemprop="top" content="1512">
              <meta itemprop="right" content="137">
              <meta itemprop="bottom" content="1586">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="506">
            <meta itemprop="label" content="attention map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="128">
              <meta itemprop="top" content="559">
              <meta itemprop="right" content="160">
              <meta itemprop="bottom" content="631">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="510">
            <meta itemprop="label" content="network">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="730">
              <meta itemprop="top" content="255">
              <meta itemprop="right" content="764">
              <meta itemprop="bottom" content="331">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="512">
            <meta itemprop="label" content="concatenation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1473">
              <meta itemprop="top" content="2450">
              <meta itemprop="right" content="1506">
              <meta itemprop="bottom" content="2522">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="518">
            <meta itemprop="label" content="initial pooling layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1537">
              <meta itemprop="top" content="1278">
              <meta itemprop="right" content="1569">
              <meta itemprop="bottom" content="1352">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="524">
            <meta itemprop="label" content="subsequent pooling layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1509">
              <meta itemprop="top" content="911">
              <meta itemprop="right" content="1540">
              <meta itemprop="bottom" content="987">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="532">
            <meta itemprop="label" content="connected layers">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1685">
              <meta itemprop="top" content="459">
              <meta itemprop="right" content="1719">
              <meta itemprop="bottom" content="532">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/cc/38/a8/ce3540dd41a1e6/US11861853-20240102-D00006.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/fd/51/bc/ffa555334bfcdf/US11861853-20240102-D00006.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="504">
            <meta itemprop="label" content="line segmentation branch">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="20">
              <meta itemprop="top" content="168">
              <meta itemprop="right" content="49">
              <meta itemprop="bottom" content="243">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="612">
            <meta itemprop="label" content="input image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1293">
              <meta itemprop="top" content="1396">
              <meta itemprop="right" content="1327">
              <meta itemprop="bottom" content="1470">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="614">
            <meta itemprop="label" content="backbone network">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="402">
              <meta itemprop="top" content="2362">
              <meta itemprop="right" content="436">
              <meta itemprop="bottom" content="2435">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="616">
            <meta itemprop="label" content="module">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="149">
              <meta itemprop="top" content="1050">
              <meta itemprop="right" content="182">
              <meta itemprop="bottom" content="1124">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="618">
            <meta itemprop="label" content="line segmentation head">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="299">
              <meta itemprop="top" content="18">
              <meta itemprop="right" content="333">
              <meta itemprop="bottom" content="93">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="622">
            <meta itemprop="label" content="output feature map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1124">
              <meta itemprop="top" content="75">
              <meta itemprop="right" content="1156">
              <meta itemprop="bottom" content="149">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/3f/c5/e6/918441a287e4a4/US11861853-20240102-D00007.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/ac/85/86/87f98db958dac0/US11861853-20240102-D00007.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="Method RMSE MAE">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="985">
              <meta itemprop="top" content="2087">
              <meta itemprop="right" content="1016">
              <meta itemprop="bottom" content="2139">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="704">
            <meta itemprop="label" content="token z">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="326">
              <meta itemprop="top" content="1806">
              <meta itemprop="right" content="359">
              <meta itemprop="bottom" content="1882">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="724">
            <meta itemprop="label" content="Attention">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1011">
              <meta itemprop="top" content="428">
              <meta itemprop="right" content="1042">
              <meta itemprop="bottom" content="504">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/a2/f5/52/8aa75bc1a0579f/US11861853-20240102-D00008.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/f6/b2/83/70958f7abcf39e/US11861853-20240102-D00008.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/b9/d4/09/5c226c4a0a6a26/US11861853-20240102-D00009.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/39/8c/96/2810fcbbe4120e/US11861853-20240102-D00009.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/42/15/b3/0f891119844f47/US11861853-20240102-D00010.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/dc/87/40/57df2534ac0bda/US11861853-20240102-D00010.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="1000">
            <meta itemprop="label" content="computer system">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="487">
              <meta itemprop="top" content="2095">
              <meta itemprop="right" content="534">
              <meta itemprop="bottom" content="2222">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="ego vehicle">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1351">
              <meta itemprop="top" content="2093">
              <meta itemprop="right" content="1395">
              <meta itemprop="bottom" content="2191">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="4">
            <meta itemprop="label" content="volume">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1637">
              <meta itemprop="top" content="1571">
              <meta itemprop="right" content="1690">
              <meta itemprop="bottom" content="1621">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="400">
            <meta itemprop="label" content="hardware implementation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1135">
              <meta itemprop="top" content="2092">
              <meta itemprop="right" content="1182">
              <meta itemprop="bottom" content="2193">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/c2/9d/97/b0d87176625b12/US11861853-20240102-D00011.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/d8/0d/6e/af3799441acc77/US11861853-20240102-D00011.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="ego vehicle">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1310">
              <meta itemprop="top" content="2275">
              <meta itemprop="right" content="1358">
              <meta itemprop="bottom" content="2381">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="4">
            <meta itemprop="label" content="volume">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1767">
              <meta itemprop="top" content="1694">
              <meta itemprop="right" content="1852">
              <meta itemprop="bottom" content="1777">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="400">
            <meta itemprop="label" content="hardware implementation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="917">
              <meta itemprop="top" content="2275">
              <meta itemprop="right" content="963">
              <meta itemprop="bottom" content="2382">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="500">
            <meta itemprop="label" content="CNN architecture">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="725">
              <meta itemprop="top" content="2276">
              <meta itemprop="right" content="768">
              <meta itemprop="bottom" content="2380">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/1b/d6/06/7b52053f1cf3f1/US11861853-20240102-D00012.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/7c/7b/05/ad42c48877b214/US11861853-20240102-D00012.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/d7/13/10/59a3c09dab8c61/US11861853-20240102-D00013.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/c3/f2/7e/c227fea6fcb9b0/US11861853-20240102-D00013.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="13">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="ego vehicle">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="673">
              <meta itemprop="top" content="2353">
              <meta itemprop="right" content="728">
              <meta itemprop="bottom" content="2463">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="13">
            <meta itemprop="id" content="4">
            <meta itemprop="label" content="volume">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1842">
              <meta itemprop="top" content="1778">
              <meta itemprop="right" content="1906">
              <meta itemprop="bottom" content="1818">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/c4/ed/b9/5ee7670eaa794b/US11861853-20240102-D00014.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/39/ba/38/3465b5dca6df22/US11861853-20240102-D00014.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1000">
            <meta itemprop="label" content="computer system">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="20">
              <meta itemprop="top" content="2241">
              <meta itemprop="right" content="52">
              <meta itemprop="bottom" content="2342">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1002">
            <meta itemprop="label" content="main memory">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1397">
              <meta itemprop="top" content="1073">
              <meta itemprop="right" content="1435">
              <meta itemprop="bottom" content="1170">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1004">
            <meta itemprop="label" content="non-volatile storage device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="871">
              <meta itemprop="top" content="1638">
              <meta itemprop="right" content="905">
              <meta itemprop="bottom" content="1737">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1006">
            <meta itemprop="label" content="Network Controller">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1399">
              <meta itemprop="top" content="1902">
              <meta itemprop="right" content="1433">
              <meta itemprop="bottom" content="2001">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1008">
            <meta itemprop="label" content="more Displays">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1935">
              <meta itemprop="top" content="581">
              <meta itemprop="right" content="1969">
              <meta itemprop="bottom" content="681">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1012">
            <meta itemprop="label" content="graphics board">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1399">
              <meta itemprop="top" content="658">
              <meta itemprop="right" content="1432">
              <meta itemprop="bottom" content="756">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1016">
            <meta itemprop="label" content="Display Adapter">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1395">
              <meta itemprop="top" content="259">
              <meta itemprop="right" content="1432">
              <meta itemprop="bottom" content="359">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1018">
            <meta itemprop="label" content="Peripherals">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="202">
              <meta itemprop="top" content="1150">
              <meta itemprop="right" content="234">
              <meta itemprop="bottom" content="1247">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1026">
            <meta itemprop="label" content="system bus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1106">
              <meta itemprop="top" content="1109">
              <meta itemprop="right" content="1139">
              <meta itemprop="bottom" content="1209">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="1050">
            <meta itemprop="label" content="processing cores">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1428">
              <meta itemprop="top" content="1485">
              <meta itemprop="right" content="1461">
              <meta itemprop="bottom" content="1586">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="99">
            <meta itemprop="label" content="network">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1449">
              <meta itemprop="top" content="2568">
              <meta itemprop="right" content="1491">
              <meta itemprop="bottom" content="2637">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/1d/52/c6/03ad5e048de484/US11861853-20240102-D00015.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/13/49/8d/5f2d07a01ada90/US11861853-20240102-D00015.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1100">
            <meta itemprop="label" content="data processing system">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1406">
              <meta itemprop="top" content="16">
              <meta itemprop="right" content="1536">
              <meta itemprop="bottom" content="66">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1120">
            <meta itemprop="label" content="ICH">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="824">
              <meta itemprop="top" content="1105">
              <meta itemprop="right" content="921">
              <meta itemprop="bottom" content="1139">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1125">
            <meta itemprop="label" content="MCH">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="824">
              <meta itemprop="top" content="664">
              <meta itemprop="right" content="919">
              <meta itemprop="bottom" content="695">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1145">
            <meta itemprop="label" content="memory">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1437">
              <meta itemprop="top" content="660">
              <meta itemprop="right" content="1531">
              <meta itemprop="bottom" content="697">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1150">
            <meta itemprop="label" content="graphics processor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="249">
              <meta itemprop="top" content="661">
              <meta itemprop="right" content="344">
              <meta itemprop="bottom" content="697">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1156">
            <meta itemprop="label" content="ROM">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1284">
              <meta itemprop="top" content="1051">
              <meta itemprop="right" content="1379">
              <meta itemprop="bottom" content="1087">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1158">
            <meta itemprop="label" content="graphics controller">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="446">
              <meta itemprop="top" content="1083">
              <meta itemprop="right" content="540">
              <meta itemprop="bottom" content="1116">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1160">
            <meta itemprop="label" content="Hard disk drive">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="851">
              <meta itemprop="top" content="1768">
              <meta itemprop="right" content="947">
              <meta itemprop="bottom" content="1804">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1162">
            <meta itemprop="label" content="PCI bus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="125">
              <meta itemprop="top" content="1709">
              <meta itemprop="right" content="220">
              <meta itemprop="bottom" content="1742">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1164">
            <meta itemprop="label" content="port">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="334">
              <meta itemprop="top" content="1708">
              <meta itemprop="right" content="431">
              <meta itemprop="bottom" content="1744">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1166">
            <meta itemprop="label" content="optical drive">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="587">
              <meta itemprop="top" content="1738">
              <meta itemprop="right" content="683">
              <meta itemprop="bottom" content="1774">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1170">
            <meta itemprop="label" content="keyboard">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1146">
              <meta itemprop="top" content="1919">
              <meta itemprop="right" content="1242">
              <meta itemprop="bottom" content="1953">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1172">
            <meta itemprop="label" content="mouse">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1447">
              <meta itemprop="top" content="1890">
              <meta itemprop="right" content="1541">
              <meta itemprop="bottom" content="1922">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1176">
            <meta itemprop="label" content="serial port">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1293">
              <meta itemprop="top" content="2246">
              <meta itemprop="right" content="1391">
              <meta itemprop="bottom" content="2281">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="1178">
            <meta itemprop="label" content="parallel port">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1631">
              <meta itemprop="top" content="2248">
              <meta itemprop="right" content="1725">
              <meta itemprop="bottom" content="2280">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/8e/70/84/e4b6fc53bbfc2e/US11861853-20240102-D00016.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/ca/44/61/a6bea185e728e3/US11861853-20240102-D00016.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1130">
            <meta itemprop="label" content="CPU">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="105">
              <meta itemprop="top" content="110">
              <meta itemprop="right" content="202">
              <meta itemprop="bottom" content="147">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1232">
            <meta itemprop="label" content="register">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="747">
              <meta itemprop="top" content="948">
              <meta itemprop="right" content="841">
              <meta itemprop="bottom" content="983">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1236">
            <meta itemprop="label" content="control logic">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="463">
              <meta itemprop="top" content="318">
              <meta itemprop="right" content="558">
              <meta itemprop="bottom" content="354">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1238">
            <meta itemprop="label" content="instruction register">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="252">
              <meta itemprop="top" content="1347">
              <meta itemprop="right" content="347">
              <meta itemprop="bottom" content="1380">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1240">
            <meta itemprop="label" content="fast memory">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1198">
              <meta itemprop="top" content="1579">
              <meta itemprop="right" content="1293">
              <meta itemprop="bottom" content="1613">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/65/6f/7e/b02a1731d41a0e/US11861853-20240102-D00017.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/b4/b3/89/7aac20a7896489/US11861853-20240102-D00017.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1312">
            <meta itemprop="label" content="tablet">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="86">
              <meta itemprop="top" content="601">
              <meta itemprop="right" content="189">
              <meta itemprop="bottom" content="635">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1314">
            <meta itemprop="label" content="mobile device terminal">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="161">
              <meta itemprop="top" content="1044">
              <meta itemprop="right" content="259">
              <meta itemprop="bottom" content="1081">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1316">
            <meta itemprop="label" content="Fixed terminals">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="25">
              <meta itemprop="top" content="1575">
              <meta itemprop="right" content="124">
              <meta itemprop="bottom" content="1612">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1320">
            <meta itemprop="label" content="mobile network service">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="971">
              <meta itemprop="top" content="339">
              <meta itemprop="right" content="1069">
              <meta itemprop="bottom" content="376">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1322">
            <meta itemprop="label" content="central processors">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1037">
              <meta itemprop="top" content="921">
              <meta itemprop="right" content="1133">
              <meta itemprop="bottom" content="953">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1324">
            <meta itemprop="label" content="server">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1316">
              <meta itemprop="top" content="919">
              <meta itemprop="right" content="1413">
              <meta itemprop="bottom" content="953">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1326">
            <meta itemprop="label" content="database">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1587">
              <meta itemprop="top" content="893">
              <meta itemprop="right" content="1683">
              <meta itemprop="bottom" content="929">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1330">
            <meta itemprop="label" content="cloud">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="525">
              <meta itemprop="top" content="2317">
              <meta itemprop="right" content="623">
              <meta itemprop="bottom" content="2352">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1334">
            <meta itemprop="label" content="data center">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1212">
              <meta itemprop="top" content="2838">
              <meta itemprop="right" content="1310">
              <meta itemprop="bottom" content="2874">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1336">
            <meta itemprop="label" content="cloud controller">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="788">
              <meta itemprop="top" content="2542">
              <meta itemprop="right" content="886">
              <meta itemprop="bottom" content="2577">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1338">
            <meta itemprop="label" content="data storage">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1645">
              <meta itemprop="top" content="2653">
              <meta itemprop="right" content="1743">
              <meta itemprop="bottom" content="2692">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1340">
            <meta itemprop="label" content="provisioning tool">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1476">
              <meta itemprop="top" content="2010">
              <meta itemprop="right" content="1571">
              <meta itemprop="bottom" content="2047">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1352">
            <meta itemprop="label" content="satellite">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="637">
              <meta itemprop="top" content="1281">
              <meta itemprop="right" content="733">
              <meta itemprop="bottom" content="1317">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1354">
            <meta itemprop="label" content="access point">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="570">
              <meta itemprop="top" content="926">
              <meta itemprop="right" content="666">
              <meta itemprop="bottom" content="963">
            </span>
          </li>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>Classifications</h2>
    <ul>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/20</span>&mdash;<span itemprop="Description">Analysis of motion</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/246</span>&mdash;<span itemprop="Description">Analysis of motion using feature-based methods, e.g. the tracking of corners or segments</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10</span>&mdash;<span itemprop="Description">Image acquisition modality</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10016</span>&mdash;<span itemprop="Description">Video; Image sequence</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10</span>&mdash;<span itemprop="Description">Image acquisition modality</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10024</span>&mdash;<span itemprop="Description">Color image</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20</span>&mdash;<span itemprop="Description">Special algorithmic details</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20081</span>&mdash;<span itemprop="Description">Training; Learning</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20</span>&mdash;<span itemprop="Description">Special algorithmic details</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20084</span>&mdash;<span itemprop="Description">Artificial neural networks [ANN]</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30</span>&mdash;<span itemprop="Description">Subject of image; Context of image processing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30248</span>&mdash;<span itemprop="Description">Vehicle exterior or interior</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30252</span>&mdash;<span itemprop="Description">Vehicle exterior; Vicinity of vehicle</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30256</span>&mdash;<span itemprop="Description">Lane; Road marking</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
    </ul>
  </section>

  

  

  <section>
    <h2>Definitions</h2>
    <ul>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the present disclosure</span>
        <span itemprop="definition">is directed to a neural network time series model, and preferably, a 3D Convolutional Neural Network (3D-CNN). with masked-attention (3D-CMA) architecture to estimate ego vehicle speed using a single front-facing monocular camera.</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D-CNN</span>
        <span itemprop="definition">3D Convolutional Neural Network</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D-CMA</span>
        <span itemprop="definition">masked-attention</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Speed estimation of an ego vehicle</span>
        <span itemprop="definition">is crucial to enable autonomous driving and advanced driver assistance technologies. Due to functional and legacy issues, conventional methods depend on in-car sensors to extract vehicle speed through the Controller Area Network (CAN) bus.</span>
        <meta itemprop="num_attr" content="0006">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">CAN</span>
        <span itemprop="definition">Controller Area Network</span>
        <meta itemprop="num_attr" content="0006">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FlowNet and PWC-Net</span>
        <span itemprop="definition">are deep neural networks to estimate optical flow in videos. See Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Flownet 2.0</span>
        <span itemprop="definition">Evolution of optical flow estimation with deep networks.</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Pwc-net</span>
        <span itemprop="definition">Cnns for optical flow using pyramid, warping, and cost volume.</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FlowNet or PWC-Net</span>
        <span itemprop="definition">can be used to estimate the ego vehicle speed. See RÃ³bert-Adrian Rill. Speed estimation evaluation on the kitti benchmark based on motion and monocular depth information. arXiv preprint arXiv: 1907.06989, 2019; and Jun Hayakawa and Behzad Dariush. Ego-motion and surrounding vehicle state estimation using a monocular camera. In 2019 IEEE Intelligent Vehicles Symposium ( IV ), pages 2550-2556. IEEE, 2019, incorporated herein by reference in their entirety. However, ego vehicle speed estimation is performed by further post-processing on the optical flow pixel velocity. No work demonstrates end-to-end architecture capability where the speed could be learned with differentiation of the loss function.</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D-CNN</span>
        <span itemprop="definition">3D convolutional neural network</span>
        <meta itemprop="num_attr" content="0010">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An aspect of the present disclosure</span>
        <span itemprop="definition">is a system for ego vehicle speed estimation.</span>
        <meta itemprop="num_attr" content="0011">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the system</span>
        <span itemprop="definition">can include a car-mounted monocular camera for capturing a sequence of video frames of an outdoor scene from a moving car, where the outdoor scene includes a road, as a camera channel; processing circuitry configured with a single-shot network and a neural network time series model, the single-shot network segments features of the road in the video frame sequence and generates a masked-attention map for the segmented road features; a concatenation operation that concatenates the masked-attention map as an additional channel to the camera channel to generate a masked-attention input; the neural network time series model receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of the segmented road features in the video sequences; and output circuitry to output a signal indicating the estimated speed.</span>
        <meta itemprop="num_attr" content="0011">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a further aspect of the present disclosure</span>
        <span itemprop="definition">is an embedded ego vehicle speed estimation apparatus.</span>
        <meta itemprop="num_attr" content="0012">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the apparatus</span>
        <span itemprop="definition">can include processing circuitry configured with a single-shot network and a neural network time series model, the single-shot network segments features in a video frame sequence of a road and generates a masked-attention map for the segmented road features; a concatenation the neural network time seties model receives the masked-attention input and generates an estimated operation that concatenates the masked-attention map as an additional channel to a camera channel to generate a masked-attention input; speed of the ego vehicle based on displacement of the lane line segments in the video sequences; and output circuitry to output a signal indicating the estimated speed.</span>
        <meta itemprop="num_attr" content="0012">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a further aspect of the present disclosure</span>
        <span itemprop="definition">is a non-transitory computer readable storage medium storing computer instructions, which when executed by processing circuitry, perform a method of ego vehicle speed estimation.</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">can include segmenting, by a single-shot network, features in a video frame sequence of a road and generates a masked-attention map for the segmented road features; concatenating, by a concatenation operation, the masked-attention map as an additional channel to a camera channel to generate a masked-attention input; receiving, by a neural network time series model, the masked-attention input and generating an estimated speed of the ego vehicle based on displacement of the lane line segments in the video sequences; and outputting a signal indicating the estimated speed.</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">illustrates estimation of ego-vehicle speed using a continuous camera stream</span>
        <meta itemprop="num_attr" content="0016">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">is a top view of an exemplary vehicle having video cameras mounted thereon;</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3</span>
        <span itemprop="definition">illustrates an exemplary USB dashcam</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a block diagram of an architecture of 3D-CMA</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">is a block diagram of an architecture having lane line segmentation including an encoder and a decoder</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">is a block diagram of an architecture of ViViT</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 8 A- 8 D</span>
        <span itemprop="definition">illustrate visualization of sample images of the KITTI and nuImages dataset</span>
        <meta itemprop="num_attr" content="0023">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 9 A- 9 D</span>
        <span itemprop="definition">are graphs of train/test speed data distribution for nuImages and KITTI datasets.</span>
        <meta itemprop="num_attr" content="0024">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 10</span>
        <span itemprop="definition">is an illustration of a non-limiting example of details of computing hardware used in the computing system, according to aspects of the present disclosure</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 11</span>
        <span itemprop="definition">is an exemplary schematic diagram of a data processing system used within the computing system, according to aspects of the present disclosure.</span>
        <meta itemprop="num_attr" content="0026">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 12</span>
        <span itemprop="definition">is an exemplary schematic diagram of a processor used with the computing system, according to aspects of the present disclosure.</span>
        <meta itemprop="num_attr" content="0027">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 13</span>
        <span itemprop="definition">is an illustration of a non-limiting example of distributed components that may share processing with the controller, according to aspects of the present disclosure.</span>
        <meta itemprop="num_attr" content="0028">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the terms âapproximately,â âapproximate,â âabout,â and similar terms</span>
        <span itemprop="definition">generally refer to ranges that include the identified value within a margin of 20%, 10%, or preferably 5%, and any values therebetween.</span>
        <meta itemprop="num_attr" content="0030">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ADAS</span>
        <span itemprop="definition">Advanced Driver Assistance Systems</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Vehicles</span>
        <span itemprop="definition">may offer driver assistance technologies including Autonomous Emergency Braking and a safe distance warning.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ADAS</span>
        <span itemprop="definition">may take into consideration environmental conditions and vehicle performance characteristics.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Environmental conditions</span>
        <span itemprop="definition">can be obtained using vehicle environment sensors.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Vehicle cameras</span>
        <span itemprop="definition">can capture a continuous camera stream.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ego vehicle</span>
        <span itemprop="definition">refers to a vehicle that contains vehicle environment sensors that perceive the environment around the vehicle.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Edge computing devices</span>
        <span itemprop="definition">are computing devices that are proximate to the data source, such as vehicle environment sensors.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">illustrates estimation of ego-vehicle speed using a continuous camera stream.</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the present disclosure</span>
        <span itemprop="definition">includes a 3D Convolutional Neural Network (3D-CNN) architecture trained on short videos using corresponding grayscale image frames 102 and corresponding focus masks, such as masks that focus on road lane lines 104 , lane line segmentation masks.</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network architecture</span>
        <span itemprop="definition">is used to estimate the speed 112 of the ego vehicle, which can, in turn help in ADAS, including, among other things, to estimate the speed of vehicles of interest (VOI) in the surrounding environment.</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VOI</span>
        <span itemprop="definition">speed of vehicles of interest</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">is a top view of an exemplary ego vehicle having video cameras mounted thereon.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Video cameras mounted on an ego vehicle</span>
        <span itemprop="definition">may be used to obtain video images to be used to estimate the speed of the ego vehicle.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego vehicle 200</span>
        <span itemprop="definition">can be of any make in the market.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a non-limiting ego vehicle</span>
        <span itemprop="definition">can be equipped with a number of exterior cameras 204 and interior cameras 210 .</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One camera 214 for speed estimation</span>
        <span itemprop="definition">can be mounted on the front dash, the front windshield, or embedded on the front portion of the exterior body and/or on the ego vehicle roof in order to capture images in front of the ego vehicle for external vehicles.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the camera</span>
        <span itemprop="definition">is preferably mounted integrally with a rearview/side mirror on the driver&#39;s side of the ego vehicle on a forward-facing surface (i.e., facing traffic preceding the ego vehicle). In this position the camera is generally oriented within the view of an individual inside the ego vehicle such that a driver can concurrently check for oncoming traffic behind the ego vehicle using the rearview side mirror and monitor the position of preceding vehicles.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3</span>
        <span itemprop="definition">illustrates an exemplary exterior-facing camera, which may be, but is not limited to, a USB camera 310 with a base that can be attached to the rearview mirror, side mirror, windshield, dashboard, front body panel, or roof of the ego vehicle 200 , to name a few.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the camera 310</span>
        <span itemprop="definition">can be a USB camera for connection to an edge computing device, that is proximate to the USB camera, by a USB cable.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the USB camera 310</span>
        <span itemprop="definition">may be of any make which can channel a video stream.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the speed estimation apparatus</span>
        <span itemprop="definition">is an all-in-one portable module that is removably mounted on a ego vehicle 200 .</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the all-in-one portable module</span>
        <span itemprop="definition">has a camera back plate which is curved to generally match the contours of the forward-facing surface of a side view mirror, e.g., an ovoidal shape having a flat inner surface matching the contours of the forward face of the side view mirror and a curved dome-like front surface with the camera lens/opening located at an apex of the dome shape.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the back plate</span>
        <span itemprop="definition">is optionally integral with a neck portion that terminates in a thin plate having a length of 5-20 cm which can be inserted into the gap between the window and patrol vehicle door to secure the all-in-one portable module to the ego vehicle.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a cable and/or wireless capability</span>
        <span itemprop="definition">may be included to transfer captured images to the edge computing device while the ego vehicle is moving.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the video camera 310</span>
        <span itemprop="definition">is capable of capturing a sequence of image frames at a predetermined frame rate.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the frame rate</span>
        <span itemprop="definition">may be fixed or may be adjusted in a manual setting, or may be set based on the mode of image capture.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a video camera</span>
        <span itemprop="definition">may have an adjustable frame rate for image capture, or may automatically set a frame rate depending on the type of image capture.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a burst image</span>
        <span itemprop="definition">may be set for one predetermined frame rate, while video capture may be set for another predetermined frame rate.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ego vehicle speed</span>
        <span itemprop="definition">is estimated based on video images of the surrounding environment.</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the speed estimation</span>
        <span itemprop="definition">is determined using machine learning technology.</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">2D Convolutional Neural Networks</span>
        <span itemprop="definition">have proven to be excellent at extracting feature maps for images and are predominantly used for understanding the spatial aspects of images relevant to image classification and object detection. However, 2D Convolutional Neural Networks cannot capture the spatio-temporal features of videos spread across multiple continuous frames.</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Neural network time series models</span>
        <span itemprop="definition">can be configured for video classification.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Neural network approaches that have been used for time series prediction</span>
        <span itemprop="definition">include recurrent neural networks (RNN) and long short-term memory (LSTM) neural networks.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">RNN</span>
        <span itemprop="definition">recurrent neural networks</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">LSTM</span>
        <span itemprop="definition">long short-term memory</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D Convolutional Neural Networks</span>
        <span itemprop="definition">can learn spatio-temporal features and thus help in video classification, human action recognition, and sign language recognition. Attention on top of 3D-CNN has also been used. See Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video action transformer network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 244-253, 2019; Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick. Long-term feature banks for detailed video understanding.</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Hand3d</span>
        <span itemprop="definition">Hand pose estimation using 3d neural network.</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the approaches</span>
        <span itemprop="definition">perform regression perform spatial localization-related tasks such as human pose or 3D hand pose.</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Vision Transformers</span>
        <span itemprop="definition">capitalize on processes used in transformers in the field of Natural Language Processing. A non-overlapping takes patches of an image and creates token embeddings after performing linear projection. See Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Thai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020, incorporated herein by reference in its entirety.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ViTs</span>
        <span itemprop="definition">have been used to replace CNNs, they lack the inductive bias, whereas CNN&#39;s are translation invariant due to the local neighborhood structure of the convolution kernels. Moreover, transformers have quadratic complexity for their operations and scale with the input dimensions. On the other hand, ViTs provide global attention and long-range interaction.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D-CNN with masked attention</span>
        <span itemprop="definition">3D-CMA</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Video transformer architectures</span>
        <span itemprop="definition">can be classified based on the embeddings (backbone and minimal embeddings), tokenization (patch tokenization, frame tokenization, clip tokenization), and positional embeddings.</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego vehicle speed</span>
        <span itemprop="definition">is estimated by relying purely on video streams from a monocular camera.</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego vehicle speed</span>
        <span itemprop="definition">can be estimated by onboard hardware that implements a neural network time series model.</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego vehicle speed</span>
        <span itemprop="definition">is estimated using a hybrid CNN-Transformer (3D-CMA).</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">is a block diagram of an onboard hardware implementation of an ego vehicle speed estimation system in accordance with an exemplary aspect of the disclosure.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hardware implementation of the speed estimation system 400</span>
        <span itemprop="definition">includes an image/video capturing device (video camera 310 ) and an edge computing device 420 .</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the video camera 310</span>
        <span itemprop="definition">is capable of capturing a sequence of image frames at a predetermined frame rate.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the frame rate</span>
        <span itemprop="definition">may be fixed or may be adjusted in a manual setting, or be set based on the mode of image capture.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a video camera</span>
        <span itemprop="definition">may have an adjustable frame rate for image capture, or may automatically set a frame rate depending on the type of image capture.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a burst image</span>
        <span itemprop="definition">may be set for one predetermined frame rate, while video capture may be set for another predetermined frame rate.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge computing device 420</span>
        <span itemprop="definition">is configured as an embedded processing circuitry for ego vehicle speed estimation.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge computing device 420</span>
        <span itemprop="definition">is a portable, or removably mounted, computing device which is equipped with a Graphical Processing Unit (GPU) or a type of machine learning engine, as well as a general purpose central processing unit (CPU) 422 , and its internal modules.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge computing device 420</span>
        <span itemprop="definition">provides computing power that is sufficient for machine learning inferencing in real time for tasks including vehicle speed estimation and object detection, preferably all with a single monocular camera.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Internal modules</span>
        <span itemprop="definition">can include communication modules, such as Global System for Mobile Communication (GSM) 426 and Global Positioning System (GPS) 424 , as well as an input interface 414 for connection to the vehicle network (Controller Area Network, CAN).</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GSM</span>
        <span itemprop="definition">Global System for Mobile Communication</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GPS</span>
        <span itemprop="definition">Global Positioning System</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a supervisory unit 412</span>
        <span itemprop="definition">may control input and output communication with the vehicle internal network.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GPU/CPU configured edge computing device 420</span>
        <span itemprop="definition">is an NVIDIA Jetson Series (including Orin, Xavier, Tx2, Nano) system on module or an equivalent high-performance processing module from any other manufacturer like Intel, etc.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the video camera 310</span>
        <span itemprop="definition">may be connected to the edge computing device 420 by a plug-in wired connection, such as USB, or may communicate with the edge computing device 420 by a wireless connection, such as Bluetooth Low Energy, depending on distance to the edge device and/or communication quality in a vehicle. This set up is powered by the vehicle&#39;s battery as a power source.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a power management component 416</span>
        <span itemprop="definition">may control or regulate power to the GPU/CPU 422 , on an as needed basis.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a time-series model</span>
        <span itemprop="definition">must be utilized to capture the relative motion between adjacent image data samples.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D-CNNs</span>
        <span itemprop="definition">can learn spatial and temporal features simultaneously using 3D kernels.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">small receptive fields of 3 â 3 â 3</span>
        <span itemprop="definition">are used as the convolutional kernels throughout the network.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Many 3D-CNN architectures</span>
        <span itemprop="definition">lose big chunks of temporal information after the first 3D pooling layer. This is especially valid in the case of short-term spatio-temporal features propagated by utilizing smaller temporal windows.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the pooling kernel size</span>
        <span itemprop="definition">is d â k â k, where d is the kernel temporal depth, and s is the spatial kernel size.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a block diagram of an architecture of 3D-CNN with masked attention (3D-CMA).</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">is a block diagram of an architecture having lane line segmentation including an encoder and a decoder as part of the masked attention layer.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a masked-attention layer 504</span>
        <span itemprop="definition">is added into the 3D-CNN architecture 500 to guide the model to focus on relevant features that help with ego-vehicle speed computation.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the relevant features</span>
        <span itemprop="definition">are road lane lines.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An image of an outdoor scene captured from a moving car</span>
        <span itemprop="definition">typically has significant clutter and random motion that can obscure the model learning.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a scene</span>
        <span itemprop="definition">can be obstructed by other moving vehicles, moving pedestrians, or birds and other animals.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Road work zones and temporary markers or lane markings</span>
        <span itemprop="definition">may create unusual views of the road.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">road markings</span>
        <span itemprop="definition">may transition from temporary markings in work zones to regular lane line markings.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Some roads</span>
        <span itemprop="definition">may offer periodic mile markers.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a 3D-CNN model</span>
        <span itemprop="definition">is preferably trained to filter out the irrelevant movements (such as that of other cars, pedestrians, etc.) that do not contribute towards the ego-vehicle speed estimation and focus only on features that matter.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">irrelevant movements</span>
        <span itemprop="definition">such as that of other cars, pedestrians, etc.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">adding masked-attention</span>
        <span itemprop="definition">helps to attain improved model performance with faster model convergence.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the error in speed estimation</span>
        <span itemprop="definition">is reduced by adding masked-attention to the 3D-CNN network 500 . Further details about the impact of masked-attention are described as part of an ablation study below.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Convolutional neural networks</span>
        <span itemprop="definition">comprise a learned set of filters, where each filter extracts a different feature from the image.</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An object</span>
        <span itemprop="definition">is to inhibit or exhibit the activation of features based on the appearance of objects of interest in the images.</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Typical scenes captured by car-mounted imaging devices</span>
        <span itemprop="definition">include background objects such as the sky, and other vehicles in the environment, which do not contribute to ego-vehicle speed estimation. In fact, the relative motion of environmental vehicles often contributes negatively to the ability of the neural network to inhibit irrelevant features.</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a masked-attention map 506</span>
        <span itemprop="definition">is concatenated to the input image 502 before passing an input image through the neural network.</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a single-shot network 504</span>
        <span itemprop="definition">is used with a shared encoder 614 and three separate decoders that accomplish specific tasks such as object detection, drivable area segmentation, and lane line segmentation.</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">CSP-Darknet</span>
        <span itemprop="definition">is preferably used as the backbone network 614 of the encoder, while the neck is mainly composed of Spatial Pyramid Pooling (SPP) module 616 and Feature Pyramid Network (FPN) module.</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SPP</span>
        <span itemprop="definition">Spatial Pyramid Pooling</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FPN</span>
        <span itemprop="definition">Feature Pyramid Network</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the masked-attention map 506</span>
        <span itemprop="definition">is generated from input video sequences 502 using the lane line segmentation branch 504 .</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the concatenation 512 of lane line segmentation as an additional channel to the camera channel</span>
        <span itemprop="definition">allows the 3D-CNN 510 to focus on the apparent displacement of the lane line segments in the video sequences to best estimate the ego-vehicle speed.</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the architecture 504 for lane line segmentation</span>
        <span itemprop="definition">includes an encoder 614 and a decoder 618 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the backbone network 614</span>
        <span itemprop="definition">is used to extract the features of the input image 612 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">some classic image classification network</span>
        <span itemprop="definition">serves as the backbone.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">CSP Darknet</span>
        <span itemprop="definition">is used as the backbone.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the SPP 616</span>
        <span itemprop="definition">generates and fuses features of different scales.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the lane line segmentation head 618</span>
        <span itemprop="definition">is configured such that after three upsampling processes, an output feature map 622 is restored to the size of (W; H; 2), which represents the probability of each pixel in the input image 612 for the lane line and the background.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">road features</span>
        <span itemprop="definition">may be used in the segmentation for masked attention.</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Other road features</span>
        <span itemprop="definition">can include, but are not limited to, periodic reflectors marking road boundaries, road center rumble ridges, road barriers having reflective markings, and mile marker posts.</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the background</span>
        <span itemprop="definition">is used to classify a road condition.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Road conditions</span>
        <span itemprop="definition">can include wet road, dry road, icy road, or snow conditions, to name a few.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the background</span>
        <span itemprop="definition">can be used to classify the type of road, including paved road vs an unfinished road.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">multiple branches</span>
        <span itemprop="definition">may be used in addition to lane line segmentation branch 504 for determining masked attention maps.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each of the multiple branches</span>
        <span itemprop="definition">may be for each of the different types of road features that can be used to focus attention for speed estimation.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the 3D-CNN architecture with masked-attention (3D-CMA) for ego vehicle speed estimation</span>
        <span itemprop="definition">is illustrated in FIG. 5 .</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the RGB stream</span>
        <span itemprop="definition">can be converted to grayscale since color information is not vital for speed estimation.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a masked attention map 406</span>
        <span itemprop="definition">is concatenated 512 as an additional channel to the grayscale image 502 .</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the original input streams</span>
        <span itemprop="definition">are resized to 64 â 64 before feeding them into the network 510 .</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the input to the model</span>
        <span itemprop="definition">has a dimension of n â 64 â 64 â 2, where n is the number of frames in the temporal sequence.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">all convolutional 3D layers 516 , 522</span>
        <span itemprop="definition">use a fixed kernel size of 3 â 3 â 3.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the initial pooling layer 518</span>
        <span itemprop="definition">uses a kernel size of 1 â 2 â 2 to preserve the temporal information.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the subsequent pooling layer 524</span>
        <span itemprop="definition">which appears at the center of the network, compresses the temporal and spatial domains with a kernel size of 2 â 2 â 2.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Six 3D convolutional layers 516 , 522 , 526 , 528</span>
        <span itemprop="definition">are incorporated with the number of filters for the layers from 1-6 being 32; 32; 64; 64; 128; 128 respectively.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">four fully connected layers 532 , 434, 436, 438</span>
        <span itemprop="definition">have 512; 256; 64 and 1 nodes.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the L2 loss function which is used for training the 3D-CNN</span>
        <span itemprop="definition">is as follows:</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">n</span>
        <span itemprop="definition">is the number of frames in the input and Si is the speed value ground truth of ith corresponding frame, and Si is the inferred speed value.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Xi</span>
        <span itemprop="definition">is the grayscale image channel, and X M is the masked-attention channel for every frame.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">W</span>
        <span itemprop="definition">is the weight tensor of the 3D convolutional kernel.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego vehicle speed estimation</span>
        <span itemprop="definition">may encounter varying conditions, such as varying road markings, varying road conditions, and even varying road surface types.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego vehicle speed estimation</span>
        <span itemprop="definition">can be configured to go into power conserve modes depending on such varying conditions.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the onboard hardware implementation of an ego vehicle speed estimation system 400</span>
        <span itemprop="definition">may be configured to use power efficiently.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hardware implementation 400</span>
        <span itemprop="definition">can be configured to halt processing of the 3D-CNN network when the segmented features do not include road features that may be used to determine ego vehicle speed.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hardware implementation 400</span>
        <span itemprop="definition">can be configured to monitor ego vehicle speed obtained from internal sensors while the 3D-CNN network is in the halted state.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hardware implementation 400</span>
        <span itemprop="definition">can be configured to intermittently perform processing using the 3D-CNN network.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hardware implementation 400</span>
        <span itemprop="definition">can be configured to continuously monitor vehicle speed while the ego vehicle is in an operating state and periodically estimate speed of the ego vehicle using the 3D-CNN network.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the effectiveness of the 3D-CMA model</span>
        <span itemprop="definition">was evaluated. First, the public datasets used in experiments are described. Then the metrics used for evaluation are described. The 3D-CMA model architecture is compared against a ViViT, a state-of-the-art vision transformer architectures. Additionally, some ablation studies are described to characterize the contribution of masked-attention within the network architecture and compare its performance by discarding the same from the 3D-CNN.</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ViViT</span>
        <span itemprop="definition">Video Vision Transformer</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Vivit</span>
        <span itemprop="definition">A video vision transformer.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ViViT</span>
        <span itemprop="definition">is easily reproducible and has a good balance between the parameters and accuracy for small datasets.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ViViT-H</span>
        <span itemprop="definition">scores an accuracy of 95.8, just below the 95.9 accuracy score by Swin-L as per the Video Transformers Survey over HowTo100M.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">is a block diagram of an architecture of ViViT.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the frames from the video(N)</span>
        <span itemprop="definition">are tokenized using 3D-Convolutional tubelet embeddings and further passed to multiple transformer encoders to regress the speed value finally.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ViViT</span>
        <span itemprop="definition">includes extracting non-overlapping, spatio-temporal âtubesâ from the input volume, and to linearly project this to d .</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">This method</span>
        <span itemprop="definition">is an extension of ViT&#39;s embedding to 3D, and corresponds to a 3D convolution. For a tubelet of dimension</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">tokens</span>
        <span itemprop="definition">are extracted from temporal, height, and width dimensions respectively. Smaller tubelet dimensions thus result in more tokens which increases the computation.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a ViT</span>
        <span itemprop="definition">extracts N non-overlapping image patches, x i  â R h â w , 602 performs a linear projection and then rasterises them into 1D tokens z i  â  d .</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the Transformer Encoder</span>
        <span itemprop="definition">can be trained with the spatio-temporal embeddings.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DBNet</span>
        <span itemprop="definition">is a large-scale dataset for driving behavior research which includes aligned videos and vehicular speed from 1000 km driving stretch. See Yiping Chen, Jingkang Wang, Jonathan Li, Cewu Lu, Zhipeng Luo, Han Xue, and Cheng Wang. Lidar-video driving dataset: Learning driving policies effectively. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5870-5878, 2018, incorporated herein by reference in its entirety. However, the test set is not available for public usage. Likewise, the test set of comma.ai speed challenge is not open to the public.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">KITTI dataset</span>
        <span itemprop="definition">has been utilized for speed estimation using motion and monocular depth estimation. See RÃ³bert-Adrian Rill. Speed estimation evaluation on the kitti benchmark based on motion and monocular depth information, 2019, incorporated herein by reference in its entirety. However, there is no information about the train and test splits used for the evaluation of the models. In the present disclosure, two public datasets are utilized for experimentsânulmages and KITTI. Some sample images extracted from video sequences for nulmages and KITTI are shown in FIGS. 7 A- 7 D .</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">nulmages</span>
        <span itemprop="definition">is derived from nuScenes and is a large-scale autonomous driving dataset having 93 k video clips of 6 seconds each. See Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020, incorporated herein by reference in its entirety. The dataset is collated from two diverse citiesâBoston and Singapore. Each video clip consists of 13 frames spaced out at 2 Hz. The annotated images include rain, snow, and night time, which are important for autonomous driving applications.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each sample in the nulmages dataset</span>
        <span itemprop="definition">comprises of an annotated camera image with an associated timestamp and past and future images. It is to be noted that the six previous and six future images are not annotated.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the sample frame</span>
        <span itemprop="definition">has meta-data information available as token ids regarding the previous and future frames associated with the particular sample.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle speed</span>
        <span itemprop="definition">is extracted from the CAN bus data and linked to the sample data through sample tokens.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the train and test splits of the nulmages dataset</span>
        <span itemprop="definition">have been strictly followed for training and evaluating the AI models.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the distribution of speed data across train and test splits of the nulmages dataset</span>
        <span itemprop="definition">are shown in FIGS. 8 A- 8 D .</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the KITTI Vision Benchmark Suite</span>
        <span itemprop="definition">is a public dataset containing raw data recordings that are captured and synchronized at 10 Hz. See Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3354-3361, 2012; and A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231-1237, 2013, each incorporated herein by reference in their entirety.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Geiger et al., 2012</span>
        <span itemprop="definition">presented the benchmark challenges, their creation and use for evaluating state-of-the-art computer vision methods, while Geiger et al., 2013, was a follow-up work that provided technical details on the raw data itself, describing the recording platform, the data format and the utilities.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the dataset</span>
        <span itemprop="definition">was captured by driving around the mid-size city of DÃ¼sseldorf.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the âsynched&#43;rectifiedâ processed data</span>
        <span itemprop="definition">is utilized where images are rectified and undistorted and where the data frame numbers correspond across all sensor streams. While the dataset provides both grayscale and color stereo sequences, an RGB stream is utilized extracted from camera ID 03 only.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego-vehicle speed values</span>
        <span itemprop="definition">are extracted from IMU sensor readings.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the raw data</span>
        <span itemprop="definition">is split across six categoriesâCity, Residential, Road, Campus, Person, and Calibration. For an experiment, data from City and Road categories is utilized. Some video samples in the City category have prolonged periods where the car is stationary. Such video samples are discarded where the vehicle was stationary for most of the video samples.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">train and test splits</span>
        <span itemprop="definition">are reported in Table 1. The distribution of speed data across train and test splits from the KITTI dataset is shown in FIGS. 9 A- 9 D .</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the MAE and RMSE</span>
        <span itemprop="definition">are computed as follows :</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">y i</span>
        <span itemprop="definition">denotes the ground truth ego-vehicle speed value</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">â  i</span>
        <span itemprop="definition">denotes the predicted speed value by the AI model</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the KITTI dataset</span>
        <span itemprop="definition">has a camera image resolution of 1238_374.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the temporal dimension we used for the KITTI dataset</span>
        <span itemprop="definition">is ten frames.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the KITTI dataset</span>
        <span itemprop="definition">is sampled at 10 Hz, which means that the models are fed with video frames containing visual information from a time window of 1 sec.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego-vehicle velocity assigned to any temporal sequence</span>
        <span itemprop="definition">is the speed value tagged to the closest time stamp of the 10th frame in the input sequence.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the camera image resolution for the nulmages dataset</span>
        <span itemprop="definition">is 1600_900.</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">nulmages dataset</span>
        <span itemprop="definition">is sampled at 2 Hz. Six frames each are taken, preceding and succeeding the sample frame. This means that the models are fed with video frames containing visual information spanning a time window of approximately 6 sec.</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the ego vehicle velocity assigned to any temporal sequence</span>
        <span itemprop="definition">is the speed value tagged to the closest time-stamp of the sample frame (7th frame in the input sequence).</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the number of transformer layers in the implementation</span>
        <span itemprop="definition">is 16.</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the number of heads for multi-headed self-attention blocks</span>
        <span itemprop="definition">is 16, and the dimension of embeddings is 128.</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the AI models</span>
        <span itemprop="definition">were trained using an Nvidia GeForce RTX-3070 Max-Q Design GPU having 8 GB VRAM.</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the learning rate used for training all models</span>
        <span itemprop="definition">is 1 â 10  â 3 .</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">All models</span>
        <span itemprop="definition">are trained for 100 epochs with early stopping criteria set to terminate the training process if validation loss does not improve for ten epochs consecutively.</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the optimizer utilized</span>
        <span itemprop="definition">is Adam since it utilizes both momentum and scaling</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the performance of the proposed 3D-CMA architecture</span>
        <span itemprop="definition">is evaluated and compared against the standard ViViT with spatio-temporal attention.</span>
        <meta itemprop="num_attr" content="0093">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the evaluation metrics</span>
        <span itemprop="definition">are reported on the test set for KITTI and nulmages datasets in the subsections below. The evaluation across all datasets consistently reported better results for the 3D-CMA architecture.</span>
        <meta itemprop="num_attr" content="0093">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Evaluation scores for the nulmages dataset</span>
        <span itemprop="definition">are shown in Table 2. Approximately 27% improvement was observed in RMSE and MAE for 3D-CMA compared to ViViT for the nulmages dataset.</span>
        <meta itemprop="num_attr" content="0094">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Evaluation scores for the nulmages dataset</span>
        <span itemprop="definition">are shown in Table 4.</span>
        <meta itemprop="num_attr" content="0099">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the addition of masked-attention</span>
        <span itemprop="definition">reduces RMSE by 23:6% and MAE by 25:9% for the nulmages dataset.</span>
        <meta itemprop="num_attr" content="0099">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Evaluation scores for the KITTI dataset</span>
        <span itemprop="definition">are shown in Table 5.</span>
        <meta itemprop="num_attr" content="0101">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the addition of masked-attention</span>
        <span itemprop="definition">reduces the RMSE by 25:8% and MAE by 30:1% for the KITTI dataset.</span>
        <meta itemprop="num_attr" content="0101">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">nuImages-trained models</span>
        <span itemprop="definition">require the temporal window to be 13 frames across 6 secs.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">KITTI dataset video streams</span>
        <span itemprop="definition">are sampled at 10 Hz.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the frame decimation</span>
        <span itemprop="definition">was used to sample the video at 2 Hz and concatenate frames across 6 secs of the stream to encapsulate the 13 frames temporal window.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the images</span>
        <span itemprop="definition">were resized and were allowed the mismatch in the image dimensions between the two datasets to diversify the gap between them in the evaluation. The results for two models are reported below in Table 6.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the present disclosure</span>
        <span itemprop="definition">includes a modified 3D-CNN architecture with masked-attention employed for ego vehicle speed estimation using single-camera video streams.</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D-CNN</span>
        <span itemprop="definition">is effective in capturing temporal elements within an image sequence. However, it was determined that presence of background clutter and non-cohesive motion within the video stream often confused the model.</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the 3D-CNN</span>
        <span itemprop="definition">is modified to employ a masked-attention mechanism to steer the model to focus on relevant regions.</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the lane segmentation mask</span>
        <span itemprop="definition">is concatenated as an additional channel to the input images before feeding them to the 3D-CNN.</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the modified 3D-CNN</span>
        <span itemprop="definition">has demonstrated better performance in several evaluations with the inclusion of the masked-attention.</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a controller 1000</span>
        <span itemprop="definition">is a computing device which includes a CPU 1050 which can perform the processes described above.</span>
        <meta itemprop="num_attr" content="0109">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">may be an AI workstation running an operating system, for example Ubuntu Linux OS, Windows, a version of Unix OS, or Mac OS.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computer system 1000</span>
        <span itemprop="definition">may include one or more central processing units (CPU) 1050 having multiple cores.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computer system 1000</span>
        <span itemprop="definition">may include a graphics board 1012 having multiple GPUs, each GPU having GPU memory.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the graphics board 1012</span>
        <span itemprop="definition">may perform many of the mathematical operations of the disclosed machine learning methods.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computer system 1000</span>
        <span itemprop="definition">includes main memory 1002 , typically random access memory RAM, which contains the software being executed by the processing cores 1050 and GPUs 1012 , as well as a non-volatile storage device 1004 for storing data and the software programs.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Several interfaces for interacting with the computer system 1000</span>
        <span itemprop="definition">may be provided, including an I/O Bus Interface 1010 , Input/Peripherals 1018 such as a keyboard, touch pad, mouse, Display Adapter 1016 and one or more Displays 1008 , and a Network Controller 1006 to enable wired or wireless communication through a network 99.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the interfaces, memory and processors</span>
        <span itemprop="definition">may communicate over the system bus 1026 .</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computer system 1000</span>
        <span itemprop="definition">includes a power supply 1021 , which may be a redundant power supply.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computer system 1000</span>
        <span itemprop="definition">may include a server CPU and a graphics card by NVIDIA, in which the GPUs have multiple CUDA cores. In some embodiments, the computer system 1000 may include a machine learning engine 1012 .</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">circuitry configured to perform features described herein</span>
        <span itemprop="definition">may be implemented in multiple circuit units (e.g., chips), or the features may be combined in circuitry on a single chipset, as shown on FIG. 11 .</span>
        <meta itemprop="num_attr" content="0112">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 11</span>
        <span itemprop="definition">shows a schematic diagram of a data processing system 1100 used within the computing system, according to exemplary aspects of the present disclosure.</span>
        <meta itemprop="num_attr" content="0113">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the data processing system 1100</span>
        <span itemprop="definition">is an example of a computer in which code or instructions implementing the processes of the illustrative aspects of the present disclosure may be located.</span>
        <meta itemprop="num_attr" content="0113">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">data processing system 1180</span>
        <span itemprop="definition">employs a hub architecture including a north bridge and memory controller hub (NB/MCH) 1125 and a south bridge and input/output (I/O) controller hub (SB/ICH) 1120 .</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the central processing unit (CPU) 1130</span>
        <span itemprop="definition">is connected to NB/MCH 1125 .</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the NB/MCH 1125</span>
        <span itemprop="definition">also connects to the memory 1145 via a memory bus, and connects to the graphics processor 1150 via an accelerated graphics port (AGP).</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AGP</span>
        <span itemprop="definition">accelerated graphics port</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the NB/MCH 1125</span>
        <span itemprop="definition">also connects to the SB/ICH 1120 via an internal bus (e.g., a unified media interface or a direct media interface).</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the CPU Processing unit 1130</span>
        <span itemprop="definition">may contain one or more processors and even may be implemented using one or more heterogeneous processor systems.</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 12</span>
        <span itemprop="definition">shows one aspects of the present disclosure of CPU 1130 .</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instruction register 1238</span>
        <span itemprop="definition">retrieves instructions from the fast memory 1240 . At least part of these instructions is fetched from the instruction register 1238 by the control logic 1236 and interpreted according to the instruction set architecture of the CPU 1130 . Part of the instructions can also be directed to the register 1232 .</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions</span>
        <span itemprop="definition">are decoded according to a hardwired method, and in another aspect of the present disclosure the instructions are decoded according to a microprogram that translates instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions</span>
        <span itemprop="definition">are executed using the arithmetic logic unit (ALU) 1234 that loads values from the register 1232 and performs logical and mathematical operations on the loaded values according to the instructions.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the results from these operations</span>
        <span itemprop="definition">can be feedback into the register and/or stored in the fast memory 1240 .</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instruction set architecture of the CPU 1130</span>
        <span itemprop="definition">can use a reduced instruction set architecture, a complex instruction set architecture, a vector processor architecture, a very large instruction word architecture.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the CPU 1130</span>
        <span itemprop="definition">can be based on the Von Neuman model or the Harvard model.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the CPU 1130</span>
        <span itemprop="definition">can be a digital signal processor, an FPGA, an ASIC, a PLA, a PLD, or a CPLD.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the CPU 1130</span>
        <span itemprop="definition">can be an x86 processor by Intel or by AMD; an ARM processor, a Power architecture processor by, e.g., IBM; a SPARC architecture processor by Sun Microsystems or by Oracle; or other known CPU architecture.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the data processing system 1180</span>
        <span itemprop="definition">can include that the SB/ICH 1120 is coupled through a system bus to an I/O Bus, a read only memory (ROM) 1156 , universal serial bus (USB) port 1164 , a flash binary input/output system (BIOS) 1168 , and a graphics controller 1158 .</span>
        <meta itemprop="num_attr" content="0116">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">PCI/PCIe devices</span>
        <span itemprop="definition">can also be coupled to SB/ICH 1120 through a PCI bus 1162 .</span>
        <meta itemprop="num_attr" content="0116">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the PCI devices</span>
        <span itemprop="definition">may include, for example, Ethernet adapters, add-in cards, and PC cards for notebook computers.</span>
        <meta itemprop="num_attr" content="0117">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the Hard disk drive 1160 and CD-ROM 1156</span>
        <span itemprop="definition">can use, for example, an integrated drive electronics (IDE) or serial advanced technology attachment (SATA) interface.</span>
        <meta itemprop="num_attr" content="0117">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">IDE</span>
        <span itemprop="definition">integrated drive electronics</span>
        <meta itemprop="num_attr" content="0117">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SATA</span>
        <span itemprop="definition">serial advanced technology attachment</span>
        <meta itemprop="num_attr" content="0117">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the I/O bus</span>
        <span itemprop="definition">can include a super I/O (SIO) device.</span>
        <meta itemprop="num_attr" content="0117">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hard disk drive (HDD) 1160 and optical drive 1166</span>
        <span itemprop="definition">can also be coupled to the SB/ICH 1120 through a system bus.</span>
        <meta itemprop="num_attr" content="0118">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a keyboard 1170 , a mouse 1172 , a parallel port 1178 , and a serial port 1176</span>
        <span itemprop="definition">can be connected to the system bus through the I/O bus.</span>
        <meta itemprop="num_attr" content="0118">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Other peripherals and devices that can be connected to the SB/ICH 1120 using a mass storage controller</span>
        <span itemprop="definition">such as SATA or PATA, an Ethernet port, an ISA bus, an LPC bridge, SMBus, a DMA controller, and an Audio Codec.</span>
        <meta itemprop="num_attr" content="0118">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">circuitry described herein</span>
        <span itemprop="definition">may be adapted based on changes on battery sizing and chemistry, or based on the requirements of the intended back-up load to be powered.</span>
        <meta itemprop="num_attr" content="0119">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the functions and features described herein</span>
        <span itemprop="definition">may also be executed by various distributed components of a system.</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">one or more processors</span>
        <span itemprop="definition">may execute these system functions, wherein the processors are distributed across multiple components communicating in a network.</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the distributed components</span>
        <span itemprop="definition">may include one or more client and server machines, which may share processing, as shown by FIG. 13 , in addition to various human interface and communication devices (e.g., display monitors, smart phones, tablets, personal digital assistants (PDAs)). More specifically, FIG. 13 illustrates client devices including smart phone 1311 , tablet 1312 , mobile device terminal 1314 and fixed terminals 1316 . These client devices may be commutatively coupled with a mobile network service 1320 via base station 1356 , access point 1354 , satellite 1352 or via an internet connection.</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">PDAs</span>
        <span itemprop="definition">personal digital assistants</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Mobile network service 1320</span>
        <span itemprop="definition">may comprise central processors 1322 , server 1324 and database 1326 .</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Fixed terminals 1316 and mobile network service 1320</span>
        <span itemprop="definition">may be commutatively coupled via an internet connection to functions in cloud 1330 that may comprise security gateway 1332 , data center 1334 , cloud controller 1336 , data storage 1338 and provisioning tool 1340 .</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the network</span>
        <span itemprop="definition">may be a private network, such as a LAN or WAN, or may be a public network, such as the Internet.</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Input to the system</span>
        <span itemprop="definition">may be received via direct user input and received remotely either in real-time or as a batch process. Additionally, some aspects of the present disclosure may be performed on modules or hardware not identical to those described.</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the invention</span>
        <span itemprop="definition">may be practiced otherwise than as specifically described herein.</span>
        <meta itemprop="num_attr" content="0121">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the invention</span>
        <span itemprop="definition">may be practiced to utilize the speed of ego vehicle to estimate the speeds and locations of environment vehicles for in-vehicle motion and path planning.</span>
        <meta itemprop="num_attr" content="0121">
      </li>
    </ul>
  </section>

  


  <section itemprop="abstract" itemscope>
    <h2>Abstract</h2>
    
    <div itemprop="content" html><abstract mxw-id="PA654585166" lang="EN" source="national office" load-source="docdb">
    <div class="abstract">A system, apparatus and method of embedded ego vehicle speed estimation apparatus, including a car-mounted monocular camera for capturing a sequence of video frames of an outdoor scene from a moving car, where the outdoor scene includes a road, as a camera channel, and processing circuitry. The processing circuitry is configured with a single-shot network and a 3D convolutional neural network (3D-CNN), the single-shot network segments features of the road in the video frame sequence and generates a masked-attention map for the segmented road features, a concatenation circuit concatenates the masked-attention map as an additional channel to the camera channel to generate a masked-attention input, and the 3D-CNN network receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of the segmented road features in the video sequences.</div>
  </abstract>
  </div>
  </section>

  <section itemprop="description" itemscope>
    <h2>Description</h2>
    
    <div itemprop="content" html><div mxw-id="PDES446774412" lang="EN" load-source="patent-office" class="description">
    
    <heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
    <div id="p-0002" num="0001" class="description-paragraph">This application claims the benefit of priority to provisional application No. 63/426,211 filed Nov. 17, 2022, the entire contents of which are incorporated herein by reference.</div>
    <div id="p-0003" num="0002" class="description-paragraph">This application is related to provisional application No. 63/397,049 filed Aug. 18, 2022, the entire contents of which are incorporated herein by reference.</div>
    <div id="p-0004" num="0003" class="description-paragraph">This application is related to Attorney Docket No. 544445US titled âSystem and method to detect Tailgating Vehicle on high speed road from a moving vehicleâ, U.S. application Ser. No. 18/173,126 having a filing date of Feb. 23, 2023, the entire contents of which are incorporated herein by reference.</div>
    
    
    <heading id="h-0002">STATEMENT REGARDING PRIOR DISCLOSURE BY THE INVENTORS</heading>
    <div id="p-0005" num="0004" class="description-paragraph">Aspects of this technology are described in Mathew, Athul M., Thariq Khalid, and Riad souissi, â3DCMA: 3D Convolution with Masked Attention for Ego Vehicle Speed Estimation,â <i>Secure and Safe Autonomous Driving </i>(<i>SSAD</i>) <i>Workshop and Challenge, </i>Vancouver, Canada, Jun. 19, 2023, and preprint thereof, <i>arXiv preprint arXiv:</i>2212.05432 (2022), and is incorporated herein by reference in its entirety.</div>
    <heading id="h-0003">TECHNICAL FIELD</heading>
    <heading id="h-0004">Background</heading>
    <div id="p-0006" num="0005" class="description-paragraph">The present disclosure is directed to a neural network time series model, and preferably, a 3D Convolutional Neural Network (3D-CNN). with masked-attention (3D-CMA) architecture to estimate ego vehicle speed using a single front-facing monocular camera.</div>
    <heading id="h-0005">DESCRIPTION OF RELATED ART</heading>
    <div id="p-0007" num="0006" class="description-paragraph">Speed estimation of an ego vehicle is crucial to enable autonomous driving and advanced driver assistance technologies. Due to functional and legacy issues, conventional methods depend on in-car sensors to extract vehicle speed through the Controller Area Network (CAN) bus.</div>
    <div id="p-0008" num="0007" class="description-paragraph">The impact of electric vehicles today in contributing to an energy-efficient and sustainable world is immense. See Graeme Hill, Oliver Heidrich, Felix Creutzig, and Phil Blythe. The role of electric vehicles in near-term mitigation pathways and achieving the UK&#39;s carbon budget. <i>Applied Energy, </i>251:113111, 2019. Electric vehicles are a significant influencing factor in the global push against climate change. To this end, self-driving vehicles add further value by enabling smart mobility, planning, and control for intelligent transportation systems. Predicting the ego vehicle speed reduces fuel consumption and optimizes cruise control. See Chao Sun, Xiaosong Hu, Scott J Moura, and Fengchun Sun. Velocity predictors for predictive energy management in hybrid electric vehicles. <i>IEEE Transactions on Control Systems Technology, </i>23(3):1197-1204, 2014; and Thomas Stanger and Luigi del Re. A model predictive cooperative adaptive cruise control approach. In <i> <b>2013</b> American control conference, </i>pages 1374-1379. IEEE, 2013.</div>
    <div id="p-0009" num="0008" class="description-paragraph">Early work estimated ego-motion using correspondence points detection, road region detection, moving object detection, and other derived features. See Koichiro Yamaguchi, Takeo Kato, and Yoshiki Ninomiya. Vehicle ego-motion estimation and moving object detection using a monocular camera. In <i> <b>18</b>th International Conference on Pattern Recognition </i>(<i>ICPR&#39;<b>06</b> </i>), <figure-callout id="4" label="volume" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00011.png" state="{{state}}">volume</figure-callout> 4, pages 610-613. IEEE, 2006, incorporated herein by reference in its entirety. Furthermore, 8-point algorithm and RANSAC have been applied to get the essential matrix of ego-motion. See Richard I Hartley. In defense of the eight-point algorithm. <i>IEEE Transactions on pattern analysis and machine intelligence, </i>19(6):580-593, 1997; and Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. <i>Communications of the ACM, </i>24(6):381-395, 1981, each incorporated herein by reference in their entirety. Recent work implemented an end-to-end CNN-LSTM network to estimate the speed of an ego vehicle, seeHitesh Linganna Bandari and Binoy B Nair. âAn end to end learning based ego vehicle speed estimation system.â In 2021 IEEE International Power and Renewable Energy Conference (IPRECON), pages 1-8. IEEE, 2021, incorporated herein by reference in its entirety. The work performs evaluation on DBNet and comma.ai speed challenge dataset. See Yiping Chen, Jingkang Wang, Jonathan Li, Cewu Lu, Zhipeng Luo, Han Xue, and Cheng Wang. Lidar-video driving dataset: Learning driving policies effectively. In <i> <b>2018</b> IEEE/CVF Conference on Computer Vision and Pattern Recognition, </i>pages 5870-5878, 2018; and comma.ai speed challege. https://github.com/commaai/speedchallenge, 2018, incorporated herein by reference in their entirety. Other work has proposed speed estimation of vehicles from a CCTV point of view. See Hector Mejia, Esteban Palomo, Ezequiel LÃ³pez-Rubio, Israel Pineda, and Rigoberto Fonseca. Vehicle speed estimation using computer vision and evolutionary camera calibration. In <i>NeurIPS <b>2021</b> Workshop LatinX in AI, </i>2021, incorporated herein by reference in its entirety. Most require camera calibration and fixed view so that the vehicles pass through certain lines or regions of interest.</div>
    <div id="p-0010" num="0009" class="description-paragraph">FlowNet and PWC-Net are deep neural networks to estimate optical flow in videos. See Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition, </i>pages 2462-2470, 2017; and Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition, </i>pages 8934-8943, 2018, incorporated herein by reference in their entirety. FlowNet or PWC-Net can be used to estimate the ego vehicle speed. See RÃ³bert-Adrian Rill. Speed estimation evaluation on the kitti benchmark based on motion and monocular depth information. <i>arXiv preprint arXiv: </i>1907.06989, 2019; and Jun Hayakawa and Behzad Dariush. Ego-motion and surrounding vehicle state estimation using a monocular camera. In <i> <b>2019</b> IEEE Intelligent Vehicles Symposium </i>(<i>IV</i>), pages 2550-2556. IEEE, 2019, incorporated herein by reference in their entirety. However, ego vehicle speed estimation is performed by further post-processing on the optical flow pixel velocity. No work demonstrates end-to-end architecture capability where the speed could be learned with differentiation of the loss function.</div>
    <div id="p-0011" num="0010" class="description-paragraph">Accordingly it is one object of the present disclosure to provide a method and system for ego vehicle speed estimation that includes camera data video frames from a moving car, processed with a neural network time series model, in particular, 3D convolutional neural network (3D-CNN), that generates a masked-attention input which the 3D-CNN network uses to estimate a speed of the ego vehicle.</div>
    <heading id="h-0006">SUMMARY</heading>
    <div id="p-0012" num="0011" class="description-paragraph">An aspect of the present disclosure is a system for ego vehicle speed estimation. The system can include a car-mounted monocular camera for capturing a sequence of video frames of an outdoor scene from a moving car, where the outdoor scene includes a road, as a camera channel; processing circuitry configured with a single-shot network and a neural network time series model, the single-shot network segments features of the road in the video frame sequence and generates a masked-attention map for the segmented road features; a concatenation operation that concatenates the masked-attention map as an additional channel to the camera channel to generate a masked-attention input; the neural network time series model receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of the segmented road features in the video sequences; and output circuitry to output a signal indicating the estimated speed.</div>
    <div id="p-0013" num="0012" class="description-paragraph">A further aspect of the present disclosure is an embedded ego vehicle speed estimation apparatus. The apparatus can include processing circuitry configured with a single-shot network and a neural network time series model, the single-shot network segments features in a video frame sequence of a road and generates a masked-attention map for the segmented road features; a concatenation the neural network time seties model receives the masked-attention input and generates an estimated operation that concatenates the masked-attention map as an additional channel to a camera channel to generate a masked-attention input; speed of the ego vehicle based on displacement of the lane line segments in the video sequences; and output circuitry to output a signal indicating the estimated speed.</div>
    <div id="p-0014" num="0013" class="description-paragraph">A further aspect of the present disclosure is a non-transitory computer readable storage medium storing computer instructions, which when executed by processing circuitry, perform a method of ego vehicle speed estimation. The method can include segmenting, by a single-shot network, features in a video frame sequence of a road and generates a masked-attention map for the segmented road features; concatenating, by a concatenation operation, the masked-attention map as an additional channel to a camera channel to generate a masked-attention input; receiving, by a neural network time series model, the masked-attention input and generating an estimated speed of the ego vehicle based on displacement of the lane line segments in the video sequences; and outputting a signal indicating the estimated speed.</div>
    <div id="p-0015" num="0014" class="description-paragraph">The foregoing general description of the illustrative embodiments and the following detailed description thereof are merely exemplary aspects of the teachings of this disclosure, and are not restrictive.</div>
    
    
    <description-of-drawings>
      <heading id="h-0007">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <div id="p-0016" num="0015" class="description-paragraph">A more complete appreciation of the invention and many of the attendant advantages thereof will be readily obtained as the same becomes better understood by reference to the following detailed description when considered in connection with the accompanying drawings, wherein:</div>
      <div id="p-0017" num="0016" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> illustrates estimation of ego-vehicle speed using a continuous camera stream;</div>
      <div id="p-0018" num="0017" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> is a top view of an exemplary vehicle having video cameras mounted thereon;</div>
      <div id="p-0019" num="0018" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>3</b> </figref> illustrates an exemplary USB dashcam;</div>
      <div id="p-0020" num="0019" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref> is a block diagram of a hardware implementation of a tailgating detection system in accordance with an exemplary aspect of the disclosure;</div>
      <div id="p-0021" num="0020" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref> is a block diagram of an architecture of 3D-CMA;</div>
      <div id="p-0022" num="0021" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> is a block diagram of an architecture having lane line segmentation including an encoder and a decoder;</div>
      <div id="p-0023" num="0022" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref> is a block diagram of an architecture of ViViT;</div>
      <div id="p-0024" num="0023" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>D</figref> illustrate visualization of sample images of the KITTI and nuImages dataset;</div>
      <div id="p-0025" num="0024" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>D</figref> are graphs of train/test speed data distribution for nuImages and KITTI datasets; and</div>
      <div id="p-0026" num="0025" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref> is an illustration of a non-limiting example of details of computing hardware used in the computing system, according to aspects of the present disclosure;</div>
      <div id="p-0027" num="0026" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>11</b> </figref> is an exemplary schematic diagram of a data processing system used within the computing system, according to aspects of the present disclosure;</div>
      <div id="p-0028" num="0027" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>12</b> </figref> is an exemplary schematic diagram of a processor used with the computing system, according to aspects of the present disclosure; and</div>
      <div id="p-0029" num="0028" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>13</b> </figref> is an illustration of a non-limiting example of distributed components that may share processing with the controller, according to aspects of the present disclosure.</div>
    </description-of-drawings>
    
    
    <heading id="h-0008">DETAILED DESCRIPTION</heading>
    <div id="p-0030" num="0029" class="description-paragraph">In the drawings, like reference numerals designate identical or corresponding parts throughout the several views. Further, as used herein, the words âa,â âanâ and the like generally carry a meaning of âone or more,â unless stated otherwise. The drawings are generally drawn to scale unless specified otherwise or illustrating schematic structures or flowcharts.</div>
    <div id="p-0031" num="0030" class="description-paragraph">Furthermore, the terms âapproximately,â âapproximate,â âabout,â and similar terms generally refer to ranges that include the identified value within a margin of 20%, 10%, or preferably 5%, and any values therebetween.</div>
    <div id="p-0032" num="0031" class="description-paragraph">The present disclosure provides effective yet simple modular components for autonomous or intelligent traffic systems. Advanced Driver Assistance Systems (ADAS) are being made to improve automotive safety. Vehicles may offer driver assistance technologies including Autonomous Emergency Braking and a safe distance warning. ADAS may take into consideration environmental conditions and vehicle performance characteristics. Environmental conditions can be obtained using vehicle environment sensors. Vehicle cameras can capture a continuous camera stream. The term ego vehicle refers to a vehicle that contains vehicle environment sensors that perceive the environment around the vehicle. Edge computing devices are computing devices that are proximate to the data source, such as vehicle environment sensors.</div>
    <div id="p-0033" num="0032" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> illustrates estimation of ego-vehicle speed using a continuous camera stream. The present disclosure includes a 3D Convolutional Neural Network (3D-CNN) architecture trained on short videos using corresponding grayscale image frames <b>102</b> and corresponding focus masks, such as masks that focus on <figure-callout id="104" label="road lane lines" filenames="US11861853-20240102-D00001.png" state="{{state}}">road lane lines</figure-callout> <b>104</b>, lane line segmentation masks. The neural network architecture is used to estimate the <figure-callout id="112" label="speed" filenames="US11861853-20240102-D00001.png" state="{{state}}">speed</figure-callout> <b>112</b> of the ego vehicle, which can, in turn help in ADAS, including, among other things, to estimate the speed of vehicles of interest (VOI) in the surrounding environment.</div>
    <div id="p-0034" num="0033" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> is a top view of an exemplary ego vehicle having video cameras mounted thereon. Video cameras mounted on an ego vehicle may be used to obtain video images to be used to estimate the speed of the ego vehicle. The <figure-callout id="200" label="ego vehicle" filenames="US11861853-20240102-D00002.png,US11861853-20240102-D00010.png" state="{{state}}">ego vehicle</figure-callout> <b>200</b> can be of any make in the market. A non-limiting ego vehicle can be equipped with a number of <figure-callout id="204" label="exterior cameras" filenames="US11861853-20240102-D00002.png" state="{{state}}">exterior cameras</figure-callout> <b>204</b> and <figure-callout id="210" label="interior cameras" filenames="US11861853-20240102-D00002.png" state="{{state}}">interior cameras</figure-callout> <b>210</b>. One camera <b>214</b> for speed estimation can be mounted on the front dash, the front windshield, or embedded on the front portion of the exterior body and/or on the ego vehicle roof in order to capture images in front of the ego vehicle for external vehicles. The camera is preferably mounted integrally with a rearview/side mirror on the driver&#39;s side of the ego vehicle on a forward-facing surface (i.e., facing traffic preceding the ego vehicle). In this position the camera is generally oriented within the view of an individual inside the ego vehicle such that a driver can concurrently check for oncoming traffic behind the ego vehicle using the rearview side mirror and monitor the position of preceding vehicles.</div>
    <div id="p-0035" num="0034" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>3</b> </figref> illustrates an exemplary exterior-facing camera, which may be, but is not limited to, a <figure-callout id="310" label="USB camera" filenames="US11861853-20240102-D00003.png" state="{{state}}">USB camera</figure-callout> <b>310</b> with a base that can be attached to the rearview mirror, side mirror, windshield, dashboard, front body panel, or roof of the <figure-callout id="200" label="ego vehicle" filenames="US11861853-20240102-D00002.png,US11861853-20240102-D00010.png" state="{{state}}">ego vehicle</figure-callout> <b>200</b>, to name a few. The <figure-callout id="310" label="camera" filenames="US11861853-20240102-D00003.png" state="{{state}}">camera</figure-callout> <b>310</b> can be a USB camera for connection to an edge computing device, that is proximate to the USB camera, by a USB cable. The <figure-callout id="310" label="USB camera" filenames="US11861853-20240102-D00003.png" state="{{state}}">USB camera</figure-callout> <b>310</b> may be of any make which can channel a video stream. In one embodiment, the speed estimation apparatus is an all-in-one portable module that is removably mounted on a <figure-callout id="200" label="ego vehicle" filenames="US11861853-20240102-D00002.png,US11861853-20240102-D00010.png" state="{{state}}">ego vehicle</figure-callout> <b>200</b>. Preferably the all-in-one portable module has a camera back plate which is curved to generally match the contours of the forward-facing surface of a side view mirror, e.g., an ovoidal shape having a flat inner surface matching the contours of the forward face of the side view mirror and a curved dome-like front surface with the camera lens/opening located at an apex of the dome shape. The back plate is optionally integral with a neck portion that terminates in a thin plate having a length of 5-20 cm which can be inserted into the gap between the window and patrol vehicle door to secure the all-in-one portable module to the ego vehicle. A cable and/or wireless capability may be included to transfer captured images to the edge computing device while the ego vehicle is moving.</div>
    <div id="p-0036" num="0035" class="description-paragraph">The <figure-callout id="310" label="video camera" filenames="US11861853-20240102-D00003.png" state="{{state}}">video camera</figure-callout> <b>310</b> is capable of capturing a sequence of image frames at a predetermined frame rate. The frame rate may be fixed or may be adjusted in a manual setting, or may be set based on the mode of image capture. For example, a video camera may have an adjustable frame rate for image capture, or may automatically set a frame rate depending on the type of image capture. A burst image may be set for one predetermined frame rate, while video capture may be set for another predetermined frame rate.</div>
    <div id="p-0037" num="0036" class="description-paragraph">In embodiments, ego vehicle speed is estimated based on video images of the surrounding environment. In some embodiments, the speed estimation is determined using machine learning technology. 2D Convolutional Neural Networks have proven to be excellent at extracting feature maps for images and are predominantly used for understanding the spatial aspects of images relevant to image classification and object detection. However, 2D Convolutional Neural Networks cannot capture the spatio-temporal features of videos spread across multiple continuous frames.</div>
    <div id="p-0038" num="0037" class="description-paragraph">Neural network time series models can be configured for video classification. Neural network approaches that have been used for time series prediction include recurrent neural networks (RNN) and long short-term memory (LSTM) neural networks.</div>
    <div id="p-0039" num="0038" class="description-paragraph">In addition, 3D Convolutional Neural Networks can learn spatio-temporal features and thus help in video classification, human action recognition, and sign language recognition. Attention on top of 3D-CNN has also been used. See Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video action transformer network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 244-253, 2019; Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick. Long-term feature banks for detailed video understanding. In <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, </i>pages 284-293, 2019; and Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition, </i>pages 7794-7803, 2018, each incorporated herein by reference in their entirety. However, they are limited to action recognition use cases. Regression can also be performed using 3D-CNNs. See Agne Grinciunaite, Amogh Gudi, Emrah Tasli, and Marten den Uyl. Human pose estimation in space and time using 3d cnn. In <i>European Conference on Computer Vision, </i>pages 32-39. Springer, 2016; Xiaoming Deng, Shuo Yang, Yinda Zhang, Ping Tan, Liang Chang, and Hongan Wang. Hand3d: Hand pose estimation using 3d neural network. <i>arXiv preprint arXiv: </i>1704.02224, 2017; and Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann. 3d convolutional neural networks for efficient and robust hand pose estimation from single depth images. In Proceedings of the <i>IEEE conference on computer vision and pattern recognition, </i>pages 1991-2000, 2017, each incorporated herein by reference in their entirety. However, the approaches perform regression perform spatial localization-related tasks such as human pose or 3D hand pose.</div>
    <div id="p-0040" num="0039" class="description-paragraph">Vision Transformers (ViTs) capitalize on processes used in transformers in the field of Natural Language Processing. A non-overlapping takes patches of an image and creates token embeddings after performing linear projection. See Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Thai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. <i>arXiv preprint arXiv: </i>2010.11929, 2020, incorporated herein by reference in its entirety. These embeddings are concatenated with position embeddings, after which they are processed with the transformer block, which contains layer normalization, Multi-Head Attention, and MLP operations to produce a final classification output. ViTs have been used to replace CNNs, they lack the inductive bias, whereas CNN&#39;s are translation invariant due to the local neighborhood structure of the convolution kernels. Moreover, transformers have quadratic complexity for their operations and scale with the input dimensions. On the other hand, ViTs provide global attention and long-range interaction.</div>
    <div id="p-0041" num="0040" class="description-paragraph">The inventors have determined that a hybrid CNN-Transformer with a CNN backbone, referred to as 3D-CNN with masked attention (3D-CMA) can outperform the pure ViT approach.</div>
    <div id="p-0042" num="0041" class="description-paragraph">Video transformer architectures can be classified based on the embeddings (backbone and minimal embeddings), tokenization (patch tokenization, frame tokenization, clip tokenization), and positional embeddings.</div>
    <div id="p-0043" num="0042" class="description-paragraph">In disclosed embodiments, the ego vehicle speed is estimated by relying purely on video streams from a monocular camera. The ego vehicle speed can be estimated by onboard hardware that implements a neural network time series model. In some embodiments, the ego vehicle speed is estimated using a hybrid CNN-Transformer (3D-CMA).</div>
    <div id="p-0044" num="0043" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref> is a block diagram of an onboard hardware implementation of an ego vehicle speed estimation system in accordance with an exemplary aspect of the disclosure. The hardware implementation of the <figure-callout id="400" label="speed estimation system" filenames="US11861853-20240102-D00004.png,US11861853-20240102-D00010.png" state="{{state}}">speed estimation system</figure-callout> <b>400</b> includes an image/video capturing device (video camera <b>310</b>) and an <figure-callout id="420" label="edge computing device" filenames="US11861853-20240102-D00004.png" state="{{state}}">edge computing device</figure-callout> <b>420</b>. The <figure-callout id="310" label="video camera" filenames="US11861853-20240102-D00003.png" state="{{state}}">video camera</figure-callout> <b>310</b> is capable of capturing a sequence of image frames at a predetermined frame rate. The frame rate may be fixed or may be adjusted in a manual setting, or be set based on the mode of image capture. For example, a video camera may have an adjustable frame rate for image capture, or may automatically set a frame rate depending on the type of image capture. A burst image may be set for one predetermined frame rate, while video capture may be set for another predetermined frame rate.</div>
    <div id="p-0045" num="0044" class="description-paragraph">The <figure-callout id="420" label="edge computing device" filenames="US11861853-20240102-D00004.png" state="{{state}}">edge computing device</figure-callout> <b>420</b> is configured as an embedded processing circuitry for ego vehicle speed estimation. In one embodiment, the <figure-callout id="420" label="edge computing device" filenames="US11861853-20240102-D00004.png" state="{{state}}">edge computing device</figure-callout> <b>420</b> is a portable, or removably mounted, computing device which is equipped with a Graphical Processing Unit (GPU) or a type of machine learning engine, as well as a general purpose central processing unit (CPU) <b>422</b>, and its internal modules. The <figure-callout id="420" label="edge computing device" filenames="US11861853-20240102-D00004.png" state="{{state}}">edge computing device</figure-callout> <b>420</b> provides computing power that is sufficient for machine learning inferencing in real time for tasks including vehicle speed estimation and object detection, preferably all with a single monocular camera. Internal modules can include communication modules, such as Global System for Mobile Communication (GSM) <b>426</b> and Global Positioning System (GPS) <b>424</b>, as well as an <figure-callout id="414" label="input interface" filenames="US11861853-20240102-D00004.png" state="{{state}}">input interface</figure-callout> <b>414</b> for connection to the vehicle network (Controller Area Network, CAN). A <figure-callout id="412" label="supervisory unit" filenames="US11861853-20240102-D00004.png" state="{{state}}">supervisory unit</figure-callout> <b>412</b> may control input and output communication with the vehicle internal network. In one embodiment, the GPU/CPU configured <figure-callout id="420" label="edge computing device" filenames="US11861853-20240102-D00004.png" state="{{state}}">edge computing device</figure-callout> <b>420</b> is an NVIDIA Jetson Series (including Orin, Xavier, Tx2, Nano) system on module or an equivalent high-performance processing module from any other manufacturer like Intel, etc. The <figure-callout id="310" label="video camera" filenames="US11861853-20240102-D00003.png" state="{{state}}">video camera</figure-callout> <b>310</b> may be connected to the <figure-callout id="420" label="edge computing device" filenames="US11861853-20240102-D00004.png" state="{{state}}">edge computing device</figure-callout> <b>420</b> by a plug-in wired connection, such as USB, or may communicate with the <figure-callout id="420" label="edge computing device" filenames="US11861853-20240102-D00004.png" state="{{state}}">edge computing device</figure-callout> <b>420</b> by a wireless connection, such as Bluetooth Low Energy, depending on distance to the edge device and/or communication quality in a vehicle. This set up is powered by the vehicle&#39;s battery as a power source. A <figure-callout id="416" label="power management component" filenames="US11861853-20240102-D00004.png" state="{{state}}">power management component</figure-callout> <b>416</b> may control or regulate power to the GPU/<figure-callout id="422" label="CPU" filenames="US11861853-20240102-D00004.png" state="{{state}}">CPU</figure-callout> <b>422</b>, on an as needed basis.</div>
    <div id="p-0046" num="0045" class="description-paragraph">A time-series model must be utilized to capture the relative motion between adjacent image data samples.</div>
    <div id="p-0047" num="0046" class="description-paragraph">As a basis, a 2D convolution operation over an image I using a kernel K of size mÃn is:</div>
    <div id="p-0048" num="0047" class="description-paragraph">
      <maths id="MATH-US-00001" num="00001">
        <math overflow="scroll">
          <mrow>
            <mrow>
              <mi>S</mi>
              <mo>â¡</mo>
              <mo>(</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mo>=</mo>
            <mrow>
              <mrow>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>I</mi>
                    <mo>*</mo>
                    <mi>K</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mo>â¢</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>,</mo>
                    <mi>j</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mrow>
                <munder>
                  <mo>â</mo>
                  <mi>m</mi>
                </munder>
                <mrow>
                  <munder>
                    <mo>â</mo>
                    <mi>n</mi>
                  </munder>
                  <mrow>
                    <mrow>
                      <mi>I</mi>
                      <mo>â¡</mo>
                      <mo>(</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mi>j</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mo>â¢</mo>
                    <mrow>
                      <mi>K</mi>
                      <mo>â¡</mo>
                      <mo>(</mo>
                      <mrow>
                        <mrow>
                          <mi>i</mi>
                          <mo>-</mo>
                          <mi>m</mi>
                        </mrow>
                        <mo>,</mo>
                        <mrow>
                          <mi>j</mi>
                          <mo>-</mo>
                          <mi>n</mi>
                        </mrow>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </math>
      </maths>
    </div>
    <div id="p-0049" num="0048" class="description-paragraph">See Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. <i>Deep Learning. </i>MIT Press, Cambridge, MA, USA, 2016, which is incorporated herein by reference in its entirety.</div>
    <div id="p-0050" num="0049" class="description-paragraph">Expanding further on the above equation, the 3D convolution operation can be expressed as:</div>
    <div id="p-0051" num="0050" class="description-paragraph"> <maths id="MATH-US-00002" num="00002"> <math overflow="scroll"> <mrow> <mrow> <mi>S</mi> <mo>â¡</mo> <mo>(</mo> <mrow> <mi>h</mi> <mo>,</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> <mo>=</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mi>I</mi> <mo>*</mo> <mi>K</mi> </mrow> <mo>)</mo> </mrow> <mo>â¢</mo> <mrow> <mo>(</mo> <mrow> <mi>h</mi> <mo>,</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <munder> <mo>â</mo> <mi>l</mi> </munder> <mrow> <munder> <mo>â</mo> <mi>m</mi> </munder> <mrow> <munder> <mo>â</mo> <mi>n</mi> </munder> <mrow> <mrow> <mi>I</mi> <mo>â¡</mo> <mo>(</mo> <mrow> <mi>h</mi> <mo>,</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> <mo>â¢</mo> <mrow> <mi>K</mi> <mo>â¡</mo> <mo>(</mo> <mrow> <mrow> <mi>h</mi> <mo>-</mo> <mi>l</mi> </mrow> <mo>,</mo> <mrow> <mi>i</mi> <mo>-</mo> <mi>m</mi> </mrow> <mo>,</mo> <mrow> <mi>j</mi> <mo>-</mo> <mi>n</mi> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mrow> </mrow> </math> </maths> <br/>
where h is the additional dimension that includes the number of frames the kernel has to go through. In one embodiment, the kernel is convoluted with the concatenation of the grayscale images and lane line segmentation masks.
</div>
    <div id="p-0052" num="0051" class="description-paragraph">To this extent, a 3D-CNN network is incorporated to preserve the temporal information of the input signals and compute the ego vehicle speed. 3D-CNNs can learn spatial and temporal features simultaneously using 3D kernels. In one embodiment, small receptive fields of 3Ã3Ã3 are used as the convolutional kernels throughout the network. Many 3D-CNN architectures lose big chunks of temporal information after the first 3D pooling layer. This is especially valid in the case of short-term spatio-temporal features propagated by utilizing smaller temporal windows. The pooling kernel size is dÃkÃk, where d is the kernel temporal depth, and s is the spatial kernel size. In one embodiment, d=1 is used for the first max pooling layer to preserve the temporal information. In this embodiment, it can be ensured that the temporal information does not collapse entirely after the initial convolutional layers.</div>
    <div id="p-0053" num="0052" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref> is a block diagram of an architecture of 3D-CNN with masked attention (3D-CMA). <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> is a block diagram of an architecture having lane line segmentation including an encoder and a decoder as part of the masked attention layer. In some embodiments, a masked-<figure-callout id="504" label="attention layer" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">attention layer</figure-callout> <b>504</b> is added into the 3D-<figure-callout id="500" label="CNN architecture" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">CNN architecture</figure-callout> <b>500</b> to guide the model to focus on relevant features that help with ego-vehicle speed computation. In one embodiment, the relevant features are road lane lines.</div>
    <div id="p-0054" num="0053" class="description-paragraph">An image of an outdoor scene captured from a moving car typically has significant clutter and random motion that can obscure the model learning. For example, a scene can be obstructed by other moving vehicles, moving pedestrians, or birds and other animals. Road work zones and temporary markers or lane markings may create unusual views of the road. In some cases, road markings may transition from temporary markings in work zones to regular lane line markings. Some roads may offer periodic mile markers.</div>
    <div id="p-0055" num="0054" class="description-paragraph">A 3D-CNN model is preferably trained to filter out the irrelevant movements (such as that of other cars, pedestrians, etc.) that do not contribute towards the ego-vehicle speed estimation and focus only on features that matter. However, such a 3D-CNN model typically requires training with large quantities of data. In a more practical scenario where unlimited resources are not available, adding masked-attention helps to attain improved model performance with faster model convergence. As shown herein, the error in speed estimation is reduced by adding masked-attention to the 3D-<figure-callout id="500" label="CNN network" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">CNN network</figure-callout> <b>500</b>. Further details about the impact of masked-attention are described as part of an ablation study below.</div>
    <div id="p-0056" num="0055" class="description-paragraph">Convolutional neural networks comprise a learned set of filters, where each filter extracts a different feature from the image. An object is to inhibit or exhibit the activation of features based on the appearance of objects of interest in the images. Typical scenes captured by car-mounted imaging devices include background objects such as the sky, and other vehicles in the environment, which do not contribute to ego-vehicle speed estimation. In fact, the relative motion of environmental vehicles often contributes negatively to the ability of the neural network to inhibit irrelevant features.</div>
    <div id="p-0057" num="0056" class="description-paragraph">To inhibit and exhibit features based on relevance, a masked-<figure-callout id="506" label="attention map" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">attention map</figure-callout> <b>506</b> is concatenated to the <figure-callout id="502" label="input image" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">input image</figure-callout> <b>502</b> before passing an input image through the neural network. Regarding <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref>, a single-<figure-callout id="504" label="shot network" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">shot network</figure-callout> <b>504</b> is used with a shared <figure-callout id="614" label="encoder" filenames="US11861853-20240102-D00006.png" state="{{state}}">encoder</figure-callout> <b>614</b> and three separate decoders that accomplish specific tasks such as object detection, drivable area segmentation, and lane line segmentation. Preferably, there are no complex and/or redundant shared blocks between different decoders, which reduces computational consumption. CSP-Darknet is preferably used as the <figure-callout id="614" label="backbone network" filenames="US11861853-20240102-D00006.png" state="{{state}}">backbone network</figure-callout> <b>614</b> of the encoder, while the neck is mainly composed of Spatial Pyramid Pooling (SPP) <figure-callout id="616" label="module" filenames="US11861853-20240102-D00006.png" state="{{state}}">module</figure-callout> <b>616</b> and Feature Pyramid Network (FPN) module. See Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling cross stage partial network, 2020; Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In <i>Computer VisionâECCV <b>2014</b>, </i>pages 346-361. Springer International Publishing, 2014; and Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection, 2016, each incorporated herein by reference in their entirety. SPP generates and fuses features of different scales, and FPN fuses features at different semantic levels, making the generated features contain multiple scales and semantic level information.</div>
    <div id="p-0058" num="0057" class="description-paragraph">In one embodiment, the masked-<figure-callout id="506" label="attention map" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">attention map</figure-callout> <b>506</b> is generated from <figure-callout id="502" label="input video sequences" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">input video sequences</figure-callout> <b>502</b> using the lane <figure-callout id="504" label="line segmentation branch" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">line segmentation branch</figure-callout> <b>504</b>. The <figure-callout id="512" label="concatenation" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">concatenation</figure-callout> <b>512</b> of lane line segmentation as an additional channel to the camera channel allows the 3D-<figure-callout id="510" label="CNN" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">CNN</figure-callout> <b>510</b> to focus on the apparent displacement of the lane line segments in the video sequences to best estimate the ego-vehicle speed.</div>
    <div id="p-0059" num="0058" class="description-paragraph">Referring back to <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref>, the <figure-callout id="504" label="architecture" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">architecture</figure-callout> <b>504</b> for lane line segmentation includes an <figure-callout id="614" label="encoder" filenames="US11861853-20240102-D00006.png" state="{{state}}">encoder</figure-callout> <b>614</b> and a <figure-callout id="618" label="decoder" filenames="US11861853-20240102-D00006.png" state="{{state}}">decoder</figure-callout> <b>618</b>. The <figure-callout id="614" label="backbone network" filenames="US11861853-20240102-D00006.png" state="{{state}}">backbone network</figure-callout> <b>614</b> is used to extract the features of the <figure-callout id="612" label="input image" filenames="US11861853-20240102-D00006.png" state="{{state}}">input image</figure-callout> <b>612</b>. Typically, some classic image classification network serves as the backbone. In one embodiment, CSP Darknet is used as the backbone. The <figure-callout id="616" label="SPP" filenames="US11861853-20240102-D00006.png" state="{{state}}">SPP</figure-callout> <b>616</b> generates and fuses features of different scales.</div>
    <div id="p-0060" num="0059" class="description-paragraph">The lane <figure-callout id="618" label="line segmentation head" filenames="US11861853-20240102-D00006.png" state="{{state}}">line segmentation head</figure-callout> <b>618</b> is configured such that after three upsampling processes, an <figure-callout id="622" label="output feature map" filenames="US11861853-20240102-D00006.png" state="{{state}}">output feature map</figure-callout> <b>622</b> is restored to the size of (W; H; 2), which represents the probability of each pixel in the <figure-callout id="612" label="input image" filenames="US11861853-20240102-D00006.png" state="{{state}}">input image</figure-callout> <b>612</b> for the lane line and the background.</div>
    <div id="p-0061" num="0060" class="description-paragraph">In some embodiments, other road features may be used in the segmentation for masked attention. Other road features can include, but are not limited to, periodic reflectors marking road boundaries, road center rumble ridges, road barriers having reflective markings, and mile marker posts.</div>
    <div id="p-0062" num="0061" class="description-paragraph">In some embodiments, the background is used to classify a road condition. Road conditions can include wet road, dry road, icy road, or snow conditions, to name a few. In some embodiments, the background can be used to classify the type of road, including paved road vs an unfinished road.</div>
    <div id="p-0063" num="0062" class="description-paragraph">In some embodiments, multiple branches may be used in addition to lane <figure-callout id="504" label="line segmentation branch" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">line segmentation branch</figure-callout> <b>504</b> for determining masked attention maps. Each of the multiple branches may be for each of the different types of road features that can be used to focus attention for speed estimation.</div>
    <div id="p-0064" num="0063" class="description-paragraph">The 3D-CNN architecture with masked-attention (3D-CMA) for ego vehicle speed estimation is illustrated in <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref>.</div>
    <div id="p-0065" num="0064" class="description-paragraph">In the 3D-CNN architecture of <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref>, the RGB stream can be converted to grayscale since color information is not vital for speed estimation. However, a masked attention map <b>406</b> is concatenated <b>512</b> as an additional channel to the <figure-callout id="502" label="grayscale image" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">grayscale image</figure-callout> <b>502</b>. To reduce the computational complexity and memory requirement, the original input streams are resized to 64Ã64 before feeding them into the <figure-callout id="510" label="network" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">network</figure-callout> <b>510</b>. Thus, the input to the model has a dimension of nÃ64Ã64Ã2, where n is the number of frames in the temporal sequence.</div>
    <div id="p-0066" num="0065" class="description-paragraph">In one embodiment, all convolutional 3D layers <b>516</b>, <b>522</b> use a fixed kernel size of 3Ã3Ã3. The <figure-callout id="518" label="initial pooling layer" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">initial pooling layer</figure-callout> <b>518</b> uses a kernel size of 1Ã2Ã2 to preserve the temporal information. The <figure-callout id="524" label="subsequent pooling layer" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">subsequent pooling layer</figure-callout> <b>524</b>, which appears at the center of the network, compresses the temporal and spatial domains with a kernel size of 2Ã2Ã2. Six 3D convolutional layers <b>516</b>, <b>522</b>, <b>526</b>, <b>528</b> are incorporated with the number of filters for the layers from 1-6 being 32; 32; 64; 64; 128; 128 respectively. Finally, four fully <figure-callout id="532" label="connected layers" filenames="US11861853-20240102-D00000.png,US11861853-20240102-D00005.png" state="{{state}}">connected layers</figure-callout> <b>532</b>, 434, 436, 438 have 512; 256; 64 and 1 nodes.</div>
    <div id="p-0067" num="0066" class="description-paragraph">The L2 loss function which is used for training the 3D-CNN is as follows:</div>
    <div id="p-0068" num="0067" class="description-paragraph">
      <maths id="MATH-US-00003" num="00003">
        <math overflow="scroll">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <msub>
                    <mi>â</mi>
                    <mrow>
                      <mi>s</mi>
                      <mo>â¢</mo>
                      <mi>p</mi>
                      <mo>â¢</mo>
                      <mi>e</mi>
                      <mo>â¢</mo>
                      <mi>e</mi>
                      <mo>â¢</mo>
                      <mi>d</mi>
                    </mrow>
                  </msub>
                  <mo>=</mo>
                  <mrow>
                    <mfrac>
                      <mn>1</mn>
                      <mi>n</mi>
                    </mfrac>
                    <mo>â¢</mo>
                    <mrow>
                      <munderover>
                        <mo>â</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>0</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                      <msup>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <msub>
                              <mi>S</mi>
                              <mi>i</mi>
                            </msub>
                            <mo>-</mo>
                            <msub>
                              <mover accent="true">
                                <mi>S</mi>
                                <mi>Ë</mi>
                              </mover>
                              <mi>i</mi>
                            </msub>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                        <mn>2</mn>
                      </msup>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mo>=</mo>
                  <mrow>
                    <mfrac>
                      <mn>1</mn>
                      <mi>n</mi>
                    </mfrac>
                    <mo>â¢</mo>
                    <mrow>
                      <munderover>
                        <mo>â</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>0</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>S</mi>
                            <mi>i</mi>
                          </msub>
                          <mo>-</mo>
                          <mrow>
                            <msup>
                              <mi>W</mi>
                              <mi>T</mi>
                            </msup>
                            <mo>â¢</mo>
                            <mi>X</mi>
                          </mrow>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mo>=</mo>
                  <mrow>
                    <mfrac>
                      <mn>1</mn>
                      <mi>n</mi>
                    </mfrac>
                    <mo>â¢</mo>
                    <mrow>
                      <munderover>
                        <mo>â</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>0</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                      <msup>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <msub>
                              <mi>S</mi>
                              <mi>i</mi>
                            </msub>
                            <mo>-</mo>
                            <mrow>
                              <msup>
                                <mi>W</mi>
                                <mi>T</mi>
                              </msup>
                              <mo>(</mo>
                              <mrow>
                                <msub>
                                  <mi>X</mi>
                                  <mi>l</mi>
                                </msub>
                                <mo>+</mo>
                                <msub>
                                  <mi>X</mi>
                                  <mi>M</mi>
                                </msub>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                        <mn>2</mn>
                      </msup>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </maths>
    </div>
    <div id="p-0069" num="0068" class="description-paragraph">where n is the number of frames in the input and Si is the speed value ground truth of ith corresponding frame, and Si is the inferred speed value. Xi is the grayscale image channel, and X<sub>M </sub>is the masked-attention channel for every frame. W is the weight tensor of the 3D convolutional kernel.</div>
    <div id="p-0070" num="0069" class="description-paragraph">The ego vehicle speed estimation may encounter varying conditions, such as varying road markings, varying road conditions, and even varying road surface types. The ego vehicle speed estimation can be configured to go into power conserve modes depending on such varying conditions. In some embodiments, the onboard hardware implementation of an ego vehicle <figure-callout id="400" label="speed estimation system" filenames="US11861853-20240102-D00004.png,US11861853-20240102-D00010.png" state="{{state}}">speed estimation system</figure-callout> <b>400</b> may be configured to use power efficiently. The <figure-callout id="400" label="hardware implementation" filenames="US11861853-20240102-D00004.png,US11861853-20240102-D00010.png" state="{{state}}">hardware implementation</figure-callout> <b>400</b> can be configured to halt processing of the 3D-CNN network when the segmented features do not include road features that may be used to determine ego vehicle speed. The <figure-callout id="400" label="hardware implementation" filenames="US11861853-20240102-D00004.png,US11861853-20240102-D00010.png" state="{{state}}">hardware implementation</figure-callout> <b>400</b> can be configured to monitor ego vehicle speed obtained from internal sensors while the 3D-CNN network is in the halted state. The <figure-callout id="400" label="hardware implementation" filenames="US11861853-20240102-D00004.png,US11861853-20240102-D00010.png" state="{{state}}">hardware implementation</figure-callout> <b>400</b> can be configured to intermittently perform processing using the 3D-CNN network. The <figure-callout id="400" label="hardware implementation" filenames="US11861853-20240102-D00004.png,US11861853-20240102-D00010.png" state="{{state}}">hardware implementation</figure-callout> <b>400</b> can be configured to continuously monitor vehicle speed while the ego vehicle is in an operating state and periodically estimate speed of the ego vehicle using the 3D-CNN network.</div>
    <div id="p-0071" num="0070" class="description-paragraph">The effectiveness of the 3D-CMA model was evaluated. First, the public datasets used in experiments are described. Then the metrics used for evaluation are described. The 3D-CMA model architecture is compared against a ViViT, a state-of-the-art vision transformer architectures. Additionally, some ablation studies are described to characterize the contribution of masked-attention within the network architecture and compare its performance by discarding the same from the 3D-CNN.</div>
    <div id="p-0072" num="0071" class="description-paragraph">A Video Vision Transformer(ViViT) is used for some cases due to its representation of the 3D convolution in the form of Tubelet embedding. See Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luc{hacek over (â)}icâ², and Cordelia Schmid. Vivit: A video vision transformer. In <i>Proceedings of the IEEE/CVF International Conference on Computer Vision, </i>pages 6836-6846, 2021, incorporated herein by reference in its entirety. ViViT is easily reproducible and has a good balance between the parameters and accuracy for small datasets. Moreover, ViViT-H scores an accuracy of 95.8, just below the 95.9 accuracy score by Swin-L as per the Video Transformers Survey over HowTo100M. See Javier Selva, Anders S Johansen, Sergio Escalera, Kamal Nasrollahi, Thomas B Moeslund, and Albert ClapÃ©s. Video transformers: A survey. arXiv preprint arXiv:2201.05991, 2022; and Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In <i>Proceedings of the IEEE/CVF International Conference on Computer Vision, </i>pages 2630-2640, 2019, each incorporated herein by reference in their entirety.</div>
    <div id="p-0073" num="0072" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref> is a block diagram of an architecture of ViViT. In the ViViT, the frames from the video(N) are tokenized using 3D-Convolutional tubelet embeddings and further passed to multiple transformer encoders to regress the speed value finally.</div>
    <div id="p-0074" num="0073" class="description-paragraph">The ViViT includes extracting non-overlapping, spatio-temporal âtubesâ from the input volume, and to linearly project this to <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/67/0d/6e/7d45079d54ef25/US11861853-20240102-P00001.png"><img id="CUSTOM-CHARACTER-00001" he="3.22mm" wi="2.12mm" file="US11861853-20240102-P00001.TIF" alt="Figure US11861853-20240102-P00001" img-content="character" img-format="tif" orientation="portrait" inline="no" width="8" height="13" alt="Figure US11861853-20240102-P00001" class="patent-full-image" src="https://patentimages.storage.googleapis.com/67/0d/6e/7d45079d54ef25/US11861853-20240102-P00001.png"/></a></div> <sup>d</sup>. This method is an extension of ViT&#39;s embedding to 3D, and corresponds to a 3D convolution. For a tubelet of dimension</div>
    <div id="p-0075" num="0074" class="description-paragraph"> <maths id="MATH-US-00004" num="00004"> <math overflow="scroll"> <mrow> <mrow> <mi>t</mi> <mo>Ã</mo> <mi>h</mi> <mo>Ã</mo> <mi>w</mi> </mrow> <mo>,</mo> <mrow> <msub> <mi>n</mi> <mi>t</mi> </msub> <mo>=</mo> <mrow> <mo>[</mo> <mfrac> <mi>T</mi> <mi>t</mi> </mfrac> <mo>]</mo> </mrow> </mrow> <mo>,</mo> <mrow> <msub> <mi>n</mi> <mi>h</mi> </msub> <mo>=</mo> <mrow> <mrow> <mrow> <mo>[</mo> <mfrac> <mi>H</mi> <mi>h</mi> </mfrac> <mo>]</mo> </mrow> <mo>â¢</mo> <mtext>   </mtext> <mi>and</mi> <mo>â¢</mo> <mtext>   </mtext> <msub> <mi>n</mi> <mi>w</mi> </msub> </mrow> <mo>=</mo> <mrow> <mo>[</mo> <mfrac> <mi>W</mi> <mi>w</mi> </mfrac> <mo>]</mo> </mrow> </mrow> </mrow> <mo>,</mo> </mrow> </math> </maths> <br/>
tokens are extracted from temporal, height, and width dimensions respectively. Smaller tubelet dimensions thus result in more tokens which increases the computation.
</div>
    <div id="p-0076" num="0075" class="description-paragraph">A ViT extracts N non-overlapping image patches, x<sub>i</sub>âR<sup>hÃw</sup>, 602 performs a linear projection and then rasterises them into 1D tokens z<sub>i</sub>â<div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/30/a0/e6/0a19d21cd87215/US11861853-20240102-P00002.png"><img id="CUSTOM-CHARACTER-00002" he="3.22mm" wi="2.12mm" file="US11861853-20240102-P00002.TIF" alt="Figure US11861853-20240102-P00002" img-content="character" img-format="tif" orientation="portrait" inline="no" width="8" height="13" alt="Figure US11861853-20240102-P00002" class="patent-full-image" src="https://patentimages.storage.googleapis.com/30/a0/e6/0a19d21cd87215/US11861853-20240102-P00002.png"/></a></div> <sup>d</sup>. The sequence of tokens input to the following transformer encoder is
<br/>
<i>Z=[z</i> <sub>cls</sub> <i>, Ex</i>1, <i>Ex</i>2<i>, . . . , Ex</i> <sub>N</sub> <i>]+p </i>
<br/>
where the projection by E is equivalent to a 2D convolution.
</div>
    <div id="p-0077" num="0076" class="description-paragraph">As shown in <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref>, an optional learned classification <figure-callout id="704" label="token z" filenames="US11861853-20240102-D00007.png" state="{{state}}">token z<sub> </sub> </figure-callout> <sub>cls </sub> <b>704</b> is prepended to this sequence, and its representation at the final layer of the encoder serves as the final representation used by the classification layer. In addition, a learned positional embedding, pâ<div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/1d/2c/3b/b4b3a7e182ca30/US11861853-20240102-P00003.png"><img id="CUSTOM-CHARACTER-00003" he="3.22mm" wi="2.12mm" file="US11861853-20240102-P00003.TIF" alt="Figure US11861853-20240102-P00003" img-content="character" img-format="tif" orientation="portrait" inline="no" width="8" height="13" alt="Figure US11861853-20240102-P00003" class="patent-full-image" src="https://patentimages.storage.googleapis.com/1d/2c/3b/b4b3a7e182ca30/US11861853-20240102-P00003.png"/></a></div> <sup>NÃd </sup> <b>706</b>, is added to the tokens to retain positional information, as the subsequent self-attention operations in the transformer are permutation invariant. The tokens are then passed through an encoder consisting of a sequence of L transformer layers <b>710</b>. Each <figure-callout id="1" label="layer" filenames="US11861853-20240102-D00001.png" state="{{state}}">layer</figure-callout> 1 comprises of Multi-Headed Self-<figure-callout id="724" label="Attention" filenames="US11861853-20240102-D00007.png" state="{{state}}">Attention</figure-callout> <b>724</b>, layer normalisation (LN) <b>618</b>, <b>626</b>, and MLP blocks <b>716</b>.</div>
    <div id="p-0078" num="0077" class="description-paragraph">The Transformer Encoder can be trained with the spatio-temporal embeddings.</div>
    <div id="p-0079" num="0078" class="description-paragraph">There is a lack of standardized datasets available for the estimation of ego-vehicle speed from a monocular camera stream. DBNet is a large-scale dataset for driving behavior research which includes aligned videos and vehicular speed from 1000 km driving stretch. See Yiping Chen, Jingkang Wang, Jonathan Li, Cewu Lu, Zhipeng Luo, Han Xue, and Cheng Wang. Lidar-video driving dataset: Learning driving policies effectively. In <i> <b>2018</b> IEEE/CVF Conference on Computer Vision and Pattern Recognition, </i>pages 5870-5878, 2018, incorporated herein by reference in its entirety. However, the test set is not available for public usage. Likewise, the test set of comma.ai speed challenge is not open to the public. See comma.ai speed challenge, 2018, incorporated herein by reference in its entirety. KITTI dataset has been utilized for speed estimation using motion and monocular depth estimation. See RÃ³bert-Adrian Rill. Speed estimation evaluation on the kitti benchmark based on motion and monocular depth information, 2019, incorporated herein by reference in its entirety. However, there is no information about the train and test splits used for the evaluation of the models. In the present disclosure, two public datasets are utilized for experimentsânulmages and KITTI. Some sample images extracted from video sequences for nulmages and KITTI are shown in <figref idrefs="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>D</figref>.</div>
    <div id="p-0080" num="0079" class="description-paragraph">nulmages is derived from nuScenes and is a large-scale autonomous driving dataset having 93 k video clips of 6 seconds each. See Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020, incorporated herein by reference in its entirety. The dataset is collated from two diverse citiesâBoston and Singapore. Each video clip consists of 13 frames spaced out at 2 Hz. The annotated images include rain, snow, and night time, which are important for autonomous driving applications.</div>
    <div id="p-0081" num="0080" class="description-paragraph">Each sample in the nulmages dataset comprises of an annotated camera image with an associated timestamp and past and future images. It is to be noted that the six previous and six future images are not annotated. The sample frame has meta-data information available as token ids regarding the previous and future frames associated with the particular sample.</div>
    <div id="p-0082" num="0081" class="description-paragraph">The vehicle speed is extracted from the CAN bus data and linked to the sample data through sample tokens. The train and test splits of the nulmages dataset have been strictly followed for training and evaluating the AI models. The distribution of speed data across train and test splits of the nulmages dataset are shown in <figref idrefs="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>D</figref>.</div>
    <div id="p-0083" num="0082" class="description-paragraph">The KITTI Vision Benchmark Suite is a public dataset containing raw data recordings that are captured and synchronized at 10 Hz. See Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In <i> <b>2012</b> IEEE Conference on Computer Vision and Pattern Recognition, </i>pages 3354-3361, 2012; and A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets robotics: The kitti dataset. <i>The International Journal of Robotics Research, </i>32(11):1231-1237, 2013, each incorporated herein by reference in their entirety. Geiger et al., <b>2012</b>, presented the benchmark challenges, their creation and use for evaluating state-of-the-art computer vision methods, while Geiger et al., 2013, was a follow-up work that provided technical details on the raw data itself, describing the recording platform, the data format and the utilities.</div>
    <div id="p-0084" num="0083" class="description-paragraph">The dataset was captured by driving around the mid-size city of Karlsruhe. The âsynched+rectifiedâ processed data is utilized where images are rectified and undistorted and where the data frame numbers correspond across all sensor streams. While the dataset provides both grayscale and color stereo sequences, an RGB stream is utilized extracted from camera ID 03 only. The ego-vehicle speed values are extracted from IMU sensor readings. The raw data is split across six categoriesâCity, Residential, Road, Campus, Person, and Calibration. For an experiment, data from City and Road categories is utilized. Some video samples in the City category have prolonged periods where the car is stationary. Such video samples are discarded where the vehicle was stationary for most of the video samples. To facilitate future benchmarks from the research community for ego-vehicle speed estimation, train and test splits are reported in Table 1. The distribution of speed data across train and test splits from the KITTI dataset is shown in <figref idrefs="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>D</figref>.</div>
    <div id="p-0085" num="0084" class="description-paragraph">
      <tables id="TABLE-US-00001" num="00001">
        <patent-tables frame="none" colsep="0" rowsep="0">
          <table align="left" colsep="0" rowsep="0" cols="1" class="description-table" width="100%">
            <thead>
              <tr class="description-tr">
                <td namest="1" nameend="1" rowsep="1" class="description-td" colspan="1">TABLE 1</td>
              </tr>
            </thead>
            
              <tbody><tr class="description-tr">
                <td namest="1" nameend="1" align="center" rowsep="1" class="description-td" colspan="1"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">train and test video samples for KITTI dataset</td>
              </tr>
            
          </tbody></table>
          <table align="left" colsep="0" rowsep="0" cols="5" class="description-table" width="100%">
            <tbody><tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">KITTI</td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">Category</td>
                <td class="description-td">Train</td>
                <td class="description-td"> </td>
                <td class="description-td">Test</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="4" align="center" rowsep="1" class="description-td" colspan="5"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">City</td>
                <td class="description-td">2011_09_26_drive</td>
                <td class="description-td">0002, 0005, 0009</td>
                <td class="description-td">0001</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">0011, 0013, 0014</td>
                <td class="description-td">0117</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">0048, 0051, 0056</td>
                <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">0059, 0084, 0091</td>
                <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">0095, 0096, 0104</td>
                <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">0106, 0113</td>
                <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">2011_09_28_drive</td>
                <td class="description-td">0001</td>
                <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">2011_09_29_drive</td>
                <td class="description-td">0071</td>
                <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">Road</td>
                <td class="description-td">2011_09_26_drive</td>
                <td class="description-td">015, 0027, 0028</td>
                <td class="description-td">0070</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">0029, 0032, 0052</td>
                <td class="description-td">0101</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">2011_09_29_drive</td>
                <td class="description-td">0004, 0016, 0042</td>
                <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">0047</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="4" align="center" rowsep="1" class="description-td" colspan="5"> </td>
              </tr>
            
          </tbody></table>
        </patent-tables>
      </tables>
    </div>
    <div id="p-0086" num="0085" class="description-paragraph">The conventional evaluation protocol used in the literature for the task of regressionâMean Absolute Error (MAE) and Root Mean Square Error (RMSE)âwas used.</div>
    <div id="p-0087" num="0086" class="description-paragraph">The MAE and RMSE are computed as follows :</div>
    <div id="p-0088" num="0087" class="description-paragraph">
      <maths id="MATH-US-00005" num="00005">
        <math overflow="scroll">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mrow>
                    <mrow>
                      <mi>RMSE</mi>
                      <mo>=</mo>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                  <mo>â¢</mo>
                  <msqrt>
                    <mrow>
                      <mo>(</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mi>n</mi>
                      </mfrac>
                      <mo>)</mo>
                    </mrow>
                  </msqrt>
                  <mo>â¢</mo>
                  <mrow>
                    <munderover>
                      <mo>â</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>-</mo>
                        <mn>1</mn>
                      </mrow>
                      <mi>n</mi>
                    </munderover>
                    <msup>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>y</mi>
                            <mi>i</mi>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mover accent="true">
                              <mi>y</mi>
                              <mi>Ë</mi>
                            </mover>
                            <mi>i</mi>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mi>MAE</mi>
                  <mo>=</mo>
                  <mrow>
                    <mrow>
                      <mo>(</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mi>n</mi>
                      </mfrac>
                      <mo>)</mo>
                    </mrow>
                    <mo>â¢</mo>
                    <mrow>
                      <munderover>
                        <mo>â</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                      <mrow>
                        <semantics definitionURL="" encoding="">
                          <mo>â</mo>
                          <annotation encoding="Mathematica">&#34;\[LeftBracketingBar]&#34;</annotation>
                        </semantics>
                        <mrow>
                          <msub>
                            <mover accent="true">
                              <mover accent="true">
                                <mi>y</mi>
                                <mi>Ë</mi>
                              </mover>
                              <mi>Ë</mi>
                            </mover>
                            <mi>i</mi>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mi>y</mi>
                            <mi>i</mi>
                          </msub>
                        </mrow>
                        <semantics definitionURL="" encoding="">
                          <mo>â</mo>
                          <annotation encoding="Mathematica">&#34;\[RightBracketingBar]&#34;</annotation>
                        </semantics>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </maths>
    </div>
    <div id="p-0089" num="0088" class="description-paragraph">where y<sub>i </sub>denotes the ground truth ego-vehicle speed value and Å·<sub>i</sub>denotes the predicted speed value by the AI model.</div>
    <div id="p-0090" num="0089" class="description-paragraph">RGB images from the camera mounted in front of the vehicle are used and ego-vehicle velocity coming from the CAN-BUS across both public datasets. This information is synchronized. The KITTI dataset has a camera image resolution of 1238_374. The temporal dimension we used for the KITTI dataset is ten frames. The KITTI dataset is sampled at 10 Hz, which means that the models are fed with video frames containing visual information from a time window of 1 sec. The ego-vehicle velocity assigned to any temporal sequence is the speed value tagged to the closest time stamp of the 10th frame in the input sequence.</div>
    <div id="p-0091" num="0090" class="description-paragraph">On the other hand, the camera image resolution for the nulmages dataset is 1600_900. nulmages dataset is sampled at 2 Hz. Six frames each are taken, preceding and succeeding the sample frame. This means that the models are fed with video frames containing visual information spanning a time window of approximately 6 sec. The ego vehicle velocity assigned to any temporal sequence is the speed value tagged to the closest time-stamp of the sample frame (7th frame in the input sequence).</div>
    <div id="p-0092" num="0091" class="description-paragraph">For the experiments with ViViT, non-overlapping, spatio-temporal tubelet embeddings of dimension tÃhÃw are taken, where t=6, h=8, and w=8. The number of transformer layers in the implementation is 16. The number of heads for multi-headed self-attention blocks is 16, and the dimension of embeddings is 128.</div>
    <div id="p-0093" num="0092" class="description-paragraph">The AI models were trained using an Nvidia GeForce RTX-3070 Max-Q Design GPU having 8 GB VRAM. The learning rate used for training all models is 1Ã10<sup>â3</sup>. All models are trained for 100 epochs with early stopping criteria set to terminate the training process if validation loss does not improve for ten epochs consecutively. The optimizer utilized is Adam since it utilizes both momentum and scaling</div>
    <div id="p-0094" num="0093" class="description-paragraph">The performance of the proposed 3D-CMA architecture is evaluated and compared against the standard ViViT with spatio-temporal attention. The evaluation metrics are reported on the test set for KITTI and nulmages datasets in the subsections below. The evaluation across all datasets consistently reported better results for the 3D-CMA architecture.</div>
    <div id="p-0095" num="0094" class="description-paragraph">Evaluation scores for the nulmages dataset are shown in Table 2. Approximately 27% improvement was observed in RMSE and MAE for 3D-CMA compared to ViViT for the nulmages dataset.</div>
    <div id="p-0096" num="0095" class="description-paragraph">
      <tables id="TABLE-US-00002" num="00002">
        <patent-tables frame="none" colsep="0" rowsep="0">
          <table align="left" colsep="0" rowsep="0" cols="1" class="description-table" width="100%">
            <thead>
              <tr class="description-tr">
                <td namest="1" nameend="1" rowsep="1" class="description-td" colspan="1">TABLE 2</td>
              </tr>
            </thead>
            
              <tbody><tr class="description-tr">
                <td namest="1" nameend="1" align="center" rowsep="1" class="description-td" colspan="1"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">nuImages evaluation for (a) ViViT (b)3DCMA</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation Metric</td>
              </tr>
            
          </tbody></table>
          <table align="left" colsep="0" rowsep="0" cols="4" class="description-table" width="100%">
            <tbody><tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">Method</td>
                <td class="description-td">RMSE</td>
                <td class="description-td">MAE</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">VIViT</td>
                <td class="description-td">1.782</td>
                <td class="description-td">1.326</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">3D CMA</td>
                <td class="description-td">1.297</td>
                <td class="description-td">0.974</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
            
          </tbody></table>
        </patent-tables>
      </tables>
    </div>
    <div id="p-0097" num="0096" class="description-paragraph">The evaluation shows 34:5% and 41:5% improvement in RMSE and MAE respectively on the KITTI dataset for 3D-CMA compared to ViViT. The results are seen in Table 3.</div>
    <div id="p-0098" num="0097" class="description-paragraph">
      <tables id="TABLE-US-00003" num="00003">
        <patent-tables frame="none" colsep="0" rowsep="0">
          <table align="left" colsep="0" rowsep="0" cols="1" class="description-table" width="100%">
            <thead>
              <tr class="description-tr">
                <td namest="1" nameend="1" rowsep="1" class="description-td" colspan="1">TABLE 3</td>
              </tr>
            </thead>
            
              <tbody><tr class="description-tr">
                <td namest="1" nameend="1" align="center" rowsep="1" class="description-td" colspan="1"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation on KITTI dataset for (a) ViViT (b)3D-CMA</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation Metric</td>
              </tr>
            
          </tbody></table>
          <table align="left" colsep="0" rowsep="0" cols="4" class="description-table" width="100%">
            <tbody><tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">Method</td>
                <td class="description-td">RMSE</td>
                <td class="description-td">MAE</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">VIViT</td>
                <td class="description-td">5.024</td>
                <td class="description-td">4.324</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">3D CMA</td>
                <td class="description-td">3.290</td>
                <td class="description-td">2.528</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
            
          </tbody></table>
        </patent-tables>
      </tables>
    </div>
    <div id="p-0099" num="0098" class="description-paragraph">To further understand the importance of masked-attention, an ablation study was conducted by removing masked attention input to the 3D-CNN network. It is to be noted that the input to the 3D-CNN model is a single-channel grayscale image after the removal of the masked-attention input.</div>
    <div id="p-0100" num="0099" class="description-paragraph">Evaluation scores for the nulmages dataset are shown in Table 4. The addition of masked-attention reduces RMSE by 23:6% and MAE by 25:9% for the nulmages dataset.</div>
    <div id="p-0101" num="0100" class="description-paragraph">
      <tables id="TABLE-US-00004" num="00004">
        <patent-tables frame="none" colsep="0" rowsep="0">
          <table align="left" colsep="0" rowsep="0" cols="1" class="description-table" width="100%">
            <thead>
              <tr class="description-tr">
                <td namest="1" nameend="1" rowsep="1" class="description-td" colspan="1">TABLE 4</td>
              </tr>
            </thead>
            
              <tbody><tr class="description-tr">
                <td namest="1" nameend="1" align="center" rowsep="1" class="description-td" colspan="1"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation on nuImages dataset for (a)3D-CNN</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">without masked-attention (b)3D-CMA</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation Metric</td>
              </tr>
            
          </tbody></table>
          <figure-callout id="3D" label="Method RMSE MAE" filenames="US11861853-20240102-D00007.png" state="{{state}}">
                </figure-callout><table align="left" colsep="0" rowsep="0" cols="4" class="description-table" width="100%">
            <tbody><tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> <figure-callout id="3D" label="Method RMSE MAE" filenames="US11861853-20240102-D00007.png" state="{{state}}">Method</figure-callout> </td> <td class="description-td">RMSE</td>
                <td class="description-td">MAE</td>  <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">3D-CNN without MA</td>
                <td class="description-td">1.698</td>
                <td class="description-td">1.315</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">3D CMA</td>
                <td class="description-td">1.297</td>
                <td class="description-td">0.974</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
            
          </tbody></table>
        </patent-tables>
      </tables>
    </div>
    <div id="p-0102" num="0101" class="description-paragraph">Evaluation scores for the KITTI dataset are shown in Table 5. The addition of masked-attention reduces the RMSE by 25:8% and MAE by 30:1% for the KITTI dataset.</div>
    <div id="p-0103" num="0102" class="description-paragraph">
      <tables id="TABLE-US-00005" num="00005">
        <patent-tables frame="none" colsep="0" rowsep="0">
          <table align="left" colsep="0" rowsep="0" cols="1" class="description-table" width="100%">
            <thead>
              <tr class="description-tr">
                <td namest="1" nameend="1" rowsep="1" class="description-td" colspan="1">TABLE 5</td>
              </tr>
            </thead>
            
              <tbody><tr class="description-tr">
                <td namest="1" nameend="1" align="center" rowsep="1" class="description-td" colspan="1"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation on KITTI dataset for (a)3D-CNN</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">without masked-attention (b)3D-CMA</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation Metric</td>
              </tr>
            
          </tbody></table>
          <figure-callout id="3D" label="Method RMSE MAE" filenames="US11861853-20240102-D00007.png" state="{{state}}">
                </figure-callout><table align="left" colsep="0" rowsep="0" cols="4" class="description-table" width="100%">
            <tbody><tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td"> <figure-callout id="3D" label="Method RMSE MAE" filenames="US11861853-20240102-D00007.png" state="{{state}}">Method</figure-callout> </td> <td class="description-td">RMSE</td>
                <td class="description-td">MAE</td>  <td class="description-td"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">3D-CNN without MA</td>
                <td class="description-td">4.437</td>
                <td class="description-td">3.617</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">3D CMA</td>
                <td class="description-td">3.290</td>
                <td class="description-td">2.528</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
            
          </tbody></table>
        </patent-tables>
      </tables>
    </div>
    <div id="p-0104" num="0103" class="description-paragraph">To take into consideration the generalization ability of the AI models, evaluations were conducted across data sets and their accuracy was reported. It is to be noted that there is a shift in the domain when testing nuImages-trained AI models on the KITTI dataset due to the reasons stated in section 4.3. To test KITTI models on the nuImages dataset, ten frames are needed within a duration of 1 second from nuImages. Since the FPS of the nuImages dataset is only 2 FPS, evaluation was unable to encapsulate ten frames within a temporal window of 1 sec. For this reason, testing discarded KITTI models on the nuImages dataset. The KITTI video stream was pre-processed to evaluate nuImages-trained models on the KITTI dataset to ensure the temporal windows are compatible. nuImages-trained models require the temporal window to be 13 frames across 6 secs. However, KITTI dataset video streams are sampled at 10 Hz. The frame decimation was used to sample the video at 2 Hz and concatenate frames across 6 secs of the stream to encapsulate the 13 frames temporal window. The images were resized and were allowed the mismatch in the image dimensions between the two datasets to diversify the gap between them in the evaluation. The results for two models are reported below in Table 6.</div>
    <div id="p-0105" num="0104" class="description-paragraph">
      <tables id="TABLE-US-00006" num="00006">
        <patent-tables frame="none" colsep="0" rowsep="0">
          <table align="left" colsep="0" rowsep="0" cols="1" class="description-table" width="100%">
            <thead>
              <tr class="description-tr">
                <td namest="1" nameend="1" rowsep="1" class="description-td" colspan="1">TABLE 6</td>
              </tr>
            </thead>
            
              <tbody><tr class="description-tr">
                <td namest="1" nameend="1" align="center" rowsep="1" class="description-td" colspan="1"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation of nuImages trained models on </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">KITTI test data for (a) ViViT (b) 3D-CMA</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td">Evaluation Metric</td>
              </tr>
            
          </tbody></table>
          <table align="left" colsep="0" rowsep="0" cols="4" class="description-table" width="100%">
            <tbody><tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">Method</td>
                <td class="description-td">RMSE (KITTI)</td>
                <td class="description-td">MAE (KITTI)</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">ViViT (nuImages)</td>
                <td class="description-td">7.420</td>
                <td class="description-td">5.957</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td class="description-td">3D CMA (nuImages)</td>
                <td class="description-td">5.880</td>
                <td class="description-td">4.694</td>
              </tr>
              <tr class="description-tr">
                <td class="description-td"> </td>
                <td namest="offset" nameend="3" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr>
            
          </tbody></table>
        </patent-tables>
      </tables>
    </div>
    <div id="p-0106" num="0105" class="description-paragraph">The present disclosure includes a modified 3D-CNN architecture with masked-attention employed for ego vehicle speed estimation using single-camera video streams. 3D-CNN is effective in capturing temporal elements within an image sequence. However, it was determined that presence of background clutter and non-cohesive motion within the video stream often confused the model. To extend some control over the focus regions within the images, the 3D-CNN is modified to employ a masked-attention mechanism to steer the model to focus on relevant regions. In one embodiment, the lane segmentation mask is concatenated as an additional channel to the input images before feeding them to the 3D-CNN. The modified 3D-CNN has demonstrated better performance in several evaluations with the inclusion of the masked-attention.</div>
    <div id="p-0107" num="0106" class="description-paragraph">The performance of the modified 3D-CNN architecture was evaluated on two publicly available datasetsânulmages and KITTI. Though there are prior works utilizing the KITTI dataset for the ego vehicle speed estimation task, none clearly stated the train and test splits being used for reporting the results. In the present disclosure, the train and test splits from KITTI Road and City categories are reported.</div>
    <div id="p-0108" num="0107" class="description-paragraph">In terms of evaluation, the 3D-CMA is compared against a recent state-of-the-art transformer network for videos, ViViT. In addition, the impact of employing masked-attention to 3D-CNN is investigated and the injection of masked-attention improved the MAE and RMSE scores across all scenarios. The increase in the RMSE and MAE scores for cross-dataset evaluation is due to the domain gap between the two datasets. However, 3D-CMA continued to perform better for the cross-data set evaluation as well.</div>
    <div id="p-0109" num="0108" class="description-paragraph">Next, further details of the hardware description of an exemplary computing environment according to embodiments is described with reference to <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref>.</div>
    <div id="p-0110" num="0109" class="description-paragraph">In <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref>, a <figure-callout id="1000" label="controller" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00014.png" state="{{state}}">controller</figure-callout> <b>1000</b> is a computing device which includes a <figure-callout id="1050" label="CPU" filenames="US11861853-20240102-D00014.png" state="{{state}}">CPU</figure-callout> <b>1050</b> which can perform the processes described above.</div>
    <div id="p-0111" num="0110" class="description-paragraph">The computing device may be an AI workstation running an operating system, for example Ubuntu Linux OS, Windows, a version of Unix OS, or Mac OS. The <figure-callout id="1000" label="computer system" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00014.png" state="{{state}}">computer system</figure-callout> <b>1000</b> may include one or more central processing units (CPU) <b>1050</b> having multiple cores. The <figure-callout id="1000" label="computer system" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00014.png" state="{{state}}">computer system</figure-callout> <b>1000</b> may include a <figure-callout id="1012" label="graphics board" filenames="US11861853-20240102-D00014.png" state="{{state}}">graphics board</figure-callout> <b>1012</b> having multiple GPUs, each GPU having GPU memory. The <figure-callout id="1012" label="graphics board" filenames="US11861853-20240102-D00014.png" state="{{state}}">graphics board</figure-callout> <b>1012</b> may perform many of the mathematical operations of the disclosed machine learning methods. The <figure-callout id="1000" label="computer system" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00014.png" state="{{state}}">computer system</figure-callout> <b>1000</b> includes <figure-callout id="1002" label="main memory" filenames="US11861853-20240102-D00014.png" state="{{state}}">main memory</figure-callout> <b>1002</b>, typically random access memory RAM, which contains the software being executed by the <figure-callout id="1050" label="processing cores" filenames="US11861853-20240102-D00014.png" state="{{state}}">processing cores</figure-callout> <b>1050</b> and <figure-callout id="1012" label="GPUs" filenames="US11861853-20240102-D00014.png" state="{{state}}">GPUs</figure-callout> <b>1012</b>, as well as a <figure-callout id="1004" label="non-volatile storage device" filenames="US11861853-20240102-D00014.png" state="{{state}}">non-volatile storage device</figure-callout> <b>1004</b> for storing data and the software programs. Several interfaces for interacting with the <figure-callout id="1000" label="computer system" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00014.png" state="{{state}}">computer system</figure-callout> <b>1000</b> may be provided, including an I/O Bus Interface <b>1010</b>, Input/<figure-callout id="1018" label="Peripherals" filenames="US11861853-20240102-D00014.png" state="{{state}}">Peripherals</figure-callout> <b>1018</b> such as a keyboard, touch pad, mouse, <figure-callout id="1016" label="Display Adapter" filenames="US11861853-20240102-D00014.png" state="{{state}}">Display Adapter</figure-callout> <b>1016</b> and one or <figure-callout id="1008" label="more Displays" filenames="US11861853-20240102-D00014.png" state="{{state}}">more Displays</figure-callout> <b>1008</b>, and a <figure-callout id="1006" label="Network Controller" filenames="US11861853-20240102-D00014.png" state="{{state}}">Network Controller</figure-callout> <b>1006</b> to enable wired or wireless communication through a <figure-callout id="99" label="network" filenames="US11861853-20240102-D00014.png" state="{{state}}">network</figure-callout> 99. The interfaces, memory and processors may communicate over the <figure-callout id="1026" label="system bus" filenames="US11861853-20240102-D00014.png" state="{{state}}">system bus</figure-callout> <b>1026</b>. The <figure-callout id="1000" label="computer system" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00014.png" state="{{state}}">computer system</figure-callout> <b>1000</b> includes a power supply <b>1021</b>, which may be a redundant power supply.</div>
    <div id="p-0112" num="0111" class="description-paragraph">In some embodiments, the <figure-callout id="1000" label="computer system" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00014.png" state="{{state}}">computer system</figure-callout> <b>1000</b> may include a server CPU and a graphics card by NVIDIA, in which the GPUs have multiple CUDA cores. In some embodiments, the <figure-callout id="1000" label="computer system" filenames="US11861853-20240102-D00010.png,US11861853-20240102-D00014.png" state="{{state}}">computer system</figure-callout> <b>1000</b> may include a <figure-callout id="1012" label="machine learning engine" filenames="US11861853-20240102-D00014.png" state="{{state}}">machine learning engine</figure-callout> <b>1012</b>.</div>
    <div id="p-0113" num="0112" class="description-paragraph">The exemplary circuit elements described in the context of the present disclosure may be replaced with other elements and structured differently than the examples provided herein. Moreover, circuitry configured to perform features described herein may be implemented in multiple circuit units (e.g., chips), or the features may be combined in circuitry on a single chipset, as shown on <figref idrefs="DRAWINGS">FIG. <b>11</b> </figref>.</div>
    <div id="p-0114" num="0113" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>11</b> </figref> shows a schematic diagram of a <figure-callout id="1100" label="data processing system" filenames="US11861853-20240102-D00015.png" state="{{state}}">data processing system</figure-callout> <b>1100</b> used within the computing system, according to exemplary aspects of the present disclosure. The <figure-callout id="1100" label="data processing system" filenames="US11861853-20240102-D00015.png" state="{{state}}">data processing system</figure-callout> <b>1100</b> is an example of a computer in which code or instructions implementing the processes of the illustrative aspects of the present disclosure may be located.</div>
    <div id="p-0115" num="0114" class="description-paragraph">In <figref idrefs="DRAWINGS">FIG. <b>11</b> </figref>, data processing system <b>1180</b> employs a hub architecture including a north bridge and memory controller hub (NB/MCH) <b>1125</b> and a south bridge and input/output (I/O) controller hub (SB/ICH) <b>1120</b>. The central processing unit (CPU) <b>1130</b> is connected to NB/<figure-callout id="1125" label="MCH" filenames="US11861853-20240102-D00015.png" state="{{state}}">MCH</figure-callout> <b>1125</b>. The NB/<figure-callout id="1125" label="MCH" filenames="US11861853-20240102-D00015.png" state="{{state}}">MCH</figure-callout> <b>1125</b> also connects to the <figure-callout id="1145" label="memory" filenames="US11861853-20240102-D00015.png" state="{{state}}">memory</figure-callout> <b>1145</b> via a memory bus, and connects to the <figure-callout id="1150" label="graphics processor" filenames="US11861853-20240102-D00015.png" state="{{state}}">graphics processor</figure-callout> <b>1150</b> via an accelerated graphics port (AGP). The NB/<figure-callout id="1125" label="MCH" filenames="US11861853-20240102-D00015.png" state="{{state}}">MCH</figure-callout> <b>1125</b> also connects to the SB/<figure-callout id="1120" label="ICH" filenames="US11861853-20240102-D00015.png" state="{{state}}">ICH</figure-callout> <b>1120</b> via an internal bus (e.g., a unified media interface or a direct media interface). The <figure-callout id="1130" label="CPU Processing unit" filenames="US11861853-20240102-D00016.png" state="{{state}}">CPU Processing unit</figure-callout> <b>1130</b> may contain one or more processors and even may be implemented using one or more heterogeneous processor systems.</div>
    <div id="p-0116" num="0115" class="description-paragraph">For example, <figref idrefs="DRAWINGS">FIG. <b>12</b> </figref> shows one aspects of the present disclosure of <figure-callout id="1130" label="CPU" filenames="US11861853-20240102-D00016.png" state="{{state}}">CPU</figure-callout> <b>1130</b>. In one aspects of the present disclosure, the <figure-callout id="1238" label="instruction register" filenames="US11861853-20240102-D00016.png" state="{{state}}">instruction register</figure-callout> <b>1238</b> retrieves instructions from the <figure-callout id="1240" label="fast memory" filenames="US11861853-20240102-D00016.png" state="{{state}}">fast memory</figure-callout> <b>1240</b>. At least part of these instructions is fetched from the <figure-callout id="1238" label="instruction register" filenames="US11861853-20240102-D00016.png" state="{{state}}">instruction register</figure-callout> <b>1238</b> by the <figure-callout id="1236" label="control logic" filenames="US11861853-20240102-D00016.png" state="{{state}}">control logic</figure-callout> <b>1236</b> and interpreted according to the instruction set architecture of the <figure-callout id="1130" label="CPU" filenames="US11861853-20240102-D00016.png" state="{{state}}">CPU</figure-callout> <b>1130</b>. Part of the instructions can also be directed to the <figure-callout id="1232" label="register" filenames="US11861853-20240102-D00016.png" state="{{state}}">register</figure-callout> <b>1232</b>. In one aspects of the present disclosure the instructions are decoded according to a hardwired method, and in another aspect of the present disclosure the instructions are decoded according to a microprogram that translates instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses. After fetching and decoding the instructions, the instructions are executed using the arithmetic logic unit (ALU) <b>1234</b> that loads values from the <figure-callout id="1232" label="register" filenames="US11861853-20240102-D00016.png" state="{{state}}">register</figure-callout> <b>1232</b> and performs logical and mathematical operations on the loaded values according to the instructions. The results from these operations can be feedback into the register and/or stored in the <figure-callout id="1240" label="fast memory" filenames="US11861853-20240102-D00016.png" state="{{state}}">fast memory</figure-callout> <b>1240</b>. According to certain aspects of the present disclosures, the instruction set architecture of the <figure-callout id="1130" label="CPU" filenames="US11861853-20240102-D00016.png" state="{{state}}">CPU</figure-callout> <b>1130</b> can use a reduced instruction set architecture, a complex instruction set architecture, a vector processor architecture, a very large instruction word architecture. Furthermore, the <figure-callout id="1130" label="CPU" filenames="US11861853-20240102-D00016.png" state="{{state}}">CPU</figure-callout> <b>1130</b> can be based on the Von Neuman model or the Harvard model. The <figure-callout id="1130" label="CPU" filenames="US11861853-20240102-D00016.png" state="{{state}}">CPU</figure-callout> <b>1130</b> can be a digital signal processor, an FPGA, an ASIC, a PLA, a PLD, or a CPLD. Further, the <figure-callout id="1130" label="CPU" filenames="US11861853-20240102-D00016.png" state="{{state}}">CPU</figure-callout> <b>1130</b> can be an x86 processor by Intel or by AMD; an ARM processor, a Power architecture processor by, e.g., IBM; a SPARC architecture processor by Sun Microsystems or by Oracle; or other known CPU architecture.</div>
    <div id="p-0117" num="0116" class="description-paragraph">Referring again to <figref idrefs="DRAWINGS">FIG. <b>11</b> </figref>, the data processing system <b>1180</b> can include that the SB/<figure-callout id="1120" label="ICH" filenames="US11861853-20240102-D00015.png" state="{{state}}">ICH</figure-callout> <b>1120</b> is coupled through a system bus to an I/O Bus, a read only memory (ROM) <b>1156</b>, universal serial bus (USB) <figure-callout id="1164" label="port" filenames="US11861853-20240102-D00015.png" state="{{state}}">port</figure-callout> <b>1164</b>, a flash binary input/output system (BIOS) <b>1168</b>, and a <figure-callout id="1158" label="graphics controller" filenames="US11861853-20240102-D00015.png" state="{{state}}">graphics controller</figure-callout> <b>1158</b>. PCI/PCIe devices can also be coupled to SB/<figure-callout id="1120" label="ICH" filenames="US11861853-20240102-D00015.png" state="{{state}}">ICH</figure-callout> <b>1120</b> through a <figure-callout id="1162" label="PCI bus" filenames="US11861853-20240102-D00015.png" state="{{state}}">PCI bus</figure-callout> <b>1162</b>.</div>
    <div id="p-0118" num="0117" class="description-paragraph">The PCI devices may include, for example, Ethernet adapters, add-in cards, and PC cards for notebook computers. The <figure-callout id="1160" label="Hard disk drive" filenames="US11861853-20240102-D00015.png" state="{{state}}">Hard disk drive</figure-callout> <b>1160</b> and CD-<figure-callout id="1156" label="ROM" filenames="US11861853-20240102-D00015.png" state="{{state}}">ROM</figure-callout> <b>1156</b> can use, for example, an integrated drive electronics (IDE) or serial advanced technology attachment (SATA) interface. In one aspects of the present disclosure the I/O bus can include a super I/O (SIO) device.</div>
    <div id="p-0119" num="0118" class="description-paragraph">Further, the hard disk drive (HDD) <b>1160</b> and <figure-callout id="1166" label="optical drive" filenames="US11861853-20240102-D00015.png" state="{{state}}">optical drive</figure-callout> <b>1166</b> can also be coupled to the SB/<figure-callout id="1120" label="ICH" filenames="US11861853-20240102-D00015.png" state="{{state}}">ICH</figure-callout> <b>1120</b> through a system bus. In one aspects of the present disclosure, a <figure-callout id="1170" label="keyboard" filenames="US11861853-20240102-D00015.png" state="{{state}}">keyboard</figure-callout> <b>1170</b>, a <figure-callout id="1172" label="mouse" filenames="US11861853-20240102-D00015.png" state="{{state}}">mouse</figure-callout> <b>1172</b>, a <figure-callout id="1178" label="parallel port" filenames="US11861853-20240102-D00015.png" state="{{state}}">parallel port</figure-callout> <b>1178</b>, and a <figure-callout id="1176" label="serial port" filenames="US11861853-20240102-D00015.png" state="{{state}}">serial port</figure-callout> <b>1176</b> can be connected to the system bus through the I/O bus. Other peripherals and devices that can be connected to the SB/<figure-callout id="1120" label="ICH" filenames="US11861853-20240102-D00015.png" state="{{state}}">ICH</figure-callout> <b>1120</b> using a mass storage controller such as SATA or PATA, an Ethernet port, an ISA bus, an LPC bridge, SMBus, a DMA controller, and an Audio Codec.</div>
    <div id="p-0120" num="0119" class="description-paragraph">Moreover, the present disclosure is not limited to the specific circuit elements described herein, nor is the present disclosure limited to the specific sizing and classification of these elements. For example, the skilled artisan will appreciate that the circuitry described herein may be adapted based on changes on battery sizing and chemistry, or based on the requirements of the intended back-up load to be powered.</div>
    <div id="p-0121" num="0120" class="description-paragraph">The functions and features described herein may also be executed by various distributed components of a system. For example, one or more processors may execute these system functions, wherein the processors are distributed across multiple components communicating in a network. The distributed components may include one or more client and server machines, which may share processing, as shown by <figref idrefs="DRAWINGS">FIG. <b>13</b> </figref>, in addition to various human interface and communication devices (e.g., display monitors, smart phones, tablets, personal digital assistants (PDAs)). More specifically, <figref idrefs="DRAWINGS">FIG. <b>13</b> </figref> illustrates client devices including smart phone <b>1311</b>, <figure-callout id="1312" label="tablet" filenames="US11861853-20240102-D00017.png" state="{{state}}">tablet</figure-callout> <b>1312</b>, <figure-callout id="1314" label="mobile device terminal" filenames="US11861853-20240102-D00017.png" state="{{state}}">mobile device terminal</figure-callout> <b>1314</b> and fixed <figure-callout id="1316" label="terminals" filenames="US11861853-20240102-D00017.png" state="{{state}}">terminals</figure-callout> <b>1316</b>. These client devices may be commutatively coupled with a <figure-callout id="1320" label="mobile network service" filenames="US11861853-20240102-D00017.png" state="{{state}}">mobile network service</figure-callout> <b>1320</b> via base station <b>1356</b>, <figure-callout id="1354" label="access point" filenames="US11861853-20240102-D00017.png" state="{{state}}">access point</figure-callout> <b>1354</b>, <figure-callout id="1352" label="satellite" filenames="US11861853-20240102-D00017.png" state="{{state}}">satellite</figure-callout> <b>1352</b> or via an internet connection. <figure-callout id="1320" label="Mobile network service" filenames="US11861853-20240102-D00017.png" state="{{state}}">Mobile network service</figure-callout> <b>1320</b> may comprise <figure-callout id="1322" label="central processors" filenames="US11861853-20240102-D00017.png" state="{{state}}">central processors</figure-callout> <b>1322</b>, <figure-callout id="1324" label="server" filenames="US11861853-20240102-D00017.png" state="{{state}}">server</figure-callout> <b>1324</b> and <figure-callout id="1326" label="database" filenames="US11861853-20240102-D00017.png" state="{{state}}">database</figure-callout> <b>1326</b>. <figure-callout id="1316" label="Fixed terminals" filenames="US11861853-20240102-D00017.png" state="{{state}}">Fixed terminals</figure-callout> <b>1316</b> and <figure-callout id="1320" label="mobile network service" filenames="US11861853-20240102-D00017.png" state="{{state}}">mobile network service</figure-callout> <b>1320</b> may be commutatively coupled via an internet connection to functions in <figure-callout id="1330" label="cloud" filenames="US11861853-20240102-D00017.png" state="{{state}}">cloud</figure-callout> <b>1330</b> that may comprise security gateway <b>1332</b>, <figure-callout id="1334" label="data center" filenames="US11861853-20240102-D00017.png" state="{{state}}">data center</figure-callout> <b>1334</b>, <figure-callout id="1336" label="cloud controller" filenames="US11861853-20240102-D00017.png" state="{{state}}">cloud controller</figure-callout> <b>1336</b>, <figure-callout id="1338" label="data storage" filenames="US11861853-20240102-D00017.png" state="{{state}}">data storage</figure-callout> <b>1338</b> and <figure-callout id="1340" label="provisioning tool" filenames="US11861853-20240102-D00017.png" state="{{state}}">provisioning tool</figure-callout> <b>1340</b>. The network may be a private network, such as a LAN or WAN, or may be a public network, such as the Internet. Input to the system may be received via direct user input and received remotely either in real-time or as a batch process. Additionally, some aspects of the present disclosure may be performed on modules or hardware not identical to those described.</div>
    <div id="p-0122" num="0121" class="description-paragraph">Numerous modifications and variations of the present invention are possible in light of the above teachings. It is therefore to be understood that within the scope of the appended claims, the invention may be practiced otherwise than as specifically described herein. As an example, the invention may be practiced to utilize the speed of ego vehicle to estimate the speeds and locations of environment vehicles for in-vehicle motion and path planning.</div>
    
  </div>
  </div>
  </section>

  <section itemprop="claims" itemscope>
    <h2>Claims (<span itemprop="count">20</span>)</h2>
    
    <div itemprop="content" html><div mxw-id="PCLM442940040" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>The invention claimed is:</claim-statement>
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A system for ego vehicle speed estimation, comprising
<div class="claim-text">a car-mounted monocular camera for capturing a sequence of video frames of an outdoor scene from a moving car, where the outdoor scene includes a road, as a camera channel;</div>
<div class="claim-text">processing circuitry configured with
<div class="claim-text">a single-shot network, and</div>
<div class="claim-text">a neural network time series model,</div>
<div class="claim-text">wherein the single-shot network segments features of the road in the video frame sequence and generates a masked-attention map for the segmented road features;</div>
<div class="claim-text">a concatenation operation that concatenates the masked-attention map as an additional channel to the camera channel to generate a masked-attention input;</div>
<div class="claim-text">wherein the neural network time series model receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of the segmented road features in the video sequences; and</div>
</div>
<div class="claim-text">output circuitry to output a signal indicating the estimated speed.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing circuitry is configured with a grayscale conversion circuit to convert RGB of the video frame sequence to a grayscale video frame sequence.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the single-shot network is a lane line segmentation network to segment lane line segments as the road features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the single-shot network includes a Spatial Pyramid Pooling component and a Feature Pyramid Network component to obtain features of the video frame sequence containing multiple scales and multiple semantic level information.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the single-shot network contains a shared encoder and three separate decoders that accomplish specific tasks of object detection, drivable area segmentation and lane line segmentation.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing circuitry halts processing of the neural network time series model while the segmented features do not include predetermined road features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the neural network time series model is a 3D convolutional neural network (3D-CNN), and
<div class="claim-text">wherein the 3D-CNN receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of the segmented road features in the video sequences.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing circuitry intermittently performs processing using the neural network time series model.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing circuitry continuously monitors vehicle speed while the ego vehicle is in an operating state and periodically estimates speed of the ego vehicle using the neural network time series model.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. An embedded ego vehicle speed estimation apparatus, comprising:
<div class="claim-text">processing circuitry configured with</div>
<div class="claim-text">a single-shot network and a neural network time series model,
<div class="claim-text">wherein the single-shot network segments features in a video frame sequence of a road and generates a masked-attention map for the segmented road features;</div>
<div class="claim-text">a concatenation operation that concatenates the masked-attention map as an additional channel to a camera channel to generate a masked-attention input;</div>
<div class="claim-text">wherein the neural network time series model receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of lane line segments in the video sequences; and</div>
</div>
<div class="claim-text">output circuitry to output a signal indicating the estimated speed.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processing circuitry is configured with a grayscale conversion circuit to convert RGB of the video frame sequence to a grayscale video frame sequence.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the single-shot network is a lane line segmentation network to segment the lane line segments as the road features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the single-shot network includes a Spatial Pyramid Pooling component and a Feature Pyramid Network component to obtain features of the video frame sequence containing multiple scales and multiple semantic level information.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the single-shot network contains a shared encoder and three separate decoders that accomplish specific tasks of object detection, drivable area segmentation and lane line segmentation.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processing circuitry halts processing of the neural network time series model while the segmented features do not include predetermined road features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text">16. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the neural ne work time series model is a 3D convolutional neural network (3D-CNN), and
<div class="claim-text">wherein the 3D-CNN receives the masked-attention input and generates an estimated speed of the ego vehicle based on displacement of the segmented road features in the video sequences.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text">17. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processing circuitry intermittently performs processing using the neural network time series model.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text">18. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processing circuitry, continuously monitors vehicle speed while the ego vehicle is in an operating state and periodically estimates speed of the ego vehicle using the neural network time series model.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text">19. A non-transitory computer readable storage medium storing computer instructions, which when executed by processing circuitry, perform a method of ego vehicle speed estimation comprising:
<div class="claim-text">segmenting, by a single-shot network, features in a video frame sequence of a road and generates a masked-attention map for the segmented road features;</div>
<div class="claim-text">concatenating, by a concatenation operation, the masked-attention map as an additional channel to a camera channel to generate a masked-attention input;</div>
<div class="claim-text">receiving, by a neural network time series model, the masked-attention input and generating an estimated speed of the ego vehicle based on displacement of lane line segments in the video sequences; and</div>
<div class="claim-text">outputting a signal indicating the estimated speed.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text">20. The storage medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising, segmenting, by the single-shot network, the lane line segments as the road features.</div>
    </div>
  </div> </div>
  </div>
  </section>

  <section itemprop="application" itemscope>

    <section itemprop="metadata" itemscope>
      <span itemprop="applicationNumber">US18/328,441</span>
      <span itemprop="priorityDate">2022-11-17</span>
      <span itemprop="filingDate">2023-06-02</span>
      <span itemprop="title">System and method of vehicle speed estimation using moving camera and time series neural network 
       </span>
      <span itemprop="ifiStatus">Active</span>
      
      <a href="/patent/US11861853B1/en">
        <span itemprop="representativePublication">US11861853B1</span>
        (<span itemprop="primaryLanguage">en</span>)
      </a>
    </section>

    <h2>Priority Applications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="priorityApps" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US18/328,441</span>
            
            <a href="/patent/US11861853B1/en">
              <span itemprop="representativePublication">US11861853B1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2022-11-17</td>
          <td itemprop="filingDate">2023-06-02</td>
          <td itemprop="title">System and method of vehicle speed estimation using moving camera and time series neural network 
       </td>
        </tr>
      </tbody>
    </table>

    <h2>Applications Claiming Priority (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US202263426211P</span>
            
          </td>
          <td itemprop="priorityDate">2022-11-17</td>
          <td itemprop="filingDate">2022-11-17</td>
          <td itemprop="title"></td>
        </tr>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US18/328,441</span>
            <a href="/patent/US11861853B1/en">
              <span itemprop="representativePublication">US11861853B1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2022-11-17</td>
          <td itemprop="filingDate">2023-06-02</td>
          <td itemprop="title">System and method of vehicle speed estimation using moving camera and time series neural network 
       </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Publications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Publication Number</th>
          <th>Publication Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="pubs" itemscope repeat>
          <td>
            <span itemprop="publicationNumber">US11861853B1</span>
            
            <span itemprop="thisPatent">true</span>
            <a href="/patent/US11861853B1/en">
              US11861853B1
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2024-01-02</td>
        </tr>
      </tbody>
    </table>

  </section>

  <section itemprop="family" itemscope>
    <h1>Family</h1>
    <h2>ID=89434503</h2>

    <h2>Family Applications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Title</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="applications" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US18/328,441</span>
            <span itemprop="ifiStatus">Active</span>
            
            <a href="/patent/US11861853B1/en">
              <span itemprop="representativePublication">US11861853B1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2022-11-17</td>
          <td itemprop="filingDate">2023-06-02</td>
          <td itemprop="title">System and method of vehicle speed estimation using moving camera and time series neural network 
       </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Country Status (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Country</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">US</span>
            (<span itemprop="num">1</span>)
            <meta itemprop="thisCountry" content="true">
          </td>
          <td>
            <a href="/patent/US11861853B1/en">
              <span itemprop="representativePublication">US11861853B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Citations (6)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20180024562A1/en">
              <span itemprop="publicationNumber">US20180024562A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2016-07-21</td>
          <td itemprop="publicationDate">2018-01-25</td>
          <td>
            <span itemprop="assigneeOriginal">Mobileye Vision Technologies Ltd.</span>
          </td>
          <td itemprop="title">Localizing vehicle navigation using lane measurements 
     </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20190244366A1/en">
              <span itemprop="publicationNumber">US20190244366A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2017-09-07</td>
          <td itemprop="publicationDate">2019-08-08</td>
          <td>
            <span itemprop="assigneeOriginal">Comcast Cable Communications, Llc</span>
          </td>
          <td itemprop="title">Relevant Motion Detection in Video 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20200324794A1/en">
              <span itemprop="publicationNumber">US20200324794A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-06-25</td>
          <td itemprop="publicationDate">2020-10-15</td>
          <td>
            <span itemprop="assigneeOriginal">Intel Corporation</span>
          </td>
          <td itemprop="title">Technology to apply driving norms for automated vehicle behavior prediction 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20210278852A1/en">
              <span itemprop="publicationNumber">US20210278852A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2020-03-05</td>
          <td itemprop="publicationDate">2021-09-09</td>
          <td>
            <span itemprop="assigneeOriginal">Uatc, Llc</span>
          </td>
          <td itemprop="title">Systems and Methods for Using Attention Masks to Improve Motion Planning 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20220292291A1/en">
              <span itemprop="publicationNumber">US20220292291A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2021-03-10</td>
          <td itemprop="publicationDate">2022-09-15</td>
          <td>
            <span itemprop="assigneeOriginal">Western Digital Technologies, Inc.</span>
          </td>
          <td itemprop="title">Attention Masks in Neural Network Video Processing 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11608083B2/en">
              <span itemprop="publicationNumber">US11608083B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-09-18</td>
          <td itemprop="publicationDate">2023-03-21</td>
          <td>
            <span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span>
          </td>
          <td itemprop="title">System and method for providing cooperation-aware lane change control in dense traffic 
       </td>
        </tr>
      </tbody>
    </table>

    

    <ul>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2023</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2023-06-02</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US18/328,441</span>
            <a href="/patent/US11861853B1/en"><span itemprop="documentId">patent/US11861853B1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            <span itemprop="thisApp" content="true" bool></span>
          </li>
        </ul>
      </li>
    </ul>

    </section>

  <section>
    <h2>Patent Citations (6)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20180024562A1/en">
              <span itemprop="publicationNumber">US20180024562A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2016-07-21</td>
          <td itemprop="publicationDate">2018-01-25</td>
          <td><span itemprop="assigneeOriginal">Mobileye Vision Technologies Ltd.</span></td>
          <td itemprop="title">Localizing vehicle navigation using lane measurements 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20190244366A1/en">
              <span itemprop="publicationNumber">US20190244366A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2017-09-07</td>
          <td itemprop="publicationDate">2019-08-08</td>
          <td><span itemprop="assigneeOriginal">Comcast Cable Communications, Llc</span></td>
          <td itemprop="title">Relevant Motion Detection in Video 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US11608083B2/en">
              <span itemprop="publicationNumber">US11608083B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-09-18</td>
          <td itemprop="publicationDate">2023-03-21</td>
          <td><span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span></td>
          <td itemprop="title">System and method for providing cooperation-aware lane change control in dense traffic 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20210278852A1/en">
              <span itemprop="publicationNumber">US20210278852A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2020-03-05</td>
          <td itemprop="publicationDate">2021-09-09</td>
          <td><span itemprop="assigneeOriginal">Uatc, Llc</span></td>
          <td itemprop="title">Systems and Methods for Using Attention Masks to Improve Motion Planning 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20200324794A1/en">
              <span itemprop="publicationNumber">US20200324794A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-06-25</td>
          <td itemprop="publicationDate">2020-10-15</td>
          <td><span itemprop="assigneeOriginal">Intel Corporation</span></td>
          <td itemprop="title">Technology to apply driving norms for automated vehicle behavior prediction 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20220292291A1/en">
              <span itemprop="publicationNumber">US20220292291A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2021-03-10</td>
          <td itemprop="publicationDate">2022-09-15</td>
          <td><span itemprop="assigneeOriginal">Western Digital Technologies, Inc.</span></td>
          <td itemprop="title">Attention Masks in Neural Network Video Processing 
       </td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Non-Patent Citations (8)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Athul M. Mathew, et al., "<a href='http://scholar.google.com/scholar?q="Ego+Vehicle+Speed+Estimation+Using+3D+Convolution+With+Masked+Attention"'>Ego Vehicle Speed Estimation Using 3D Convolution With Masked Attention</a>", Computer Science &gt; Computer Vision and Pattern Recognition (cs.CV), arXiv:2212.05432v1 [cs.CV] Dec. 11, 2022, Dec. 13, 2022, pp. 1-13.</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Bandari, Hitesh Linganna, and Binoy B. Nair. "<a href='http://scholar.google.com/scholar?q="An+End+to+End+Learning+based+Ego+Vehicle+Speed+Estimation+System."'>An End to End Learning based Ego Vehicle Speed Estimation System.</a>" 2021 IEEE International Power and Renewable Energy Conference (IPRECON). IEEE, 2021.</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Du Tran, et al., "<a href='http://scholar.google.com/scholar?q="Learning+Spatiotemporal+Features+with+3D+Convolutional+Networks"'>Learning Spatiotemporal Features with 3D Convolutional Networks</a>", IEEE International Conference On Computer Vision, Dec. 7-13, 2015, pp. 4489-4497.</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Hayakawa, Jun, and Behzad Dariush. "<a href='http://scholar.google.com/scholar?q="Ego-motion+and+surrounding+vehicle+state+estimation+using+a+monocular+camera."'>Ego-motion and surrounding vehicle state estimation using a monocular camera.</a>" 2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2019.</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Wu, Chao-Yuan, et al. "<a href='http://scholar.google.com/scholar?q="Long-term+feature+banks+for+detailed+video+understanding."'>Long-term feature banks for detailed video understanding.</a>" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Yeon, Kyuhwan, et al. "<a href='http://scholar.google.com/scholar?q="Ego-vehicle+speed+prediction+using+a+long+short-term+memory+based+recurrent+neural+network."'>Ego-vehicle speed prediction using a long short-term memory based recurrent neural network.</a>" International Journal of Automotive Technology 20 (2019): 713-722.</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Yi Zhou, et al., "<a href='http://scholar.google.com/scholar?q="Towards+Deep+Radar+Perception+for+Autonomous+Driving%3A+Datasets%2C+Methods%2C+and+Challenges"'>Towards Deep Radar Perception for Autonomous Driving: Datasets, Methods, and Challenges</a>", SENSORS, vol. 22, Issue 11, May 31, 2022, pp. 1-45.</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Zhao, Baigan, et al. "<a href='http://scholar.google.com/scholar?q="Ego-motion+estimation+using+recurrent+convolutional+neural+networks+through+optical+flow+learning."'>Ego-motion estimation using recurrent convolutional neural networks through optical flow learning.</a>" Electronics 10.3 (2021): 222.</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
      </tbody>
    </table>
  </section>

  

  

  <section>
    <h2>Similar Documents</h2>
    <table>
      <thead>
        <tr>
          <th>Publication</th>
          <th>Publication Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11604944B2/en">
                <span itemprop="publicationNumber">US11604944B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-03-14">2023-03-14</time>
            
            
          </td>
          <td itemprop="title">Regression-based line detection for autonomous driving machines 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11461583B2/en">
                <span itemprop="publicationNumber">US11461583B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-10-04">2022-10-04</time>
            
            
          </td>
          <td itemprop="title">Binary feature compression for autonomous devices 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11449713B2/en">
                <span itemprop="publicationNumber">US11449713B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-09-20">2022-09-20</time>
            
            
          </td>
          <td itemprop="title">Attention based feature compression and localization for autonomous devices 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10997433B2/en">
                <span itemprop="publicationNumber">US10997433B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-05-04">2021-05-04</time>
            
            
          </td>
          <td itemprop="title">Real-time detection of lanes and boundaries by autonomous vehicles 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11651215B2/en">
                <span itemprop="publicationNumber">US11651215B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-05-16">2023-05-16</time>
            
            
          </td>
          <td itemprop="title">Landmark detection using curve fitting for autonomous driving applications 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20180365888A1/en">
                <span itemprop="publicationNumber">US20180365888A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2018-12-20">2018-12-20</time>
            
            
          </td>
          <td itemprop="title">System and method for digital environment reconstruction 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="10451346495945694712">
              <a href="/scholar/10451346495945694712"><span itemprop="scholarAuthors">Kortli et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2022">2022</time>
            
          </td>
          <td itemprop="title">Deep embedded hybrid CNNâLSTM network for lane detection on NVIDIA Jetson Xavier NX</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN112417953B/en">
                <span itemprop="publicationNumber">CN112417953B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-07-19">2022-07-19</time>
            
            
          </td>
          <td itemprop="title">Road condition detection and map data updating method, device, system and equipment 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/JP7420734B2/en">
                <span itemprop="publicationNumber">JP7420734B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2024-01-23">2024-01-23</time>
            
            
          </td>
          <td itemprop="title">
  Data distribution systems, sensor devices and servers
 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/KR20210043516A/en">
                <span itemprop="publicationNumber">KR20210043516A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-04-21">2021-04-21</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for training trajectory planning model, electronic device, storage medium and program 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="9761625544204002741">
              <a href="/scholar/9761625544204002741"><span itemprop="scholarAuthors">Mahaur et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2022">2022</time>
            
          </td>
          <td itemprop="title">Road object detection: a comparative study of deep learning-based algorithms</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="2126709851141768732">
              <a href="/scholar/2126709851141768732"><span itemprop="scholarAuthors">Zhuang et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2021">2021</time>
            
          </td>
          <td itemprop="title">Illumination and temperature-aware multispectral networks for edge-computing-enabled pedestrian detection</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="15756884995224668088">
              <a href="/scholar/15756884995224668088"><span itemprop="scholarAuthors">Parmar et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2019">2019</time>
            
          </td>
          <td itemprop="title">Deeprange: deepâlearningâbased object detection and ranging in autonomous driving</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="15309614132868682116">
              <a href="/scholar/15309614132868682116"><span itemprop="scholarAuthors">Yusuf et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2020">2020</time>
            
          </td>
          <td itemprop="title">A robust method for lane detection under adverse weather and illumination conditions using convolutional neural network</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20240029296A1/en">
                <span itemprop="publicationNumber">US20240029296A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2024-01-25">2024-01-25</time>
            
            
          </td>
          <td itemprop="title">Orientation Determination for Mobile Computing Devices 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11861853B1/en">
                <span itemprop="publicationNumber">US11861853B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2024-01-02">2024-01-02</time>
            
            
          </td>
          <td itemprop="title">System and method of vehicle speed estimation using moving camera and time series neural network 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="15506723622694258973">
              <a href="/scholar/15506723622694258973"><span itemprop="scholarAuthors">Qu et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2023">2023</time>
            
          </td>
          <td itemprop="title">Improved YOLOv5-based for small traffic sign detection under complex weather</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="9723584247485102846">
              <a href="/scholar/9723584247485102846"><span itemprop="scholarAuthors">Kumar et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2021">2021</time>
            
          </td>
          <td itemprop="title">Lane Detection for Autonomous Vehicle in Hazy Environment with Optimized Deep Learning Techniques</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="9874244114719100649">
              <a href="/scholar/9874244114719100649"><span itemprop="scholarAuthors">Asif et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2022">2022</time>
            
          </td>
          <td itemprop="title">Performance Evaluation of Deep Learning Algorithm Using High-End Media Processing Board in Real-Time Environment</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="4243448230928068099">
              <a href="/scholar/4243448230928068099"><span itemprop="scholarAuthors">Sharma et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2021">2021</time>
            
          </td>
          <td itemprop="title">Effective utilization of a low-cost solution for remote sensing of vehicles and pedestrians</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11915499B1/en">
                <span itemprop="publicationNumber">US11915499B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2024-02-27">2024-02-27</time>
            
            
          </td>
          <td itemprop="title">Systems and methods for automated license plate recognition 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="1380357436660519481">
              <a href="/scholar/1380357436660519481"><span itemprop="scholarAuthors">Xia et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2022">2022</time>
            
          </td>
          <td itemprop="title">Toward Sustainable Transportation: Robust Lane-Change Monitoring With a Single Back View Cabin Camera</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230177839A1/en">
                <span itemprop="publicationNumber">US20230177839A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-06-08">2023-06-08</time>
            
            
          </td>
          <td itemprop="title">Deep learning based operational domain verification using camera-based inputs for autonomous systems and applications 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="17245717748292326973">
              <a href="/scholar/17245717748292326973"><span itemprop="scholarAuthors">Tumas</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2021">2021</time>
            
          </td>
          <td itemprop="title">Improvement of intelligent methods for pedestrian detection in far-infrared radiation images</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="10120660659574502584">
              <a href="/scholar/10120660659574502584"><span itemprop="scholarAuthors">Tai et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2023">2023</time>
            
          </td>
          <td itemprop="title">Vehicles of Everything combined Yolov7 with Traffic enforcement camera on the roadside system</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Legal Events</h2>
    <table>
      <thead>
        <tr>
          <th>Date</th>
          <th>Code</th>
          <th>Title</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-06-02">2023-06-02</time></td>
          <td itemprop="code">FEPP</td>
          <td itemprop="title">Fee payment procedure</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-12-13">2023-12-13</time></td>
          <td itemprop="code">STCF</td>
          <td itemprop="title">Information on status: patent grant</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">PATENTED CASE</span>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </section>

</article>

    </search-app>
    <script type="text/javascript" src="//www.gstatic.com/feedback/api.js"></script>
    <script type="text/javascript" src="//www.gstatic.com/feedback/js/help/prod/service/lazy.min.js"></script>
    <script type="text/javascript">
      if (window.help && window.help.service) {
        helpApi = window.help.service.Lazy.create(0, {apiKey: 'AIzaSyDTEI_0tLX4varJ7bwK8aT-eOI5qr3BmyI', locale: 'en-US'});
        window.requestedSurveys = new Set();
        window.requestSurvey = function(triggerId) {
          if (window.requestedSurveys.has(triggerId)) {
            return;
          }
          window.requestedSurveys.add(triggerId);
          helpApi.requestSurvey({
            triggerId: triggerId,
            enableTestingMode: false,
            callback: (requestSurveyCallbackParam) => {
              if (!requestSurveyCallbackParam.surveyData) {
                return;
              }
              helpApi.presentSurvey({
                productData: {
                  productVersion: window.version,
                  customData: {
                    "experiments": "72459301,72474719",
                  },
                },
                surveyData: requestSurveyCallbackParam.surveyData,
                colorScheme: 1,
                customZIndex: 10000,
              });
            }
          });
        };

        window.requestSurvey('YXTwAsvoW0kedxbuTdH0RArc9VhT');
      }
    </script>
    <script src="/sw/null_loader.js"></script>
  </body>
</html>
