<!DOCTYPE html>
<html lang="en">
  <head>
    <title>US11544507B2 - Method and apparatus to train image recognition model, and image recognition method and apparatus 
        - Google Patents</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <meta name="referrer" content="origin-when-crossorigin">
    <link rel="canonical" href="https://patents.google.com/patent/US11544507B2/en">
    <meta name="description" content="
     An apparatus and method to train an image recognition model to accurately estimate a location of a reference point for each class of landmark is disclosed. The apparatus and method use the image recognition model, which is trained based on calculating a class loss and a class-dependent localization loss from training data based on an image recognition model and training the image recognition model using a total loss comprising the class loss and the localization loss. 
   
   ">
    <meta name="DC.type" content="patent">
    <meta name="DC.title" content="Method and apparatus to train image recognition model, and image recognition method and apparatus 
       ">
    <meta name="DC.date" content="2019-03-28" scheme="dateSubmitted">
    <meta name="DC.description" content="
     An apparatus and method to train an image recognition model to accurately estimate a location of a reference point for each class of landmark is disclosed. The apparatus and method use the image recognition model, which is trained based on calculating a class loss and a class-dependent localization loss from training data based on an image recognition model and training the image recognition model using a total loss comprising the class loss and the localization loss. 
   
   ">
    <meta name="citation_patent_application_number" content="US:16/367,358">
    <meta name="citation_pdf_url" content="https://patentimages.storage.googleapis.com/dd/69/11/8a7ed0e7ec28c9/US11544507.pdf">
    <meta name="citation_patent_number" content="US:11544507">
    <meta name="DC.date" content="2023-01-03" scheme="issue">
    <meta name="DC.contributor" content="Wonhee Lee" scheme="inventor">
    <meta name="DC.contributor" content="Minjung SON" scheme="inventor">
    <meta name="DC.contributor" content="Kyungboo JUNG" scheme="inventor">
    <meta name="DC.contributor" content="Hyun Sung Chang" scheme="inventor">
    <meta name="DC.contributor" content="Samsung Electronics Co Ltd" scheme="assignee">
    <meta name="DC.relation" content="US:20090080780:A1" scheme="references">
    <meta name="DC.relation" content="US:20100232727:A1" scheme="references">
    <meta name="DC.relation" content="US:20140343842:A1" scheme="references">
    <meta name="DC.relation" content="KR:20150103979:A" scheme="references">
    <meta name="DC.relation" content="US:9863775" scheme="references">
    <meta name="DC.relation" content="US:20150371397:A1" scheme="references">
    <meta name="DC.relation" content="US:20160283864:A1" scheme="references">
    <meta name="DC.relation" content="US:20180190046:A1" scheme="references">
    <meta name="DC.relation" content="US:20170147905:A1" scheme="references">
    <meta name="DC.relation" content="KR:20180069501:A" scheme="references">
    <meta name="DC.relation" content="US:20190377949:A1" scheme="references">
    <meta name="DC.relation" content="US:20200117991:A1" scheme="references">
    <meta name="citation_reference" content="Cao, Yuhang, et al. âPrime sample attention in object detection.â arXiv preprint arXiv:1904.04821, 2019 (pp. 1-10)." scheme="references">
    <meta name="citation_reference" content="Extended European Search Report dated Dec. 6, 2019 in counterpart European Application No. 19178016.2 ( 8 pages in English)." scheme="references">
    <meta name="citation_reference" content="Girshick, Ross, âFast R-CNNâ, 2015 IEEE International Conference on Computer Vision (ICCV), 2015 (pp. 1440-1448)." scheme="references">
    <meta name="citation_reference" content="He, Wenhao et al., âDeep Direct Regression for Multi-oriented Scene Text Detectionâ, 2017 IEEE International Conference on Computer Vision (ICCV), 2017 (pp. 1-9)." scheme="references">
    <meta name="citation_reference" content="Hwang, Sangheum et al., âSelf-Transfer Learning for Fully Weakly Supervised Object Localizationâ, arXiv:1602.01625, Feb. 4, 2016 (pp. 1-9)." scheme="references">
    <meta name="citation_reference" content="Jung, Heechul, et al. âResNet-based vehicle classification and localization in traffic surveillance systems.â Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017. (Year: 2017)." scheme="references">
    <meta name="citation_reference" content="Lin, Di et al., âDeep LAC: Deep Localization, Alignment and Classification for Fine-grained Recognitionâ, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015 (pp. 1666-1674)." scheme="references">
    <meta name="citation_reference" content="Lin, Tsung-Yi et al., âFocal Loss for Dense Object Detectionâ, 2017 IEEE International Conference on Computer Vision (ICCV), Feb. 7, 2018 (pp. 1-10)." scheme="references">
    <meta name="citation_reference" content="Liu, Wei, et al., âSSD: Single Shot MultiBox Detectorâ, Proceedings of the European Conference on Computer Vision, Dec. 29, 2016 (17 pages in English)." scheme="references">
    <meta name="citation_reference" content="Ma, Jianqi, et al. âArbitrary-Oriented Scene Text Detection via Rotation Proposals.â (Mar. 2018). (Year: 2018)." scheme="references">
    <meta name="citation_reference" content="Redmon, Joseph, et al. âYou only look once: Unified, real-time object detection.â Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. (Year: 2016)." scheme="references">
    <meta name="citation_reference" content="Ren, Shaoqing, et al. âFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networksâ, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, Issue 6, 2017 (pp. 1-14)." scheme="references">
    <meta name="citation_reference" content="Shou, Zheng et al., âTemporal Action Localization in Untrimmed Videos via Multi-stage CNNsâ, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016 (pp. 1049-1058)." scheme="references">
    <meta name="citation_reference" content="Thi, Tuan Hue, et al., âStructured learning of local features for human action classification and localizationâ, Image and Vision Computing, vol. 30, Issue 1, Jan. 2012 (pp. 1-14)." scheme="references">
    <meta name="citation_reference" content="Xu, Hongyu, et al. âDeep regionlets for object detection.â Proceedings of the European Conference on Computer Vision (ECCV). 2018. (Year: 2018)." scheme="references">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Product+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700">

    <style>
      body { transition: none; }
    </style>

    <script>
      window.version = 'patent-search.search_20240108_RC01';

      function sendFeedback() {
        userfeedback.api.startFeedback({
          'productId': '713680',
          'bucket': 'patent-search-web',
          'productVersion': window.version,
        });
      }

      window.experiments = {};
      window.experiments.patentCountries = "ae,ag,al,am,ao,ap,ar,at,au,aw,az,ba,bb,bd,be,bf,bg,bh,bj,bn,bo,br,bw,bx,by,bz,ca,cf,cg,ch,ci,cl,cm,cn,co,cr,cs,cu,cy,cz,dd,de,dj,dk,dm,do,dz,ea,ec,ee,eg,em,ep,es,fi,fr,ga,gb,gc,gd,ge,gh,gm,gn,gq,gr,gt,gw,hk,hn,hr,hu,ib,id,ie,il,in,ir,is,it,jo,jp,ke,kg,kh,km,kn,kp,kr,kw,kz,la,lc,li,lk,lr,ls,lt,lu,lv,ly,ma,mc,md,me,mg,mk,ml,mn,mo,mr,mt,mw,mx,my,mz,na,ne,ng,ni,nl,no,nz,oa,om,pa,pe,pg,ph,pl,pt,py,qa,ro,rs,ru,rw,sa,sc,sd,se,sg,si,sk,sl,sm,sn,st,su,sv,sy,sz,td,tg,th,tj,tm,tn,tr,tt,tw,tz,ua,ug,us,uy,uz,vc,ve,vn,wo,yu,za,zm,zw";
      
      
      window.experiments.keywordWizard = true;
      
      
      
      window.experiments.definitions = true;

      window.Polymer = {
        dom: 'shady',
        lazyRegister: true,
      };
    </script>

    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/webcomponentsjs/webcomponents-lite.min.js"></script>
    <link rel="import" href="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.html">
  </head>
  <body unresolved>
    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.js"></script>
    <search-app>
      
      

      <article class="result" itemscope itemtype="http://schema.org/ScholarlyArticle">
  <h1 itemprop="pageTitle">US11544507B2 - Method and apparatus to train image recognition model, and image recognition method and apparatus 
        - Google Patents</h1>
  <span itemprop="title">Method and apparatus to train image recognition model, and image recognition method and apparatus 
       </span>

  <meta itemprop="type" content="patent">
  <a href="https://patentimages.storage.googleapis.com/dd/69/11/8a7ed0e7ec28c9/US11544507.pdf" itemprop="pdfLink">Download PDF</a>
  <h2>Info</h2>

  <dl>
    <dt>Publication number</dt>
    <dd itemprop="publicationNumber">US11544507B2</dd>
    <meta itemprop="numberWithoutCodes" content="11544507">
    <meta itemprop="kindCode" content="B2">
    <meta itemprop="publicationDescription" content="Patent ( having previously published pre-grant publication)">
    <span>US11544507B2</span>
    <span>US16/367,358</span>
    <span>US201916367358A</span>
    <span>US11544507B2</span>
    <span>US 11544507 B2</span>
    <span>US11544507 B2</span>
    <span>US 11544507B2</span>
    <span>  </span>
    <span> </span>
    <span> </span>
    <span>US 201916367358 A</span>
    <span>US201916367358 A</span>
    <span>US 201916367358A</span>
    <span>US 11544507 B2</span>
    <span>US11544507 B2</span>
    <span>US 11544507B2</span>

    <dt>Authority</dt>
    <dd itemprop="countryCode">US</dd>
    <dd itemprop="countryName">United States</dd>

    <dt>Prior art keywords</dt>
    <dd itemprop="priorArtKeywords" repeat>class</dd>
    <dd itemprop="priorArtKeywords" repeat>loss</dd>
    <dd itemprop="priorArtKeywords" repeat>subregions</dd>
    <dd itemprop="priorArtKeywords" repeat>image</dd>
    <dd itemprop="priorArtKeywords" repeat>reference point</dd>

    <dt>Prior art date</dt>
    <dd><time itemprop="priorArtDate" datetime="2018-10-17">2018-10-17</time></dd>

    <dt>Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)</dt>
    <dd itemprop="legalStatusIfi" itemscope>
      <span itemprop="status">Active</span>, expires <time itemprop="expiration" datetime="2040-01-28">2040-01-28</time>
    </dd>
  </dl>

  <dt>Application number</dt>
  <dd itemprop="applicationNumber">US16/367,358</dd>

  

  <dt>Other versions</dt>
  <dd itemprop="directAssociations" itemscope repeat>
    <a href="/patent/US20200125899A1/en">
      <span itemprop="publicationNumber">US20200125899A1</span>
      (<span itemprop="primaryLanguage">en</span>
    </a>
  </dd>

  <dt>Inventor</dt>
  <dd itemprop="inventor" repeat>Wonhee Lee</dd>
  <dd itemprop="inventor" repeat>Minjung SON</dd>
  <dd itemprop="inventor" repeat>Kyungboo JUNG</dd>
  <dd itemprop="inventor" repeat>Hyun Sung Chang</dd>

  <dt>Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)</dt>
  <dd itemprop="assigneeCurrent" repeat>
    Samsung Electronics Co Ltd
  </dd>

  <dt>Original Assignee</dt>
  <dd itemprop="assigneeOriginal" repeat>Samsung Electronics Co Ltd</dd>

  <dt>Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)</dt>
  <dd><time itemprop="priorityDate" datetime="2018-10-17">2018-10-17</time></dd>

  <dt>Filing date</dt>
  <dd><time itemprop="filingDate" datetime="2019-03-28">2019-03-28</time></dd>

  <dt>Publication date</dt>
  <dd><time itemprop="publicationDate" datetime="2023-01-03">2023-01-03</time></dd>

  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2019-03-28">2019-03-28</time>
    <span itemprop="title">Application filed by Samsung Electronics Co Ltd</span>
    <span itemprop="type">filed</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    <span itemprop="assigneeSearch">Samsung Electronics Co Ltd</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2019-03-28">2019-03-28</time>
    <span itemprop="title">Assigned to SAMSUNG ELECTRONCIS CO., LTD.</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">SAMSUNG ELECTRONCIS CO., LTD.</span>
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    <span itemprop="description" repeat>Assignors: CHANG, HYUN SUNG, JUNG, KYUNGBOO, LEE, WONHEE, SON, MINJUNG</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2020-04-23">2020-04-23</time>
    <span itemprop="title">Publication of US20200125899A1</span>
    <span itemprop="type">publication</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US20200125899A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-01-03">2023-01-03</time>
    <span itemprop="title">Application granted</span>
    <span itemprop="type">granted</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-01-03">2023-01-03</time>
    <span itemprop="title">Publication of US11544507B2</span>
    <span itemprop="type">publication</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US11544507B2/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date">Status</time>
    <span itemprop="title">Active</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    <span itemprop="current" content="true" bool>Current</span>
    
    
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2040-01-28">2040-01-28</time>
    <span itemprop="title">Adjusted expiration</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
  </dd>

  <h2>Links</h2>
  <ul>
    
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoLink">
          <a href="https://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&p=1&u=/netahtml/PTO/srchnum.html&r=1&f=G&l=50&d=PALL&s1=11544507.PN." itemprop="url" target="_blank"><span itemprop="text">USPTO</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoPatentCenterLink">
          <a href="https://patentcenter.uspto.gov/applications/16367358" itemprop="url" target="_blank"><span itemprop="text">USPTO PatentCenter</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoAssignmentLink">
          <a href="https://assignment.uspto.gov/patent/index.html#/patent/search/resultFilter?searchInput=11544507" itemprop="url" target="_blank"><span itemprop="text">USPTO Assignment</span></a>
        </li>

    <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="espacenetLink">
        <a href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=11544507B2&amp;KC=B2&amp;FT=D" itemprop="url" target="_blank"><span itemprop="text">Espacenet</span></a>
      </li>
      

    

    
      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="globalDossierLink">
        <a href="https://globaldossier.uspto.gov/#/result/patent/US/11544507/1" itemprop="url" target="_blank"><span itemprop="text">Global Dossier</span></a>
      </li>
      

      

      

      

      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="stackexchangeLink">
        <a href="https://patents.stackexchange.com/questions/tagged/US11544507" itemprop="url"><span itemprop="text">Discuss</span></a>
      </li>
      
  </ul>

  

  <section>
    <h2>Images</h2>
    <ul>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/e1/a7/64/5434785efff67c/US11544507-20230103-D00000.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/49/5f/7c/f5c7539bc5e0d0/US11544507-20230103-D00000.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="601">
            <meta itemprop="label" content="training input">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="737">
              <meta itemprop="top" content="9">
              <meta itemprop="right" content="816">
              <meta itemprop="bottom" content="47">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="610">
            <meta itemprop="label" content="image recognition model">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="794">
              <meta itemprop="top" content="271">
              <meta itemprop="right" content="881">
              <meta itemprop="bottom" content="311">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="611">
            <meta itemprop="label" content="DNN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="733">
              <meta itemprop="top" content="515">
              <meta itemprop="right" content="813">
              <meta itemprop="bottom" content="554">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="671">
            <meta itemprop="label" content="class loss L">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="327">
              <meta itemprop="top" content="1269">
              <meta itemprop="right" content="404">
              <meta itemprop="bottom" content="1306">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="672">
            <meta itemprop="label" content="localization loss L">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="835">
              <meta itemprop="top" content="1104">
              <meta itemprop="right" content="918">
              <meta itemprop="bottom" content="1144">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="679">
            <meta itemprop="label" content="total loss L">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="588">
              <meta itemprop="top" content="1876">
              <meta itemprop="right" content="674">
              <meta itemprop="bottom" content="1913">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="681">
            <meta itemprop="label" content="class information DNN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="384">
              <meta itemprop="top" content="824">
              <meta itemprop="right" content="462">
              <meta itemprop="bottom" content="864">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="682">
            <meta itemprop="label" content="point information DNN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="890">
              <meta itemprop="top" content="825">
              <meta itemprop="right" content="974">
              <meta itemprop="bottom" content="864">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="690">
            <meta itemprop="label" content="training data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1424">
              <meta itemprop="top" content="333">
              <meta itemprop="right" content="1507">
              <meta itemprop="bottom" content="371">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/ef/ed/fb/8f393f82556726/US11544507-20230103-D00001.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/2e/9e/43/a1be2f9effced0/US11544507-20230103-D00001.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="100">
            <meta itemprop="label" content="neural network">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="808">
              <meta itemprop="top" content="295">
              <meta itemprop="right" content="886">
              <meta itemprop="bottom" content="336">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="110">
            <meta itemprop="label" content="input layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="53">
              <meta itemprop="top" content="752">
              <meta itemprop="right" content="130">
              <meta itemprop="bottom" content="789">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="120">
            <meta itemprop="label" content="layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="808">
              <meta itemprop="top" content="452">
              <meta itemprop="right" content="890">
              <meta itemprop="bottom" content="489">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="130">
            <meta itemprop="label" content="output layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1563">
              <meta itemprop="top" content="650">
              <meta itemprop="right" content="1645">
              <meta itemprop="bottom" content="691">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/4a/cb/57/880b6c93cd3b79/US11544507-20230103-D00002.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/a5/d5/dd/ddcbd50d479160/US11544507-20230103-D00002.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="201">
            <meta itemprop="label" content="input image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="289">
              <meta itemprop="top" content="652">
              <meta itemprop="right" content="367">
              <meta itemprop="bottom" content="690">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="209">
            <meta itemprop="label" content="output data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1437">
              <meta itemprop="top" content="495">
              <meta itemprop="right" content="1522">
              <meta itemprop="bottom" content="535">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="210">
            <meta itemprop="label" content="image recognition model">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="863">
              <meta itemprop="top" content="630">
              <meta itemprop="right" content="949">
              <meta itemprop="bottom" content="667">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/5c/41/24/c0bd5e42eb6906/US11544507-20230103-D00003.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/51/aa/d9/6f5819dc1a9806/US11544507-20230103-D00003.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="301">
            <meta itemprop="label" content="input image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1559">
              <meta itemprop="top" content="429">
              <meta itemprop="right" content="1641">
              <meta itemprop="bottom" content="468">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="381">
            <meta itemprop="label" content="object">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1115">
              <meta itemprop="top" content="1687">
              <meta itemprop="right" content="1194">
              <meta itemprop="bottom" content="1725">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="382">
            <meta itemprop="label" content="object">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1596">
              <meta itemprop="top" content="1687">
              <meta itemprop="right" content="1679">
              <meta itemprop="bottom" content="1727">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="390">
            <meta itemprop="label" content="landmark">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="945">
              <meta itemprop="top" content="460">
              <meta itemprop="right" content="1029">
              <meta itemprop="bottom" content="499">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/10/bb/74/faa6d9e009264c/US11544507-20230103-D00004.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/1f/61/59/c3aff90ca3dc3e/US11544507-20230103-D00004.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="411">
            <meta itemprop="label" content="warning sign">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="129">
              <meta itemprop="top" content="489">
              <meta itemprop="right" content="208">
              <meta itemprop="bottom" content="525">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="412">
            <meta itemprop="label" content="sign">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="19">
              <meta itemprop="top" content="1680">
              <meta itemprop="right" content="102">
              <meta itemprop="bottom" content="1718">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="421">
            <meta itemprop="label" content="reference point">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="354">
              <meta itemprop="top" content="844">
              <meta itemprop="right" content="433">
              <meta itemprop="bottom" content="883">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="422">
            <meta itemprop="label" content="reference point">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="345">
              <meta itemprop="top" content="2225">
              <meta itemprop="right" content="429">
              <meta itemprop="bottom" content="2263">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/8b/3f/14/92f3fbfaa4a044/US11544507-20230103-D00005.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/ba/12/78/f297ae2cc8f2af/US11544507-20230103-D00005.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="413">
            <meta itemprop="label" content="sign">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="131">
              <meta itemprop="top" content="371">
              <meta itemprop="right" content="215">
              <meta itemprop="bottom" content="410">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="414">
            <meta itemprop="label" content="auxiliary sign">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="18">
              <meta itemprop="top" content="1820">
              <meta itemprop="right" content="102">
              <meta itemprop="bottom" content="1857">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="423">
            <meta itemprop="label" content="reference point">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="851">
              <meta itemprop="top" content="408">
              <meta itemprop="right" content="932">
              <meta itemprop="bottom" content="445">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="424">
            <meta itemprop="label" content="reference point">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="939">
              <meta itemprop="top" content="1775">
              <meta itemprop="right" content="1024">
              <meta itemprop="bottom" content="1812">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/a6/79/1a/c691ba50d225b2/US11544507-20230103-D00006.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/36/b4/fd/09e91345fd53af/US11544507-20230103-D00006.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="415">
            <meta itemprop="label" content="signal">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="19">
              <meta itemprop="top" content="484">
              <meta itemprop="right" content="102">
              <meta itemprop="bottom" content="520">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="425">
            <meta itemprop="label" content="reference point">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="442">
              <meta itemprop="top" content="312">
              <meta itemprop="right" content="523">
              <meta itemprop="bottom" content="350">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="426">
            <meta itemprop="label" content="reference point">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="895">
              <meta itemprop="top" content="2054">
              <meta itemprop="right" content="979">
              <meta itemprop="bottom" content="2092">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="7">
            <meta itemprop="label" content="Equations">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="359">
              <meta itemprop="top" content="1585">
              <meta itemprop="right" content="895">
              <meta itemprop="bottom" content="2142">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/57/e0/b8/ae7933a0029717/US11544507-20230103-D00007.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/22/56/09/3fb67f0d534c1e/US11544507-20230103-D00007.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="501">
            <meta itemprop="label" content="training image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="617">
              <meta itemprop="top" content="2109">
              <meta itemprop="right" content="657">
              <meta itemprop="bottom" content="2189">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="507">
            <meta itemprop="label" content="total loss">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1077">
              <meta itemprop="top" content="33">
              <meta itemprop="right" content="1116">
              <meta itemprop="bottom" content="116">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="508">
            <meta itemprop="label" content="GT data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="463">
              <meta itemprop="top" content="218">
              <meta itemprop="right" content="499">
              <meta itemprop="bottom" content="299">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="509">
            <meta itemprop="label" content="temporary output">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="461">
              <meta itemprop="top" content="956">
              <meta itemprop="right" content="499">
              <meta itemprop="bottom" content="1038">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="510">
            <meta itemprop="label" content="image recognition model">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="596">
              <meta itemprop="top" content="1531">
              <meta itemprop="right" content="633">
              <meta itemprop="bottom" content="1613">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/76/f9/d7/1ec6eecb4c12ce/US11544507-20230103-D00008.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/03/fa/08/abd8b1fb81ec01/US11544507-20230103-D00008.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="601">
            <meta itemprop="label" content="training input">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="747">
              <meta itemprop="top" content="147">
              <meta itemprop="right" content="827">
              <meta itemprop="bottom" content="189">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="610">
            <meta itemprop="label" content="image recognition model">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="807">
              <meta itemprop="top" content="413">
              <meta itemprop="right" content="891">
              <meta itemprop="bottom" content="449">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="611">
            <meta itemprop="label" content="DNN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="745">
              <meta itemprop="top" content="656">
              <meta itemprop="right" content="826">
              <meta itemprop="bottom" content="695">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="671">
            <meta itemprop="label" content="class loss L">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="339">
              <meta itemprop="top" content="1409">
              <meta itemprop="right" content="419">
              <meta itemprop="bottom" content="1449">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="672">
            <meta itemprop="label" content="localization loss L">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="847">
              <meta itemprop="top" content="1246">
              <meta itemprop="right" content="928">
              <meta itemprop="bottom" content="1284">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="679">
            <meta itemprop="label" content="total loss L">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="601">
              <meta itemprop="top" content="2017">
              <meta itemprop="right" content="684">
              <meta itemprop="bottom" content="2057">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="681">
            <meta itemprop="label" content="class information DNN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="395">
              <meta itemprop="top" content="965">
              <meta itemprop="right" content="473">
              <meta itemprop="bottom" content="1003">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="682">
            <meta itemprop="label" content="point information DNN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="903">
              <meta itemprop="top" content="965">
              <meta itemprop="right" content="984">
              <meta itemprop="bottom" content="1002">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="690">
            <meta itemprop="label" content="training data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1437">
              <meta itemprop="top" content="473">
              <meta itemprop="right" content="1520">
              <meta itemprop="bottom" content="513">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/40/f1/c9/306449c8234411/US11544507-20230103-D00009.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/8e/f3/f0/e0886047dc770e/US11544507-20230103-D00009.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="701">
            <meta itemprop="label" content="training image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="124">
              <meta itemprop="top" content="1188">
              <meta itemprop="right" content="164">
              <meta itemprop="bottom" content="1266">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="710">
            <meta itemprop="label" content="image recognition model">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="803">
              <meta itemprop="top" content="1185">
              <meta itemprop="right" content="840">
              <meta itemprop="bottom" content="1265">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="770">
            <meta itemprop="label" content="total loss">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1796">
              <meta itemprop="top" content="659">
              <meta itemprop="right" content="1833">
              <meta itemprop="bottom" content="742">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="780">
            <meta itemprop="label" content="temporary output">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1058">
              <meta itemprop="top" content="2072">
              <meta itemprop="right" content="1097">
              <meta itemprop="bottom" content="2155">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="781">
            <meta itemprop="label" content="GT landmark portion">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1184">
              <meta itemprop="top" content="2026">
              <meta itemprop="right" content="1223">
              <meta itemprop="bottom" content="2107">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="782">
            <meta itemprop="label" content="GT background portion">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1447">
              <meta itemprop="top" content="2025">
              <meta itemprop="right" content="1487">
              <meta itemprop="bottom" content="2107">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="791">
            <meta itemprop="label" content="map data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="814">
              <meta itemprop="top" content="83">
              <meta itemprop="right" content="853">
              <meta itemprop="bottom" content="160">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="792">
            <meta itemprop="label" content="GT output">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1034">
              <meta itemprop="top" content="901">
              <meta itemprop="right" content="1071">
              <meta itemprop="bottom" content="981">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/93/4e/68/3376833b6c6bf8/US11544507-20230103-D00010.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/d8/9f/c1/7e131fa636e0c2/US11544507-20230103-D00010.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="1">
            <meta itemprop="label" content="Equation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1360">
              <meta itemprop="top" content="1179">
              <meta itemprop="right" content="1369">
              <meta itemprop="bottom" content="1194">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="861">
            <meta itemprop="label" content="class information Ä">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1206">
              <meta itemprop="top" content="1325">
              <meta itemprop="right" content="1241">
              <meta itemprop="bottom" content="1404">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="862">
            <meta itemprop="label" content="class information c">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1201">
              <meta itemprop="top" content="749">
              <meta itemprop="right" content="1243">
              <meta itemprop="bottom" content="834">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="869">
            <meta itemprop="label" content="partial class loss">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1534">
              <meta itemprop="top" content="654">
              <meta itemprop="right" content="1572">
              <meta itemprop="bottom" content="739">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="879">
            <meta itemprop="label" content="partial localization loss">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1007">
              <meta itemprop="top" content="655">
              <meta itemprop="right" content="1047">
              <meta itemprop="bottom" content="740">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="880">
            <meta itemprop="label" content="subregion">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="457">
              <meta itemprop="top" content="2063">
              <meta itemprop="right" content="497">
              <meta itemprop="bottom" content="2148">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="881">
            <meta itemprop="label" content="anchor nodes">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="333">
              <meta itemprop="top" content="1803">
              <meta itemprop="right" content="369">
              <meta itemprop="bottom" content="1882">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="890">
            <meta itemprop="label" content="subregion">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="389">
              <meta itemprop="top" content="19">
              <meta itemprop="right" content="429">
              <meta itemprop="bottom" content="102">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="891">
            <meta itemprop="label" content="GT reference point">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="576">
              <meta itemprop="top" content="51">
              <meta itemprop="right" content="615">
              <meta itemprop="bottom" content="129">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/12/cf/bf/11ad0499bf52d3/US11544507-20230103-D00011.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/e0/36/b5/0af2659e9e9a95/US11544507-20230103-D00011.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="900">
            <meta itemprop="label" content="training apparatus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="688">
              <meta itemprop="top" content="412">
              <meta itemprop="right" content="770">
              <meta itemprop="bottom" content="450">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="910">
            <meta itemprop="label" content="processor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="747">
              <meta itemprop="top" content="519">
              <meta itemprop="right" content="830">
              <meta itemprop="bottom" content="555">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="920">
            <meta itemprop="label" content="memory">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="746">
              <meta itemprop="top" content="809">
              <meta itemprop="right" content="830">
              <meta itemprop="bottom" content="845">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="921">
            <meta itemprop="label" content="image recognition model">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="688">
              <meta itemprop="top" content="1305">
              <meta itemprop="right" content="766">
              <meta itemprop="bottom" content="1341">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="991">
            <meta itemprop="label" content="training image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="409">
              <meta itemprop="top" content="1705">
              <meta itemprop="right" content="486">
              <meta itemprop="bottom" content="1741">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="11">
            <meta itemprop="id" content="992">
            <meta itemprop="label" content="map data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="967">
              <meta itemprop="top" content="1704">
              <meta itemprop="right" content="1048">
              <meta itemprop="bottom" content="1738">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/c5/9c/7e/24ebad0bccd07a/US11544507-20230103-D00012.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/0e/25/ad/eb3b0943468148/US11544507-20230103-D00012.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="12">
            <meta itemprop="id" content="1010">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1376">
              <meta itemprop="top" content="575">
              <meta itemprop="right" content="1481">
              <meta itemprop="bottom" content="611">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="12">
            <meta itemprop="id" content="1020">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1375">
              <meta itemprop="top" content="864">
              <meta itemprop="right" content="1482">
              <meta itemprop="bottom" content="903">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/06/72/d2/6ccab0953d8027/US11544507-20230103-D00013.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/74/3a/e2/ffccb2ef0d8342/US11544507-20230103-D00013.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/02/c3/be/8f0dcba11f080a/US11544507-20230103-D00014.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/a9/70/40/b1daa5b52ffb8c/US11544507-20230103-D00014.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/24/0b/f0/6e81e5392959de/US11544507-20230103-D00015.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/3d/ff/d9/8613d24da4c79b/US11544507-20230103-D00015.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/27/25/1e/30a8b18f654f68/US11544507-20230103-D00016.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/e0/71/32/76acb87ea1c579/US11544507-20230103-D00016.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1200">
            <meta itemprop="label" content="image recognition apparatus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="781">
              <meta itemprop="top" content="398">
              <meta itemprop="right" content="890">
              <meta itemprop="bottom" content="435">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1205">
            <meta itemprop="label" content="communication bus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="913">
              <meta itemprop="top" content="624">
              <meta itemprop="right" content="1020">
              <meta itemprop="bottom" content="664">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1205">
            <meta itemprop="label" content="communication bus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="929">
              <meta itemprop="top" content="1819">
              <meta itemprop="right" content="1028">
              <meta itemprop="bottom" content="1853">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1210">
            <meta itemprop="label" content="image acquirer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1539">
              <meta itemprop="top" content="789">
              <meta itemprop="right" content="1647">
              <meta itemprop="bottom" content="827">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1220">
            <meta itemprop="label" content="processor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="18">
              <meta itemprop="top" content="1003">
              <meta itemprop="right" content="128">
              <meta itemprop="bottom" content="1042">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1230">
            <meta itemprop="label" content="memory">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1538">
              <meta itemprop="top" content="1213">
              <meta itemprop="right" content="1648">
              <meta itemprop="bottom" content="1250">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1240">
            <meta itemprop="label" content="display">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="17">
              <meta itemprop="top" content="1424">
              <meta itemprop="right" content="125">
              <meta itemprop="bottom" content="1464">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="1250">
            <meta itemprop="label" content="communication interface">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1517">
              <meta itemprop="top" content="1636">
              <meta itemprop="right" content="1647">
              <meta itemprop="bottom" content="1672">
            </span>
          </li>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>Classifications</h2>
    <ul>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F</span>&mdash;<span itemprop="Description">ELECTRIC DIGITAL DATA PROCESSING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/00</span>&mdash;<span itemprop="Description">Pattern recognition</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/20</span>&mdash;<span itemprop="Description">Analysing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/21</span>&mdash;<span itemprop="Description">Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/214</span>&mdash;<span itemprop="Description">Generating training patterns; Bootstrap methods, e.g. bagging or boosting</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06K9/6262</span>&mdash;<span itemprop="Description"></span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/70</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/73</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras using feature-based methods</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/75</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras using feature-based methods involving models</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/70</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding using pattern recognition or machine learning</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/82</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F</span>&mdash;<span itemprop="Description">ELECTRIC DIGITAL DATA PROCESSING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/00</span>&mdash;<span itemprop="Description">Pattern recognition</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/20</span>&mdash;<span itemprop="Description">Analysing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/21</span>&mdash;<span itemprop="Description">Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/217</span>&mdash;<span itemprop="Description">Validation; Performance evaluation; Active pattern learning techniques</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F</span>&mdash;<span itemprop="Description">ELECTRIC DIGITAL DATA PROCESSING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/00</span>&mdash;<span itemprop="Description">Pattern recognition</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/20</span>&mdash;<span itemprop="Description">Analysing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/22</span>&mdash;<span itemprop="Description">Matching criteria, e.g. proximity measures</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F</span>&mdash;<span itemprop="Description">ELECTRIC DIGITAL DATA PROCESSING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/00</span>&mdash;<span itemprop="Description">Pattern recognition</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/20</span>&mdash;<span itemprop="Description">Analysing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/24</span>&mdash;<span itemprop="Description">Classification techniques</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F18/241</span>&mdash;<span itemprop="Description">Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06K9/6215</span>&mdash;<span itemprop="Description"></span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/10</span>&mdash;<span itemprop="Description">Segmentation; Edge detection</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/11</span>&mdash;<span itemprop="Description">Region-based segmentation</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/10</span>&mdash;<span itemprop="Description">Segmentation; Edge detection</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/143</span>&mdash;<span itemprop="Description">Segmentation; Edge detection involving probabilistic approaches, e.g. Markov random field [MRF] modelling</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/00</span>&mdash;<span itemprop="Description">Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/10</span>&mdash;<span itemprop="Description">Character recognition</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/19</span>&mdash;<span itemprop="Description">Recognition using electronic means</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/191</span>&mdash;<span itemprop="Description">Design or setup of recognition systems or techniques; Extraction of features in feature space; Clustering techniques; Blind source separation</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/19173</span>&mdash;<span itemprop="Description">Classification techniques</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/00</span>&mdash;<span itemprop="Description">Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/10</span>&mdash;<span itemprop="Description">Character recognition</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/19</span>&mdash;<span itemprop="Description">Recognition using electronic means</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/192</span>&mdash;<span itemprop="Description">Recognition using electronic means using simultaneous comparisons or correlations of the image signals with a plurality of references</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V30/194</span>&mdash;<span itemprop="Description">References adjustable by an adaptive method, e.g. learning</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20</span>&mdash;<span itemprop="Description">Special algorithmic details</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20081</span>&mdash;<span itemprop="Description">Training; Learning</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
    </ul>
  </section>

  

  

  <section>
    <h2>Definitions</h2>
    <ul>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the following description</span>
        <span itemprop="definition">relates to training an image recognition model.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Automation of image recognition</span>
        <span itemprop="definition">has been implemented through a processor implemented neural network model, as a specialized computational architecture, which after substantial training may provide computationally intuitive mappings between input patterns and output patterns.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the trained capability of generating such mappings</span>
        <span itemprop="definition">may be referred to as a learning capability of the neural network.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">such specially trained neural network</span>
        <span itemprop="definition">may thereby have a generalization capability of generating a relatively accurate output with respect to an input pattern that the neural network may not have been trained to recognize.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">operations or applications</span>
        <span itemprop="definition">are performed through specialized computation architecture, and in different automated manners than they would have been performed in non-computer implemented or non-automated approaches, they also invite problems or drawbacks that only occur because of the automated and specialized computational architecture on which they are implement.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a method of training an image recognition model</span>
        <span itemprop="definition">including calculating a class loss and a class-dependent localization loss from training data based on an image recognition model, and training the image recognition model using a total loss including the class loss and the localization loss.</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the class loss and the class-dependent localization loss</span>
        <span itemprop="definition">may include calculating temporary class information and temporary reference point information from an input training image based on the image recognition model, calculating the class loss based on the temporary class information and ground truth class information, and calculating the localization loss based on the temporary reference point information and ground truth reference point information.</span>
        <meta itemprop="num_attr" content="0006">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the temporary class information and the temporary reference point information</span>
        <span itemprop="definition">may include calculating temporary class information and temporary reference point information for each of subregions of the input training image.</span>
        <meta itemprop="num_attr" content="0007">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the class loss</span>
        <span itemprop="definition">may include calculating a partial class loss between the ground truth class information and the temporary class information calculated for the each of the subregions of the input training image, and determining a sum of partial class losses calculated for the each of the subregions of the input training image to be the class loss.</span>
        <meta itemprop="num_attr" content="0008">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the class loss</span>
        <span itemprop="definition">may include selecting subregions corresponding to a ground truth landmark portion from among the subregions of the input training image, calculating a partial class loss between the ground truth class information and temporary class information calculated for each of the selected subregions, and determining a sum of partial class losses calculated for the selected subregions to be the class loss.</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the selecting of the subregions</span>
        <span itemprop="definition">may include further selecting a subregion corresponding a ground truth background portion from among the subregions of the input training image.</span>
        <meta itemprop="num_attr" content="0010">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the localization loss</span>
        <span itemprop="definition">may include calculating, for each of the subregions of the input training image, a partial localization loss between the ground truth reference point information and temporary reference point information calculated for the each of the subregions of the input training image, and determining a sum of partial localization losses calculated for the each of the subregions to be the localization loss.</span>
        <meta itemprop="num_attr" content="0011">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the localization loss</span>
        <span itemprop="definition">may include selecting subregions corresponding to a ground truth landmark portion from among the subregions of the input training image, calculating a partial localization loss between the ground truth reference point information and temporary reference point information of each of the selected subregions, and determining a sum of partial localization losses calculated for the selected subregions to be the localization loss.</span>
        <meta itemprop="num_attr" content="0012">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the partial localization loss</span>
        <span itemprop="definition">may include excluding a subregion with a ground truth background portion from the selected subregions.</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the temporary class information and the temporary reference point information for the each of the subregions of the input training image</span>
        <span itemprop="definition">may include calculating temporary class information and temporary reference point information for each of anchor nodes set for the each of the subregions.</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the temporary class information and the temporary reference point information for the each of the anchor nodes</span>
        <span itemprop="definition">may include calculating temporary class information and temporary reference point information for an anchor node having a highest confidence level from among confidence levels calculated for each of the anchor nodes.</span>
        <meta itemprop="num_attr" content="0015">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the temporary class information and the temporary reference point information for each of the anchor nodes</span>
        <span itemprop="definition">may include excluding an anchor node having a confidence level less than a threshold from among confidence levels calculated for each of the anchor nodes.</span>
        <meta itemprop="num_attr" content="0016">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the calculating of the class loss and the class-dependent localization loss</span>
        <span itemprop="definition">may include calculating a class-based weight based on temporary class information, and determining the class-dependent localization loss based on the class-based weight, temporary reference point information, and ground truth reference point information.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining of the class-dependent localization loss</span>
        <span itemprop="definition">may include determining the class-dependent localization loss by applying the class-based weight to a difference between the temporary reference point information and the ground truth reference point information.</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training</span>
        <span itemprop="definition">may include updating a parameter of the image recognition model to minimize the total loss.</span>
        <meta itemprop="num_attr" content="0019">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the updating of the parameter</span>
        <span itemprop="definition">may include repeating the updating of the parameter of the image recognition model to converge the total loss.</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the updating of the parameter</span>
        <span itemprop="definition">may include updating the parameter such that the class loss is minimized before the localization loss is minimized.</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a training apparatus</span>
        <span itemprop="definition">including a memory configured to store an image recognition model, and a processor configured to calculate a class loss and a class-dependent localization loss from training data based on the image recognition model, and train the image recognition model using a total loss including the class loss and the localization loss.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an image recognition method</span>
        <span itemprop="definition">including obtaining an input image, and estimating, from the input image, a class of a landmark in the input image and a reference point of the landmark, based on an image recognition model.</span>
        <meta itemprop="num_attr" content="0023">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">is a diagram illustrating an example of an image recognition model.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">is a diagram illustrating an example of recognizing an input image based on an image recognition model.</span>
        <meta itemprop="num_attr" content="0026">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3</span>
        <span itemprop="definition">is a diagram illustrating examples of objects in an image.</span>
        <meta itemprop="num_attr" content="0027">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 4 A through 4 F</span>
        <span itemprop="definition">are diagrams illustrating examples of various landmarks.</span>
        <meta itemprop="num_attr" content="0028">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a diagram illustrating an example of training an image recognition model.</span>
        <meta itemprop="num_attr" content="0029">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">is a diagram illustrating an example of training an image recognition model.</span>
        <meta itemprop="num_attr" content="0030">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">is a diagram illustrating an example of calculating a loss for each subregion during training.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 8</span>
        <span itemprop="definition">is a diagram illustrating an example of calculating a loss for each anchor node in each subregion during training.</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 9</span>
        <span itemprop="definition">is a diagram illustrating an example of a training apparatus.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 10</span>
        <span itemprop="definition">is a diagram illustrating an example of a training method.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 11 A through 11 C</span>
        <span itemprop="definition">are diagrams illustrating examples of reducing a loss through training for each loss function.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 12</span>
        <span itemprop="definition">is a diagram illustrating an example of an image recognition apparatus.</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">first,â âsecond,â and âthirdâ</span>
        <span itemprop="definition">may be used herein to describe various members, components, regions, layers, or sections, these members, components, regions, layers, or sections are not to be limited by these terms. Rather, these terms are only used to distinguish one member, component, region, layer, or section from another member, component, region, layer, or section. Thus, a first member, component, region, layer, or section referred to in examples described herein may also be referred to as a second member, component, region, layer, or section without departing from the teachings of the examples.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">is a diagram illustrating an example of an image recognition model.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an image recognition model</span>
        <span itemprop="definition">may be of a machine learning architecture trained to output a result of recognizing an input image.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition model</span>
        <span itemprop="definition">may be embodied by a neural network 100 , as illustrated in FIG. 1 .</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">examples of the image recognition model</span>
        <span itemprop="definition">are not limited to the illustrated example.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition model</span>
        <span itemprop="definition">may be trained by an apparatus for training the image recognition model, hereinafter simply referred to as a training apparatus.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may train the image recognition model with a location and a type, or a class as described herein, of an object, such as, a sign in an image that is output from a camera installed in a vehicle.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An image recognition apparatus</span>
        <span itemprop="definition">may recognize the input image based on the trained image recognition model.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition apparatus</span>
        <span itemprop="definition">may identify an object in the input image based on the trained image recognition model.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus and the image recognition apparatus</span>
        <span itemprop="definition">may be integrated together to be embodied as an integral apparatus.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">will be described as an example of the image recognition model with reference to FIG. 1 .</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">may be configured as a single network and may also be configured as a recurrent network.</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">may be a deep neural network (DNN).</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the DNN</span>
        <span itemprop="definition">may include a fully-connected network (FCN), a deep convolutional network (DCN), a recurrent neural network (RNN), a long-short term memory (LSTM) network, and a grated recurrent units (GRUs).</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FCN</span>
        <span itemprop="definition">fully-connected network</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DCN</span>
        <span itemprop="definition">deep convolutional network</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">RNN</span>
        <span itemprop="definition">recurrent neural network</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">LSTM</span>
        <span itemprop="definition">long-short term memory</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GRUs</span>
        <span itemprop="definition">grated recurrent units</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">may be implemented as an architecture having a plurality of layers including an input image, feature maps, and an output.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a convolution operation between the input image, and a filter referred to as a kernel</span>
        <span itemprop="definition">is performed, and as a result of the convolution operation, the feature maps are output.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the feature maps that are output</span>
        <span itemprop="definition">are input feature maps, and a convolution operation between the output feature maps and the kernel is performed again, and as a result, new feature maps are output. Based on such repeatedly performed convolution operations, results of recognition of characteristics of the input image via the neural network may be output.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">may map input data and output data that have a nonlinear relationship based on deep learning to perform tasks such as, for example, object classification, object recognition, audio or speech recognition, and image recognition.</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the deep learning</span>
        <span itemprop="definition">may be a type of machine learning that is applied to perform image recognition or speech recognition from a big dataset.</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the deep learning</span>
        <span itemprop="definition">may be performed in supervised and/or unsupervised manners, which may be applied to perform the mapping of input data and output data.</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network</span>
        <span itemprop="definition">may include an input source sentence (e.g., voice entry) instead of an input image.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a convolution operation</span>
        <span itemprop="definition">is performed on the input source sentence with a kernel, and as a result, the feature maps are output.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the convolution operation</span>
        <span itemprop="definition">is performed again on the output feature maps as input feature maps, with a kernel, and new feature maps are output.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a recognition result with respect to features of the input source sentence</span>
        <span itemprop="definition">may be finally output through the neural network.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">includes an input layer 110 , a hidden layer 120 , and an output layer 130 .</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each of the input layer 110 , the hidden layer 120 , and the output layer 130</span>
        <span itemprop="definition">may include a plurality of artificial nodes.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hidden layer 120</span>
        <span itemprop="definition">is illustrated as including three layers in FIG. 1 for convenience of description, the hidden layer 120 may include other numbers of layers.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">is illustrated as including a separate input layer, for example, the input layer 110 , to receive input data, the input data may be directly input to the hidden layer 120 .</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the artificial nodes described above</span>
        <span itemprop="definition">will be simply referred to as nodes, and nodes of layers of the neural network 100 excluding ones of the output layer 130 may be connected to nodes of a next layer through links to transmit an output signal.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the number of the links</span>
        <span itemprop="definition">may correspond to the number of the nodes included in the next layer.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">may include additional layers, such as, for example, a sub-sampling layer, a pooling layer, and a fully connected layer.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An output of an activation function associated with weighted inputs of nodes included in a previous layer</span>
        <span itemprop="definition">may be input to each of the nodes included in the hidden layer 120 .</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the weighted inputs</span>
        <span itemprop="definition">may be obtained by multiplying an input of the nodes included in the previous layer by a connection weight.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the connection weight</span>
        <span itemprop="definition">may also be referred to as a parameter of the neural network 100 .</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the activation function</span>
        <span itemprop="definition">may include a sigmoid function, a hyperbolic tangent (tan h) function, and a rectified linear unit (ReLU), and nonlinearity may be formed in the neural network 100 by the activation function.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the weighted inputs of the nodes included in the previous layer</span>
        <span itemprop="definition">may be input to each of the nodes included in the output layer 130 .</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">may calculate a function value based on the number of classes to be classified and recognized in the output layer 130 through the hidden layer 120 , and classify and recognize the input data as a class having a greatest function value.</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">may classify or recognize the input data, the classification and recognition by the neural network 100 will be described simply as recognition for convenience of description. Thus, the following description of the recognition may also be applied to the classification unless otherwise defined.</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">may have a capacity such that the neural network 100 implements a function.</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 100</span>
        <span itemprop="definition">learns a sufficiently large amount of training data through training, the neural network 100 may obtain an optimal performance of recognition.</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">is a diagram illustrating an example of recognizing an input image based on an image recognition model.</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an image recognition apparatus</span>
        <span itemprop="definition">calculates output data 209 from an input image 201 based on an image recognition model 210 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition apparatus</span>
        <span itemprop="definition">may estimate a class of a landmark and a reference point of the landmark from the input image 201 based on the image recognition model 210 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a landmark</span>
        <span itemprop="definition">will be described hereinafter with reference to FIG. 3</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a class and a reference point of the landmark</span>
        <span itemprop="definition">will be described hereinafter with reference to FIGS. 4 A through 4 F .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the output data 209</span>
        <span itemprop="definition">includes information associated with each of classes and reference points of landmarks.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition apparatus</span>
        <span itemprop="definition">may identify a maximum of N landmarks and calculate, as the output data 209 , coordinates of a reference point of an i th landmark and class information of the i th landmark.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the i th landmark</span>
        <span itemprop="definition">is indicated as Obj i</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the coordinates of the reference point of the i th landmark</span>
        <span itemprop="definition">for example, ( â circumflex over (x) â  i ,  â  i )</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the class information of the ith landmark</span>
        <span itemprop="definition">for example,  â  i , in which N denotes an integer greater than or equal to 1, and i denotes an integer greater than or equal to 1 and less than or equal to N.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the class information  â  i</span>
        <span itemprop="definition">indicates a class indicating a type, or a class as described herein, to which the i th landmark belongs among various classes of landmark.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a landmark in an input image including various objects</span>
        <span itemprop="definition">will be described hereinafter with reference to FIG. 3 .</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3</span>
        <span itemprop="definition">is a diagram illustrating examples of objects in an image.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an input image 301</span>
        <span itemprop="definition">includes various objects.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the objects</span>
        <span itemprop="definition">include a traveling object 381 that travels on a road, and a vehicle may be the traveling object 381 .</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the objects</span>
        <span itemprop="definition">also include a traveling object 382 that moves on a sidewalk, and a human being and an animal may be the traveling object 382 .</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a landmark object 390</span>
        <span itemprop="definition">which is simply referred to as a landmark herein, may represent an object fixed at a geographical location to provide a driver with information needed to drive the road.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the landmark 390</span>
        <span itemprop="definition">may include a road sign, a traffic light, and the like.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the landmark 390</span>
        <span itemprop="definition">may be installed at a main point of traffic or may be disposed at a set location, and provide useful information for autonomous driving.</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an image recognition apparatus</span>
        <span itemprop="definition">may classify a class of the landmark 390 and estimate a location of a reference point of the landmark 390 based on an image recognition model.</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a training apparatus</span>
        <span itemprop="definition">may train the image recognition model such that the image recognition model may classify the class of the landmark 390 in the input image 301 , and estimate the location of the reference point of the landmark 390 .</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 4 A through 4 F</span>
        <span itemprop="definition">are diagrams illustrating examples of various landmarks.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 4 A through 4 F</span>
        <span itemprop="definition">illustrate various classes of landmark.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">landmarks</span>
        <span itemprop="definition">may be classified into a total of six classes.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">landmarks</span>
        <span itemprop="definition">may be classified into a warning sign 411 , a regulating sign 412 , an indicating sign 413 , an auxiliary sign 414 , a signal 415 , and a road marking 416 as illustrated in FIGS. 4 A through 4 F .</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4 A</span>
        <span itemprop="definition">illustrates an example of the warning sign 411 .</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the warning sign 411</span>
        <span itemprop="definition">indicates a signal that informs a user of a road of a potentially dangerous road condition or a dangerous object nearby such that the user may take safety measures, if needed.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a reference point 421 of a landmark belonging to a class of the warning sign 411</span>
        <span itemprop="definition">may be a center point of a bounding box surrounding the landmark, for example, a two-dimensional (2D) bounding box on an image.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4 B</span>
        <span itemprop="definition">illustrates an example of the regulating sign 412 .</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the regulating sign 412</span>
        <span itemprop="definition">informs a user of a road of various regulations such as limits, restrictions, prohibitions for road traffic safety.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a reference point 422 of a landmark belonging to a class of the regulating sign 412</span>
        <span itemprop="definition">may be a center point of a bounding box surrounding the landmark.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4 C</span>
        <span itemprop="definition">illustrates an example of the indicating sign 413 .</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the indicating sign 413</span>
        <span itemprop="definition">indicates a sign that informs a user of a road of indications or instructions for road traffic safety, such as, for example, a method for passage or a passage classification.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a reference point 423 of a landmark belonging to a class of the indicating sign 413</span>
        <span itemprop="definition">may be a center point of a bounding box surrounding the landmark.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4 D</span>
        <span itemprop="definition">illustrates an example of the auxiliary sign 414 .</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the auxiliary sign 414</span>
        <span itemprop="definition">indicates an additional sign that is provided in addition to main functions of the warning sign 411 , the regulating sign 412 , and the indicating sign 413 .</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a reference point 424 of a landmark belonging to a class of the auxiliary sign 414</span>
        <span itemprop="definition">may be a center point of a bounding box surrounding the landmark.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4 E</span>
        <span itemprop="definition">illustrates an example of the signal 415 .</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the signal 415</span>
        <span itemprop="definition">indicates an installation performing a function of assigning priority to various objects in traffic by displaying, lighting, or blinking a character, a sign, and the like to inform a user of a road of progresses, stops, switches, cautions, and the like in road traffic.</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a reference point 425 of a landmark belonging to a class of the signal 415</span>
        <span itemprop="definition">may be a center point of a bounding box surrounding the signal 415 .</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4 F</span>
        <span itemprop="definition">illustrates an example of the road marking 416 .</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the road marking 416</span>
        <span itemprop="definition">indicates a mark on a road that informs a user of the road of details including, for example, various cautions, regulations, and indications, through a character, a sign, a line, and the like for road traffic safety.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a reference point 426 of a landmark belonging to a class of the road marking 416</span>
        <span itemprop="definition">may be a lower right end point of the landmark, i.e., a point at a lower right end of the landmark.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a class of a landmark and a set location of a reference point of the landmark</span>
        <span itemprop="definition">are not limited to the illustrated examples.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a class of a landmark</span>
        <span itemprop="definition">may vary depending on a country, and a location of a reference point may vary depending on a class.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An image recognition model described herein</span>
        <span itemprop="definition">may be configured to more accurately estimate a location of a reference point that may vary for each class as described above.</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Other types of classification of the landmarks</span>
        <span itemprop="definition">such as, for example, Manual on Uniform Traffic Control Devices (MUTCD), Standard Highway Signs (SHS), and Vienna Convention on Road Signs and Signals standards, may be used without departing from the spirit and scope of the illustrative examples described.</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a diagram illustrating an example of training an image recognition model.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a training apparatus</span>
        <span itemprop="definition">may train an image recognition model 510 based on training data.</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training data</span>
        <span itemprop="definition">may include a pair of a training input and a training output corresponding to the training input.</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training input</span>
        <span itemprop="definition">may be a training image 501 as illustrated in FIG. 5 .</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training output</span>
        <span itemprop="definition">may be ground truth (GT) data 508 that is provided for the training input as illustrated in FIG. 5 .</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GT data 508</span>
        <span itemprop="definition">includes a GT class of a landmark in the training image 501 and GT reference point coordinates.</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a temporary output 509 from the training image 501 based on the image recognition model 510 .</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition model 510 for which training is not completed</span>
        <span itemprop="definition">may be referred to as a temporary model.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an output of the temporary model</span>
        <span itemprop="definition">may also be referred to as the temporary output 509 .</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates, as the temporary output 509 , reference point information ( â circumflex over (x) â  i ,  â  i ) of the landmark in the training image 501 and class information  â  i of the landmark in the training image 501 .</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a loss based on the calculated temporary output 509 and the GT data 508 .</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may calculate a class loss based on temporary class information and GT class information, and calculate a localization loss based on temporary reference point coordinates and GT reference point coordinates.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a total loss 507 of the temporary model with respect to the landmark in the training image 501</span>
        <span itemprop="definition">is represented by Equation 1.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L total ( C,r, â circumflex over (r) â  )</span>
        <span itemprop="definition">L cls ( C )&#43; â [ C â  1] L loc ( r, â circumflex over (r) â ,C ) [Equation 1]</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L total</span>
        <span itemprop="definition">denotes the total loss 507</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L cls and L loc</span>
        <span itemprop="definition">denote the class loss and the localization loss, respectively.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">C</span>
        <span itemprop="definition">denotes the GT class provided to the landmark in the training image 501 .</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">r</span>
        <span itemprop="definition">denotes the GT reference point coordinates (x, y) provided to the landmark in the training image 501</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">â circumflex over (r) â</span>
        <span itemprop="definition">denotes the temporary reference point coordinates ( â circumflex over (x) â ,  â ) calculated by the temporary model.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L cls (C)</span>
        <span itemprop="definition">denotes the class loss between the GT class information and the temporary class information of the landmark in the training image 501 estimated based on the temporary model.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L loc (r,  â circumflex over (r) â , C)</span>
        <span itemprop="definition">denotes the localization loss between the GT reference point information and the temporary reference point information of the landmark in the training image 501 that is estimated based on the temporary model.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">â [C â 1]</span>
        <span itemprop="definition">denotes a weight to be set by a user with respect to the localization loss.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may train the image recognition model 510 such that the total loss 507 calculated as described above is reduced, or alternatively, minimized. For example, the training apparatus may repetitively update a parameter of the image recognition model 510 until the total loss 507 converges.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization loss</span>
        <span itemprop="definition">is partially dependent on a class as represented by Equation 1 above, and thus, the training apparatus may train the image recognition model 510 such that accuracy in estimating a location, or in localization, increases as accuracy in estimating a class increases.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Equation 1</span>
        <span itemprop="definition">The calculation of a total loss based on Equation 1, and a training process will be described in greater detail with reference to FIG. 6 .</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">is a diagram illustrating an example of how an image recognition model is trained.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operations in FIG. 6</span>
        <span itemprop="definition">may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in FIG. 6 may be performed in parallel or concurrently.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more blocks of FIG. 6 , and combinations of the blocks,</span>
        <span itemprop="definition">can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1 - 5</span>
        <span itemprop="definition">are also applicable to FIG. 6 , and are incorporated herein by reference. Thus, the above description may not be repeated here.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a training apparatus</span>
        <span itemprop="definition">obtains a training input 601 from training data 690 .</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a temporary output from the training input 601 based on an image recognition model 610 .</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition model 610</span>
        <span itemprop="definition">includes a DNN 611 .</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates, as the temporary output, temporary class information DNN cls 681 and temporary reference point information DNN loc 682 .</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a class loss L cls 671 for the temporary class information DNN cls 681 as represented by Equation 2.</span>
        <meta itemprop="num_attr" content="0085">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L cls ( C )</span>
        <span itemprop="definition">â log P C [Equation 2]</span>
        <meta itemprop="num_attr" content="0085">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Equation 2</span>
        <span itemprop="definition">L cls denotes a class loss with respect to a landmark included in a training image.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Equation 2</span>
        <span itemprop="definition">is provided as an example loss function, and thus the class loss is not limited thereto.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">P C</span>
        <span itemprop="definition">denotes a class-based weight, which may be represented by Equation 3, for example.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the temporary class information DNN cls 681</span>
        <span itemprop="definition">may include a probability that a landmark in a training image belongs to a zeroth class through an M â 1th class.</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the temporary class information DNN cls 681</span>
        <span itemprop="definition">may be indicated as a class probability vector, for example, [p c 0 , . . . , p c M-1 ], in which c j denotes a j th class from among the classes and C denotes a GT class provided to a corresponding landmark.</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">â  c exp(p C )</span>
        <span itemprop="definition">denotes an exponential sum of probabilities of a landmark belonging to each of the classes.</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">p C</span>
        <span itemprop="definition">denotes a probability of a landmark belonging to a GT class C.</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a class-based weight P C</span>
        <span itemprop="definition">may indicate a quantified value of the probability estimated for the GT class C for the landmark among probabilities estimated for the classes based on the image recognition model 610 .</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may obtain the GT class C from the training data 690 .</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a localization loss L loc 672 for the temporary localization information DNN loc 682 as represented by Equation 4.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L loc</span>
        <span itemprop="definition">denotes a localization loss for a landmark in a training image.</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">m</span>
        <span itemprop="definition">denotes GT reference point coordinates of the landmark, and  â circumflex over (m) â  denotes reference point coordinates estimated based on the image recognition model 610 .</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">obtains the GT reference point coordinates from the training data 690 .</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">smooth L 1</span>
        <span itemprop="definition">denotes a function in which an L1 loss function and an L2 loss function are mixed, and indicates a type of Euclidean distance functions.</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a distance function</span>
        <span itemprop="definition">is not limited to the example function described in the foregoing.</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">determines a class-dependent localization loss by applying the class-based weight P C to a difference, for example, a value of smooth L 1 , between the temporary reference point information DNN loc 682 and the GT reference point information.</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization loss L loc 672</span>
        <span itemprop="definition">is a loss dependent on the class-based weight P C .</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may reduce a magnitude of the localization loss L loc 672 in a total loss L total 679 when accuracy of the image recognition model 610 in classification is less than a threshold accuracy, and thereby train first a portion of the image recognition model 610 corresponding to the classification.</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may increase the magnitude of the localization loss L loc 672 in the total loss L total 679 when the accuracy in the classification is greater than or equal to the threshold accuracy, and thus train a portion of the image recognition model 610 corresponding to localization.</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may first increase a performance of the image recognition model 610 in classification and then increase a performance in localization, and train the image recognition model 610 to perform class-based recognizing localization.</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may calculate a partial class loss for each of the subregions based on Equation 2 above, and calculate a total class loss for the training image based on a sum of partial class losses.</span>
        <meta itemprop="num_attr" content="0093">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may calculate a partial localization loss, and calculate a total localization loss for the training image based on a sum of partial localization losses. The calculation of a loss for each subregion will be described hereinafter with reference to FIG. 7 .</span>
        <meta itemprop="num_attr" content="0093">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">is a diagram illustrating an example of calculating a loss for each subregion during training.</span>
        <meta itemprop="num_attr" content="0094">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a training apparatus</span>
        <span itemprop="definition">calculates a temporary output 780 for each subregion from a training image 701 based on an image recognition model 710 .</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may calculate temporary class information and temporary reference point information for each subregion of an input training image.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may calculate, for each subregion of the input training image, a partial class loss between GT class information and the temporary class information calculated for each of the subregion.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may determine, to be a class loss, a sum of partial class losses calculated for the subregions, for example, 20 subregions as illustrated in FIG. 7 .</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining of a class loss</span>
        <span itemprop="definition">is not limited to the illustrated example.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">selects subregions corresponding to a GT landmark portion 781 , for example, four subregions, from among the subregions of the input training image.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a partial class loss between GT class information and temporary class information calculated for each of the selected subregions.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">determines, to be the class loss, a sum of the partial class losses calculated for the selected subregions.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">further selects subregions corresponding to a GT background portion 782 , for example, four subregions, from among the subregions of the input training image. For a balance in training of classification of classes, the training apparatus may determine the number of the subregions corresponding to the GT landmark portion 781 and the number of the subregions corresponding to the GT background portion 782 to be similar.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a partial localization loss between the GT reference point information and the temporary reference point information calculated for each of the subregions of the input training image.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">determines, to be a localization loss, a sum of partial localization losses calculated for the subregions.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">selects subregions corresponding to the GT landmark portion 781 from among the subregions of the input training image.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a partial localization loss between the calculated reference point information and the GT reference point information for each of the selected subregions.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">determines, to be a localization loss, a sum of the partial localization losses calculated for the selected subregions.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">performs the calculation without a calculation of a partial localization loss for the GT background portion 782 , and this is because the GT background portion 782 does not include a landmark, and thus there is no need to calculate a localization loss.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">transforms map data 791 to generate a GT output 792 .</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GT output 792</span>
        <span itemprop="definition">may include GT class information and GT reference point information.</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">transforms the map data 791 to generate GT reference point information of the GT landmark portion 781 .</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the map data 791</span>
        <span itemprop="definition">may include information associated with three-dimensional (3D) coordinates at which a landmark is located.</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">transforms 3D coordinates of a landmark in the training image 701 into 2D coordinates based on a location and a posture, for example, a posture of a vehicle, in the map data 791 at which the training image 701 is captured, and a viewing angle of an image sensor capturing the training image 701 .</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a partial localization loss based on a difference between temporary reference point coordinates calculated for a subregion and GT reference point coordinates 793 transformed from the map data 791 .</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a total loss 770 based on the class loss, which is a sum of the partial class losses for the subregions, and the localization loss, which is a sum of the partial localization losses for the subregions.</span>
        <meta itemprop="num_attr" content="0099">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an anchor node used to calculate a partial class loss and a partial localization loss for each subregion</span>
        <span itemprop="definition">will be described with reference to FIG. 8 .</span>
        <meta itemprop="num_attr" content="0100">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 8</span>
        <span itemprop="definition">is a diagram illustrating an example calculating a loss for each anchor node in each subregion during training.</span>
        <meta itemprop="num_attr" content="0101">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a training apparatus</span>
        <span itemprop="definition">calculates temporary class information and temporary reference point information for each of anchor nodes 881 that is set for each subregion. For example, the training apparatus may set five anchor nodes 881 for each subregion as illustrated.</span>
        <meta itemprop="num_attr" content="0102">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates temporary localization coordinates 871 ( â circumflex over (x) â ,  â ) for each of the anchor nodes 881 in a subregion 880 of a training image.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a difference between the temporary localization coordinates 871 ( â circumflex over (x) â ,  â ) and GT coordinates (x, y) 872 of a GT reference point 891 included in a subregion 890 corresponding to a reference output.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a partial localization loss 879 from a sum of differences between the GT coordinates (x, y) 872 and the temporary localization coordinates ( â circumflex over (x) â ,  â ) 871 calculated for the anchor nodes 881 .</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may calculate, as temporary reference point information, an offset from each of the anchor nodes 881 to a reference point, for each of the anchor nodes 881 .</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the offset</span>
        <span itemprop="definition">may indicate an amount of positional change from a pixel location of each of the anchor nodes 881 to the reference point.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates temporary class information  â  861 for each of the anchor nodes 881 in the subregion 880 of the training image.</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a partial class loss 869 from GT class information c 862 included in the subregion 890 corresponding to the reference output and the temporary class information  â  861 .</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates the partial localization loss 879 and the partial class loss 869 for a subregion by adding losses calculated for the anchor nodes 881 .</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">calculates a total loss by adding partial losses for a plurality of subregions.</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may calculate temporary class information and temporary reference point information for an anchor node having a top confidence level among the anchor nodes 881 , based on a confidence level calculated for each of the anchor nodes 881 .</span>
        <meta itemprop="num_attr" content="0106">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may select K anchor nodes having top confidence levels in sequential order from among the anchor nodes 881 , and calculate temporary class information and temporary reference point information for each of the selected K anchor nodes. Based on the K anchor nodes selected from a subregion, the training apparatus may calculate a partial loss for the subregion.</span>
        <meta itemprop="num_attr" content="0106">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">K</span>
        <span itemprop="definition">denotes an integer greater than or equal to 1.</span>
        <meta itemprop="num_attr" content="0106">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may perform the calculation, without a calculation for an anchor node having a confidence level less than a threshold confidence level, based on a confidence level calculated for each of the anchor nodes 881 . That is, the training apparatus may not calculate a loss for the anchor node having the confidence level less than the threshold confidence level among the selected K anchor nodes in the subregion. Thus, the training apparatus may calculate a loss only using an anchor node that satisfies the threshold confidence level among the K anchor nodes having the top confidence levels.</span>
        <meta itemprop="num_attr" content="0106">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 9</span>
        <span itemprop="definition">is a diagram illustrating an example of a training apparatus.</span>
        <meta itemprop="num_attr" content="0107">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a training apparatus 900</span>
        <span itemprop="definition">includes a processor 910 and a memory 920 .</span>
        <meta itemprop="num_attr" content="0108">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 910</span>
        <span itemprop="definition">may calculate a class loss and a class-dependent localization loss from training data based on an image recognition model 921 .</span>
        <meta itemprop="num_attr" content="0109">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 910</span>
        <span itemprop="definition">may train the image recognition model 921 using a total loss calculated based on the class loss and the localization loss.</span>
        <meta itemprop="num_attr" content="0109">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">operations of the processor 910</span>
        <span itemprop="definition">are not limited to what is described in the forgoing, and the processor 910 may perform at least one method described above with reference to FIGS. 1 to 8 or an algorithm corresponding thereto.</span>
        <meta itemprop="num_attr" content="0109">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 910</span>
        <span itemprop="definition">refers to a data processing device configured as hardware with a circuitry in a physical structure to execute desired operations.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the desired operations</span>
        <span itemprop="definition">may include codes or instructions included in a program.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the data processing device configured as hardware</span>
        <span itemprop="definition">may include a microprocessor, a central processing unit (CPU), a processor core, a multicore processor, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA).</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 910</span>
        <span itemprop="definition">executes the program and controls the image recognition model.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 910</span>
        <span itemprop="definition">may be a graphics processor unit (GPU), reconfigurable processor, or have any other type of multi- or single-processor configuration.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the program code executed by the processor 910</span>
        <span itemprop="definition">is stored in the memory 920 . Further details regarding the processor 910 is provided below.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 920</span>
        <span itemprop="definition">may store the image recognition model 921 .</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 920</span>
        <span itemprop="definition">may also store the training data.</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 920</span>
        <span itemprop="definition">stores a result of evaluating, by the image recognition model 921 , of the training data.</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training data</span>
        <span itemprop="definition">may include a pair of a training input and a training output.</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training input</span>
        <span itemprop="definition">may be a training image 991 and the training output may be map data 992 , as illustrated.</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 920</span>
        <span itemprop="definition">stores a variety of information generated during the processing at the processor 910 .</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a variety of data and programs</span>
        <span itemprop="definition">may be stored in the memory 920 .</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 920</span>
        <span itemprop="definition">may include, for example, a volatile memory or a non-volatile memory.</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 920</span>
        <span itemprop="definition">may include a mass storage medium, such as a hard disk, to store a variety of data. Further details regarding the memory 920 is provided below.</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus 900</span>
        <span itemprop="definition">may obtain GT reference point information and GT class information by transforming the map data 992 .</span>
        <meta itemprop="num_attr" content="0112">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 910</span>
        <span itemprop="definition">may extract a landmark that may be captured by an image sensor from among landmarks included in the map data 992 , based on a location, a posture, and a viewing angle of the image sensor capturing the training image 991 , and transform 3D coordinates of the extracted landmark into 2D coordinates on an image.</span>
        <meta itemprop="num_attr" content="0112">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 10</span>
        <span itemprop="definition">is a diagram illustrating an example of a training method.</span>
        <meta itemprop="num_attr" content="0113">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operations in FIG. 10</span>
        <span itemprop="definition">may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in FIG. 10 may be performed in parallel or concurrently.</span>
        <meta itemprop="num_attr" content="0113">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more blocks of FIG. 10 , and combinations of the blocks,</span>
        <span itemprop="definition">can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions.</span>
        <meta itemprop="num_attr" content="0113">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1 - 9</span>
        <span itemprop="definition">are also applicable to FIG. 10 , and are incorporated herein by reference. Thus, the above description may not be repeated here.</span>
        <meta itemprop="num_attr" content="0113">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a training apparatus</span>
        <span itemprop="definition">calculates a class loss and a class-dependent localization loss from training data based on an image recognition model. For example, the training apparatus may calculate temporary class information and temporary reference point information from an input training image based on the image recognition model. The training apparatus may calculate the class loss based on the temporary class information and GT class information. The training apparatus may calculate the localization loss based on the temporary reference point information and GT reference point information.</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may calculate a class-based weight based on the temporary class information.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">may determine the class-dependent localization loss based on the class-based weight, the temporary reference point information, and the GT reference point information. The determining of a class-dependent localization loss is described above with reference to FIG. 6 , and thus a more detailed and repeated description is omitted here for brevity.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training apparatus</span>
        <span itemprop="definition">trains the image recognition model using a total loss calculated based on the class loss and the localization loss. For example, the training apparatus may update a parameter of the image recognition model such that the total loss is minimized. The training apparatus may repetitively update the parameter of the image recognition model until the total loss converges. Thus, the training apparatus may update the parameter such that the class loss is minimized first before the localization loss is minimized.</span>
        <meta itemprop="num_attr" content="0116">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 11 A through 11 C</span>
        <span itemprop="definition">are diagrams illustrating examples of reducing a loss through training for each loss function.</span>
        <meta itemprop="num_attr" content="0117">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a vertical axis</span>
        <span itemprop="definition">indicates a magnitude of a class loss</span>
        <meta itemprop="num_attr" content="0118">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a horizontal axis</span>
        <span itemprop="definition">indicates a magnitude of a localization loss</span>
        <meta itemprop="num_attr" content="0118">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 11 A</span>
        <span itemprop="definition">illustrates an example of how training progresses based on a loss function as represented by Equation 5.</span>
        <meta itemprop="num_attr" content="0119">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L total ( C,r, â circumflex over (r) â  )</span>
        <span itemprop="definition">L cls ( C )&#43; L loc ( r, â circumflex over (r) â  ) [Equation 5]</span>
        <meta itemprop="num_attr" content="0119">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L loc (r,  â circumflex over (r) â )</span>
        <span itemprop="definition">denotes a localization loss irrespective of a class.</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the loss function</span>
        <span itemprop="definition">is defined as represented by Equation 5, a class loss and a localization loss may linearly and equally decrease with respect to each other.</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 11 B</span>
        <span itemprop="definition">illustrates an example of how training progresses based on a loss function as represented by Equation 6.</span>
        <meta itemprop="num_attr" content="0121">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 11 C</span>
        <span itemprop="definition">illustrates an example of how training progresses based on a loss function as represented by Equation 7.</span>
        <meta itemprop="num_attr" content="0121">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L total ( C,r, â circumflex over (r) â  )</span>
        <span itemprop="definition">L cls ( C )&#43;exp(  â L cls ( C ))*( L loc ( r, â circumflex over (r) â  ) [Equation 6]</span>
        <meta itemprop="num_attr" content="0121">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L total ( C,r, â circumflex over (r) â  )</span>
        <span itemprop="definition">L cls ( C )&#43;exp(  â L cls ( C ))*( L loc ( r, â circumflex over (r) â  ) 2 [Equation 7]</span>
        <meta itemprop="num_attr" content="0121">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Equations 6 and 7</span>
        <span itemprop="definition">a localization loss L loc (r,  â circumflex over (r) â ) and a class loss L cls (C) may be associated with each other.</span>
        <meta itemprop="num_attr" content="0122">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">L loc (r,  â circumflex over (r) â ) and a class loss L cls (C)</span>
        <span itemprop="definition">may be associated with each other.</span>
        <meta itemprop="num_attr" content="0122">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 12</span>
        <span itemprop="definition">is a diagram illustrating an example of an image recognition apparatus.</span>
        <meta itemprop="num_attr" content="0123">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an image recognition apparatus 1200</span>
        <span itemprop="definition">includes an image acquirer 1210 , a processor 1220 , a memory 1230 , a UI or a display 1240 , and a communication interface 1250 .</span>
        <meta itemprop="num_attr" content="0124">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1220 , the memory 1230 , the image acquirer 1210 , the UI or the display 1240 , and the communication interface 1250</span>
        <span itemprop="definition">communicate with each other through a communication bus 1205 .</span>
        <meta itemprop="num_attr" content="0124">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image acquirer 1210</span>
        <span itemprop="definition">may obtain an input image.</span>
        <meta itemprop="num_attr" content="0125">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image acquirer 1210</span>
        <span itemprop="definition">may include an image sensor configured to capture an image.</span>
        <meta itemprop="num_attr" content="0125">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image sensor</span>
        <span itemprop="definition">may be embodied by, for example, a color camera, a depth sensor, an infrared sensor, a thermal image sensor, a radio detection and ranging (RADAR) sensor, a light detection and ranging (LiDAR) sensor, and the like.</span>
        <meta itemprop="num_attr" content="0125">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">RADAR</span>
        <span itemprop="definition">radio detection and ranging</span>
        <meta itemprop="num_attr" content="0125">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">LiDAR</span>
        <span itemprop="definition">light detection and ranging</span>
        <meta itemprop="num_attr" content="0125">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1220</span>
        <span itemprop="definition">may estimate a class of a landmark in the input image and a reference point of the landmark based on an image recognition model. For example, the processor 1220 may output a class of each of landmarks in the input image, and coordinates of a reference point of each of the landmarks in the input image. Also, the processor 1220 performs at least one method described above with reference to FIGS. 1 to 11 or an algorithm corresponding thereto.</span>
        <meta itemprop="num_attr" content="0126">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1220</span>
        <span itemprop="definition">refers to a data processing device configured as hardware with a circuitry in a physical structure to execute desired operations.</span>
        <meta itemprop="num_attr" content="0127">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the desired operations</span>
        <span itemprop="definition">may include codes or instructions included in a program.</span>
        <meta itemprop="num_attr" content="0127">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the data processing device configured as hardware</span>
        <span itemprop="definition">may include a microprocessor, a central processing unit (CPU), a processor core, a multicore processor, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA).</span>
        <meta itemprop="num_attr" content="0127">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1220</span>
        <span itemprop="definition">executes the program and controls the image recognition model.</span>
        <meta itemprop="num_attr" content="0127">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1220</span>
        <span itemprop="definition">may be a graphics processor unit (GPU), reconfigurable processor, or have any other type of multi- or single-processor configuration.</span>
        <meta itemprop="num_attr" content="0127">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the program code executed by the processor 1220</span>
        <span itemprop="definition">is stored in the memory 1230 . Further details regarding the processor 1030 is provided below.</span>
        <meta itemprop="num_attr" content="0127">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1230</span>
        <span itemprop="definition">may store the image recognition model for which training is completed.</span>
        <meta itemprop="num_attr" content="0128">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition model</span>
        <span itemprop="definition">may indicate a model having a parameter updated through a training process described above with reference to FIG. 1 through FIG. 11 .</span>
        <meta itemprop="num_attr" content="0128">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the parameter of the image recognition model</span>
        <span itemprop="definition">may not be set, but the image recognition apparatus 1200 may later update the parameter of the image recognition model in a real-time recognition process.</span>
        <meta itemprop="num_attr" content="0128">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1230</span>
        <span itemprop="definition">stores a variety of information generated during the processing at the processor 1220 .</span>
        <meta itemprop="num_attr" content="0129">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a variety of data and programs</span>
        <span itemprop="definition">may be stored in the memory 1230 .</span>
        <meta itemprop="num_attr" content="0129">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1230</span>
        <span itemprop="definition">may include, for example, a volatile memory or a non-volatile memory.</span>
        <meta itemprop="num_attr" content="0129">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1230</span>
        <span itemprop="definition">may include a mass storage medium, such as a hard disk, to store a variety of data. Further details regarding the memory 1230 is provided below.</span>
        <meta itemprop="num_attr" content="0129">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition apparatus 1200</span>
        <span itemprop="definition">may accurately estimate a location of a landmark using an image recognition model trained based on a class-dependent localization loss function as described above with reference to FIGS. 1 through 11 .</span>
        <meta itemprop="num_attr" content="0130">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the UI or a display 1240</span>
        <span itemprop="definition">outputs the location of a landmark estimated by the processor 1220 , or displays a virtual object indicating the landmark on the map data based on the accurately estimated location of the landmark.</span>
        <meta itemprop="num_attr" content="0131">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the UI or a display 1240</span>
        <span itemprop="definition">is a physical structure that includes one or more hardware components that provide the ability to render a user interface, render a display, and/or receive user input.</span>
        <meta itemprop="num_attr" content="0131">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the UI or a display 1240</span>
        <span itemprop="definition">is not limited to the example described above, and any other displays, such as, for example, smart phone and eye glass display (EGD) that are operatively connected to the image recognition apparatus 1200 may be used without departing from the spirit and scope of the illustrative examples described.</span>
        <meta itemprop="num_attr" content="0131">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition apparatus 1200</span>
        <span itemprop="definition">may perform localization on a vehicle or a mobile terminal.</span>
        <meta itemprop="num_attr" content="0132">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">vehicle described herein</span>
        <span itemprop="definition">refers to any mode of transportation, delivery, or communication such as, for example, an automobile, a truck, a tractor, a scooter, a motorcycle, a cycle, an amphibious vehicle, a snowmobile, a boat, a public transit vehicle, a bus, a monorail, a train, a tram, an autonomous or automated driving vehicle, an intelligent vehicle, a self-driving vehicle, an unmanned aerial vehicle, an electric vehicle (EV), a hybrid vehicle, a smart mobility device, an intelligent vehicle with an advanced driver assistance system (ADAS), or a drone.</span>
        <meta itemprop="num_attr" content="0132">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ADAS</span>
        <span itemprop="definition">advanced driver assistance system</span>
        <meta itemprop="num_attr" content="0132">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the smart mobility device</span>
        <span itemprop="definition">includes mobility devices such as, for example, electric wheels, electric kickboard, and electric bike.</span>
        <meta itemprop="num_attr" content="0132">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">vehicles</span>
        <span itemprop="definition">include motorized and non-motorized vehicles, for example, a vehicle with a power engine (for example, a cultivator or a motorcycle), a bicycle or a handcart.</span>
        <meta itemprop="num_attr" content="0132">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">image recognition apparatus 1200</span>
        <span itemprop="definition">may be included in various other devices, such as, for example, a smart phone, a walking assistance device, a wearable device, a security device, a robot, a mobile terminal, and various Internet of Things (IoT) devices.</span>
        <meta itemprop="num_attr" content="0133">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a smart phone</span>
        <span itemprop="definition">such as, for example, a smart phone, a walking assistance device, a wearable device, a security device, a robot, a mobile terminal, and various Internet of Things (IoT) devices.</span>
        <meta itemprop="num_attr" content="0133">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">IoT</span>
        <span itemprop="definition">Internet of Things</span>
        <meta itemprop="num_attr" content="0133">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition apparatus 1200</span>
        <span itemprop="definition">may estimate an accurate location of a reference point set for each class of landmarks in an input image, and thus be used to estimate a location and a direction of the vehicle or the mobile terminal.</span>
        <meta itemprop="num_attr" content="0134">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An accurate location of a reference point of a landmark</span>
        <span itemprop="definition">may be stored in map data, for example, a high-definition 3D map, and thus the location and the direction of the vehicle may be estimated based on the accurate location.</span>
        <meta itemprop="num_attr" content="0134">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image recognition apparatus 1200 , image recognition apparatus, training apparatus 900 , training apparatus, image acquirer 1210 , and other apparatuses, units, modules, devices, and other components described herein with respect to FIGS. 1 , 2 , 5 , 7 , 9 , and 12</span>
        <span itemprop="definition">are implemented by hardware components.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">hardware components</span>
        <span itemprop="definition">that may be used to perform the operations described in this application where appropriate include controllers, sensors, generators, drivers, memories, comparators, arithmetic logic units, adders, subtractors, multipliers, dividers, integrators, and any other electronic components configured to perform the operations described in this application.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">one or more of the hardware components that perform the operations described in this application</span>
        <span itemprop="definition">are implemented by computing hardware, for example, by one or more processors or computers.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a processor or computer</span>
        <span itemprop="definition">may be implemented by one or more processing elements, such as an array of logic gates, a controller and an arithmetic logic unit, a digital signal processor, a microcomputer, a programmable logic controller, a field-programmable gate array, a programmable logic array, a microprocessor, or any other device or combination of devices that is configured to respond to and execute instructions in a defined manner to achieve a desired result.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a processor or computer</span>
        <span itemprop="definition">includes, or is connected to, one or more memories storing instructions or software that are executed by the processor or computer.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Hardware components implemented by a processor or computer</span>
        <span itemprop="definition">may execute instructions or software, such as an operating system (OS) and one or more software applications that run on the OS, to perform the operations described in this application.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">OS</span>
        <span itemprop="definition">operating system</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hardware components</span>
        <span itemprop="definition">may also access, manipulate, process, create, and store data in response to execution of the instructions or software.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">processor</span>
        <span itemprop="definition">or âcomputerâ may be used in the description of the examples described in this application, but in other examples multiple processors or computers may be used, or a processor or computer may include multiple processing elements, or multiple types of processing elements, or both.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a single hardware component or two or more hardware components</span>
        <span itemprop="definition">may be implemented by a single processor, or two or more processors, or a processor and a controller.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more hardware components</span>
        <span itemprop="definition">may be implemented by one or more processors, or a processor and a controller, and one or more other hardware components may be implemented by one or more other processors, or another processor and another controller.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more processors</span>
        <span itemprop="definition">may implement a single hardware component, or two or more hardware components.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a hardware component</span>
        <span itemprop="definition">may have any one or more of different processing configurations, examples of which include a single processor, independent processors, parallel processors, single-instruction single-data (SISD) multiprocessing, single-instruction multiple-data (SIMD) multiprocessing, multiple-instruction single-data (MISD) multiprocessing, and multiple-instruction multiple-data (MIMD) multiprocessing.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SISD</span>
        <span itemprop="definition">single-instruction single-data</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SIMD</span>
        <span itemprop="definition">single-instruction multiple-data</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">MIMD</span>
        <span itemprop="definition">multiple-instruction multiple-data</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 5 , 6 , 7 , 8 , and 10</span>
        <span itemprop="definition">that perform the operations described in this application are performed by computing hardware, for example, by one or more processors or computers, implemented as described above executing instructions or software to perform the operations described in this application that are performed by the methods.</span>
        <meta itemprop="num_attr" content="0136">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a single operation or two or more operations</span>
        <span itemprop="definition">may be performed by a single processor, or two or more processors, or a processor and a controller.</span>
        <meta itemprop="num_attr" content="0136">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more operations</span>
        <span itemprop="definition">may be performed by one or more processors, or a processor and a controller, and one or more other operations may be performed by one or more other processors, or another processor and another controller.</span>
        <meta itemprop="num_attr" content="0136">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more processors, or a processor and a controller</span>
        <span itemprop="definition">may perform a single operation, or two or more operations.</span>
        <meta itemprop="num_attr" content="0136">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Instructions or software to control a processor or computer to implement the hardware components and perform the methods as described above</span>
        <span itemprop="definition">are written as computer programs, code segments, instructions or any combination thereof, for individually or collectively instructing or configuring the processor or computer to operate as a machine or special-purpose computer to perform the operations performed by the hardware components and the methods as described above.</span>
        <meta itemprop="num_attr" content="0137">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software</span>
        <span itemprop="definition">includes at least one of an applet, a dynamic link library (DLL), middleware, firmware, a device driver, an application program storing the method of outputting the state information.</span>
        <meta itemprop="num_attr" content="0137">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software</span>
        <span itemprop="definition">include machine code that is directly executed by the processor or computer, such as machine code produced by a compiler.</span>
        <meta itemprop="num_attr" content="0137">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software</span>
        <span itemprop="definition">include higher-level code that is executed by the processor or computer using an interpreter. Programmers of ordinary skill in the art can readily write the instructions or software based on the block diagrams and the flow charts illustrated in the drawings and the corresponding descriptions in the specification, which disclose algorithms for performing the operations performed by the hardware components and the methods as described above.</span>
        <meta itemprop="num_attr" content="0137">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software to control computing hardware</span>
        <span itemprop="definition">for example, one or more processors or computers, to implement the hardware components and perform the methods as described above, and any associated data, data files, and data structures, may be recorded, stored, or fixed in or on one or more non-transitory computer-readable storage media.</span>
        <meta itemprop="num_attr" content="0138">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Examples of a non-transitory computer-readable storage medium</span>
        <span itemprop="definition">include read-only memory (ROM), random-access programmable read only memory (PROM), electrically erasable programmable read-only memory (EEPROM), random-access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), flash memory, non-volatile memory, CD-ROMs, CD-Rs, CD&#43;Rs, CD-RWs, CD&#43;RWs, DVD-ROMs, DVD-Rs, DVD&#43;Rs, DVD-RWs, DVD&#43;RWs, DVD-RAMs, BD-ROMs, BD-Rs, BD-R LTHs, BD-REs, blue-ray or optical disk storage, hard disk drive (HDD), solid state drive (SSD), flash memory, card type memory such as multimedia card, secure digital (SD) card, or extreme digital (XD) card, magnetic tapes, floppy disks, magneto-optical data storage devices, optical data storage devices, hard disks, solid-state disks, and</span>
        <meta itemprop="num_attr" content="0138">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software and any associated data, data files, and data structures</span>
        <span itemprop="definition">are distributed over network-coupled computer systems so that the instructions and software and any associated data, data files, and data structures are stored, accessed, and executed in a distributed fashion by the one or more processors or computers.</span>
        <meta itemprop="num_attr" content="0138">
      </li>
    </ul>
  </section>

  


  <section itemprop="abstract" itemscope>
    <h2>Abstract</h2>
    
    <div itemprop="content" html><abstract mxw-id="PA556399916" lang="EN" source="national office" load-source="docdb">
    <div class="abstract">An apparatus and method to train an image recognition model to accurately estimate a location of a reference point for each class of landmark is disclosed. The apparatus and method use the image recognition model, which is trained based on calculating a class loss and a class-dependent localization loss from training data based on an image recognition model and training the image recognition model using a total loss comprising the class loss and the localization loss.</div>
  </abstract>
  </div>
  </section>

  <section itemprop="description" itemscope>
    <h2>Description</h2>
    
    <div itemprop="content" html><div mxw-id="PDES372498031" lang="EN" load-source="patent-office" class="description">
    
    <heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
    <div id="p-0002" num="0001" class="description-paragraph">This application claims the benefit under 35 USC Â§ 119(a) of Korean Patent Application No. 10-2018-0123487 filed on Oct. 17, 2018, in the Korean Intellectual Property Office, the entire disclosure of which is incorporated herein by reference for all purposes.</div>
    <heading id="h-0002"> <figure-callout id="1" label="BACKGROUND" filenames="US11544507-20230103-D00010.png" state="{{state}}">BACKGROUND</figure-callout> </heading>
    <heading id="h-0003">1. Field</heading>
    <div id="p-0003" num="0002" class="description-paragraph">The following description relates to training an image recognition model.</div>
    <heading id="h-0004">2. Description of Related Art</heading>
    <div id="p-0004" num="0003" class="description-paragraph">Automation of image recognition has been implemented through a processor implemented neural network model, as a specialized computational architecture, which after substantial training may provide computationally intuitive mappings between input patterns and output patterns. The trained capability of generating such mappings may be referred to as a learning capability of the neural network. Further, because of the specialized training, such specially trained neural network may thereby have a generalization capability of generating a relatively accurate output with respect to an input pattern that the neural network may not have been trained to recognize. However, because such operations or applications are performed through specialized computation architecture, and in different automated manners than they would have been performed in non-computer implemented or non-automated approaches, they also invite problems or drawbacks that only occur because of the automated and specialized computational architecture on which they are implement.</div>
    <heading id="h-0005">SUMMARY</heading>
    <div id="p-0005" num="0004" class="description-paragraph">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.</div>
    <div id="p-0006" num="0005" class="description-paragraph">In one general aspect, there is provided a method of training an image recognition model, including calculating a class loss and a class-dependent localization loss from training data based on an image recognition model, and training the image recognition model using a total loss including the class loss and the localization loss.</div>
    <div id="p-0007" num="0006" class="description-paragraph">The calculating of the class loss and the class-dependent localization loss may include calculating temporary class information and temporary reference point information from an input training image based on the image recognition model, calculating the class loss based on the temporary class information and ground truth class information, and calculating the localization loss based on the temporary reference point information and ground truth reference point information.</div>
    <div id="p-0008" num="0007" class="description-paragraph">The calculating of the temporary class information and the temporary reference point information may include calculating temporary class information and temporary reference point information for each of subregions of the input training image.</div>
    <div id="p-0009" num="0008" class="description-paragraph">The calculating of the class loss may include calculating a partial class loss between the ground truth class information and the temporary class information calculated for the each of the subregions of the input training image, and determining a sum of partial class losses calculated for the each of the subregions of the input training image to be the class loss.</div>
    <div id="p-0010" num="0009" class="description-paragraph">The calculating of the class loss may include selecting subregions corresponding to a ground truth landmark portion from among the subregions of the input training image, calculating a partial class loss between the ground truth class information and temporary class information calculated for each of the selected subregions, and determining a sum of partial class losses calculated for the selected subregions to be the class loss.</div>
    <div id="p-0011" num="0010" class="description-paragraph">The selecting of the subregions may include further selecting a subregion corresponding a ground truth background portion from among the subregions of the input training image.</div>
    <div id="p-0012" num="0011" class="description-paragraph">The calculating of the localization loss may include calculating, for each of the subregions of the input training image, a partial localization loss between the ground truth reference point information and temporary reference point information calculated for the each of the subregions of the input training image, and determining a sum of partial localization losses calculated for the each of the subregions to be the localization loss.</div>
    <div id="p-0013" num="0012" class="description-paragraph">The calculating of the localization loss may include selecting subregions corresponding to a ground truth landmark portion from among the subregions of the input training image, calculating a partial localization loss between the ground truth reference point information and temporary reference point information of each of the selected subregions, and determining a sum of partial localization losses calculated for the selected subregions to be the localization loss.</div>
    <div id="p-0014" num="0013" class="description-paragraph">The calculating of the partial localization loss may include excluding a subregion with a ground truth background portion from the selected subregions.</div>
    <div id="p-0015" num="0014" class="description-paragraph">The calculating of the temporary class information and the temporary reference point information for the each of the subregions of the input training image may include calculating temporary class information and temporary reference point information for each of anchor nodes set for the each of the subregions.</div>
    <div id="p-0016" num="0015" class="description-paragraph">The calculating of the temporary class information and the temporary reference point information for the each of the anchor nodes may include calculating temporary class information and temporary reference point information for an anchor node having a highest confidence level from among confidence levels calculated for each of the anchor nodes.</div>
    <div id="p-0017" num="0016" class="description-paragraph">The calculating of the temporary class information and the temporary reference point information for each of the anchor nodes may include excluding an anchor node having a confidence level less than a threshold from among confidence levels calculated for each of the anchor nodes.</div>
    <div id="p-0018" num="0017" class="description-paragraph">The calculating of the class loss and the class-dependent localization loss may include calculating a class-based weight based on temporary class information, and determining the class-dependent localization loss based on the class-based weight, temporary reference point information, and ground truth reference point information.</div>
    <div id="p-0019" num="0018" class="description-paragraph">The determining of the class-dependent localization loss may include determining the class-dependent localization loss by applying the class-based weight to a difference between the temporary reference point information and the ground truth reference point information.</div>
    <div id="p-0020" num="0019" class="description-paragraph">The training may include updating a parameter of the image recognition model to minimize the total loss.</div>
    <div id="p-0021" num="0020" class="description-paragraph">The updating of the parameter may include repeating the updating of the parameter of the image recognition model to converge the total loss.</div>
    <div id="p-0022" num="0021" class="description-paragraph">The updating of the parameter may include updating the parameter such that the class loss is minimized before the localization loss is minimized.</div>
    <div id="p-0023" num="0022" class="description-paragraph">In another general aspect, there is provided a training apparatus including a memory configured to store an image recognition model, and a processor configured to calculate a class loss and a class-dependent localization loss from training data based on the image recognition model, and train the image recognition model using a total loss including the class loss and the localization loss.</div>
    <div id="p-0024" num="0023" class="description-paragraph">In another general aspect, there is provided an image recognition method including obtaining an input image, and estimating, from the input image, a class of a landmark in the input image and a reference point of the landmark, based on an image recognition model.</div>
    <div id="p-0025" num="0024" class="description-paragraph">Other features and aspects will be apparent from the following detailed description, the drawings, and the claims.</div>
    
    
    <description-of-drawings>
      <heading id="h-0006">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <div id="p-0026" num="0025" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> is a diagram illustrating an example of an image recognition model.</div>
      <div id="p-0027" num="0026" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> is a diagram illustrating an example of recognizing an input image based on an image recognition model.</div>
      <div id="p-0028" num="0027" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>3</b> </figref> is a diagram illustrating examples of objects in an image.</div>
      <div id="p-0029" num="0028" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. <b>4</b>A through <b>4</b>F</figref> are diagrams illustrating examples of various landmarks.</div>
      <div id="p-0030" num="0029" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref> is a diagram illustrating an example of training an image recognition model.</div>
      <div id="p-0031" num="0030" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> is a diagram illustrating an example of training an image recognition model.</div>
      <div id="p-0032" num="0031" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref> is a diagram illustrating an example of calculating a loss for each subregion during training.</div>
      <div id="p-0033" num="0032" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>8</b> </figref> is a diagram illustrating an example of calculating a loss for each anchor node in each subregion during training.</div>
      <div id="p-0034" num="0033" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>9</b> </figref> is a diagram illustrating an example of a training apparatus.</div>
      <div id="p-0035" num="0034" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref> is a diagram illustrating an example of a training method.</div>
      <div id="p-0036" num="0035" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. <b>11</b>A through <b>11</b>C</figref> are diagrams illustrating examples of reducing a loss through training for each loss function.</div>
      <div id="p-0037" num="0036" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>12</b> </figref> is a diagram illustrating an example of an image recognition apparatus.</div>
    </description-of-drawings>
    
    
    <div id="p-0038" num="0037" class="description-paragraph">Throughout the drawings and the detailed description, unless otherwise described or provided, the same drawing reference numerals will be understood to refer to the same elements, features, and structures. The drawings may not be to scale, and the relative size, proportions, and depiction of elements in the drawings may be exaggerated for clarity, illustration, and convenience.</div>
    <heading id="h-0007">DETAILED DESCRIPTION</heading>
    <div id="p-0039" num="0038" class="description-paragraph">The following detailed description is provided to assist the reader in gaining a comprehensive understanding of the methods, apparatuses, and/or systems described herein. However, various changes, modifications, and equivalents of the methods, apparatuses, and/or systems described herein will be apparent after an understanding of the disclosure of this application. For example, the sequences of operations described herein are merely examples, and are not limited to those set forth herein, but may be changed as will be apparent after an understanding of the disclosure of this application, with the exception of operations necessarily occurring in a certain order. Also, descriptions of features that are known in the art may be omitted for increased clarity and conciseness.</div>
    <div id="p-0040" num="0039" class="description-paragraph">The features described herein may be embodied in different forms and are not to be construed as being limited to the examples described herein. Rather, the examples described herein have been provided merely to illustrate some of the many possible ways of implementing the methods, apparatuses, and/or systems described herein that will be apparent after an understanding of the disclosure of this application.</div>
    <div id="p-0041" num="0040" class="description-paragraph">Although terms such as âfirst,â âsecond,â and âthirdâ may be used herein to describe various members, components, regions, layers, or sections, these members, components, regions, layers, or sections are not to be limited by these terms. Rather, these terms are only used to distinguish one member, component, region, layer, or section from another member, component, region, layer, or section. Thus, a first member, component, region, layer, or section referred to in examples described herein may also be referred to as a second member, component, region, layer, or section without departing from the teachings of the examples.</div>
    <div id="p-0042" num="0041" class="description-paragraph">Throughout the specification, when a component is described as being âconnected to,â or âcoupled toâ another component, it may be directly âconnected to,â or âcoupled toâ the other component, or there may be one or more other components intervening therebetween. In contrast, when an element is described as being âdirectly connected to,â or âdirectly coupled toâ another element, there can be no other elements intervening therebetween. Likewise, similar expressions, for example, âbetweenâ and âimmediately between,â and âadjacent toâ and âimmediately adjacent to,â are also to be construed in the same way.</div>
    <div id="p-0043" num="0042" class="description-paragraph">As used herein, the term âand/orâ includes any one and any combination of any two or more of the associated listed items.</div>
    <div id="p-0044" num="0043" class="description-paragraph">The terminology used herein is for describing various examples only and is not to be used to limit the disclosure. The articles âa,â âan,â and âtheâ are intended to include the plural forms as well, unless the context clearly indicates otherwise. The terms âcomprises,â âincludes,â and âhasâ specify the presence of stated features, numbers, operations, members, elements, and/or combinations thereof, but do not preclude the presence or addition of one or more other features, numbers, operations, members, elements, and/or combinations thereof.</div>
    <div id="p-0045" num="0044" class="description-paragraph">The use of the term âmayâ herein with respect to an example or embodiment, e.g., as to what an example or embodiment may include or implement, means that at least one example or embodiment exists where such a feature is included or implemented while all examples and embodiments are not limited thereto.</div>
    <div id="p-0046" num="0045" class="description-paragraph">Also, in the description of example embodiments, detailed description of structures or functions that are thereby known after an understanding of the disclosure of the present application will be omitted when it is deemed that such description will cause ambiguous interpretation of the example embodiments.</div>
    <div id="p-0047" num="0046" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> is a diagram illustrating an example of an image recognition model.</div>
    <div id="p-0048" num="0047" class="description-paragraph">In an example, an image recognition model may be of a machine learning architecture trained to output a result of recognizing an input image. For example, the image recognition model may be embodied by a <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b>, as illustrated in <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref>. However, examples of the image recognition model are not limited to the illustrated example. In an example, the image recognition model may be trained by an apparatus for training the image recognition model, hereinafter simply referred to as a training apparatus. The training apparatus may train the image recognition model with a location and a type, or a class as described herein, of an object, such as, a sign in an image that is output from a camera installed in a vehicle. An image recognition apparatus may recognize the input image based on the trained image recognition model. For example, the image recognition apparatus may identify an object in the input image based on the trained image recognition model. However, in other examples, the training apparatus and the image recognition apparatus may be integrated together to be embodied as an integral apparatus. Hereinafter, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> will be described as an example of the image recognition model with reference to <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref>.</div>
    <div id="p-0049" num="0048" class="description-paragraph">In an example, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may be configured as a single network and may also be configured as a recurrent network. In an example, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may be a deep neural network (DNN). The DNN may include a fully-connected network (FCN), a deep convolutional network (DCN), a recurrent neural network (RNN), a long-short term memory (LSTM) network, and a grated recurrent units (GRUs).</div>
    <div id="p-0050" num="0049" class="description-paragraph">In an example, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may be implemented as an architecture having a plurality of layers including an input image, feature maps, and an output. In the neural network, a convolution operation between the input image, and a filter referred to as a kernel, is performed, and as a result of the convolution operation, the feature maps are output. Here, the feature maps that are output are input feature maps, and a convolution operation between the output feature maps and the kernel is performed again, and as a result, new feature maps are output. Based on such repeatedly performed convolution operations, results of recognition of characteristics of the input image via the neural network may be output.</div>
    <div id="p-0051" num="0050" class="description-paragraph">The <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may map input data and output data that have a nonlinear relationship based on deep learning to perform tasks such as, for example, object classification, object recognition, audio or speech recognition, and image recognition. The deep learning may be a type of machine learning that is applied to perform image recognition or speech recognition from a big dataset. The deep learning may be performed in supervised and/or unsupervised manners, which may be applied to perform the mapping of input data and output data.</div>
    <div id="p-0052" num="0051" class="description-paragraph">In another example, the neural network may include an input source sentence (e.g., voice entry) instead of an input image. In such an example, a convolution operation is performed on the input source sentence with a kernel, and as a result, the feature maps are output. The convolution operation is performed again on the output feature maps as input feature maps, with a kernel, and new feature maps are output. When the convolution operation is repeatedly performed as such, a recognition result with respect to features of the input source sentence may be finally output through the neural network.</div>
    <div id="p-0053" num="0052" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref>, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> includes an <figure-callout id="110" label="input layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">input layer</figure-callout> <b>110</b>, a <figure-callout id="120" label="hidden layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">hidden layer</figure-callout> <b>120</b>, and an <figure-callout id="130" label="output layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">output layer</figure-callout> <b>130</b>. Each of the <figure-callout id="110" label="input layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">input layer</figure-callout> <b>110</b>, the hidden <figure-callout id="120" label="layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">layer</figure-callout> <b>120</b>, and the <figure-callout id="130" label="output layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">output layer</figure-callout> <b>130</b> may include a plurality of artificial nodes.</div>
    <div id="p-0054" num="0053" class="description-paragraph">Although the hidden <figure-callout id="120" label="layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">layer</figure-callout> <b>120</b> is illustrated as including three layers in <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> for convenience of description, the hidden <figure-callout id="120" label="layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">layer</figure-callout> <b>120</b> may include other numbers of layers. In addition, although the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> is illustrated as including a separate input layer, for example, the <figure-callout id="110" label="input layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">input layer</figure-callout> <b>110</b>, to receive input data, the input data may be directly input to the hidden <figure-callout id="120" label="layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">layer</figure-callout> <b>120</b>. The artificial nodes described above will be simply referred to as nodes, and nodes of layers of the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> excluding ones of the <figure-callout id="130" label="output layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">output layer</figure-callout> <b>130</b> may be connected to nodes of a next layer through links to transmit an output signal. The number of the links may correspond to the number of the nodes included in the next layer. In an example, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may include additional layers, such as, for example, a sub-sampling layer, a pooling layer, and a fully connected layer.</div>
    <div id="p-0055" num="0054" class="description-paragraph">An output of an activation function associated with weighted inputs of nodes included in a previous layer may be input to each of the nodes included in the hidden <figure-callout id="120" label="layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">layer</figure-callout> <b>120</b>. The weighted inputs may be obtained by multiplying an input of the nodes included in the previous layer by a connection weight. The connection weight may also be referred to as a parameter of the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b>. In an example, the activation function may include a sigmoid function, a hyperbolic tangent (tan h) function, and a rectified linear unit (ReLU), and nonlinearity may be formed in the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> by the activation function. The weighted inputs of the nodes included in the previous layer may be input to each of the nodes included in the <figure-callout id="130" label="output layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">output layer</figure-callout> <b>130</b>.</div>
    <div id="p-0056" num="0055" class="description-paragraph">When input data is given, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may calculate a function value based on the number of classes to be classified and recognized in the <figure-callout id="130" label="output layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">output layer</figure-callout> <b>130</b> through the hidden <figure-callout id="120" label="layer" filenames="US11544507-20230103-D00001.png" state="{{state}}">layer</figure-callout> <b>120</b>, and classify and recognize the input data as a class having a greatest function value. Although the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may classify or recognize the input data, the classification and recognition by the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> will be described simply as recognition for convenience of description. Thus, the following description of the recognition may also be applied to the classification unless otherwise defined.</div>
    <div id="p-0057" num="0056" class="description-paragraph">When a width and a depth of the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> are sufficiently large, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may have a capacity such that the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> implements a function. When the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> learns a sufficiently large amount of training data through training, the <figure-callout id="100" label="neural network" filenames="US11544507-20230103-D00001.png" state="{{state}}">neural network</figure-callout> <b>100</b> may obtain an optimal performance of recognition.</div>
    <div id="p-0058" num="0057" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> is a diagram illustrating an example of recognizing an input image based on an image recognition model.</div>
    <div id="p-0059" num="0058" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref>, an image recognition apparatus calculates <figure-callout id="209" label="output data" filenames="US11544507-20230103-D00002.png" state="{{state}}">output data</figure-callout> <b>209</b> from an <figure-callout id="201" label="input image" filenames="US11544507-20230103-D00002.png" state="{{state}}">input image</figure-callout> <b>201</b> based on an <figure-callout id="210" label="image recognition model" filenames="US11544507-20230103-D00002.png" state="{{state}}">image recognition model</figure-callout> <b>210</b>. For example, the image recognition apparatus may estimate a class of a landmark and a reference point of the landmark from the <figure-callout id="201" label="input image" filenames="US11544507-20230103-D00002.png" state="{{state}}">input image</figure-callout> <b>201</b> based on the <figure-callout id="210" label="image recognition model" filenames="US11544507-20230103-D00002.png" state="{{state}}">image recognition model</figure-callout> <b>210</b>. A landmark will be described hereinafter with reference to <figref idrefs="DRAWINGS">FIG. <b>3</b> </figref>, and a class and a reference point of the landmark will be described hereinafter with reference to <figref idrefs="DRAWINGS">FIGS. <b>4</b>A through <b>4</b>F</figref>.</div>
    <div id="p-0060" num="0059" class="description-paragraph">The <figure-callout id="209" label="output data" filenames="US11544507-20230103-D00002.png" state="{{state}}">output data</figure-callout> <b>209</b> includes information associated with each of classes and reference points of landmarks. For example, the image recognition apparatus may identify a maximum of N landmarks and calculate, as the <figure-callout id="209" label="output data" filenames="US11544507-20230103-D00002.png" state="{{state}}">output data</figure-callout> <b>209</b>, coordinates of a reference point of an i<sup>th </sup>landmark and class information of the i<sup>th </sup>landmark. As illustrated in <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref>, the i<sup>th </sup>landmark is indicated as Obj<sub>i</sub>, the coordinates of the reference point of the i<sup>th </sup>landmark, for example, ({circumflex over (x)}<sub>i</sub>, Å·<sub>i</sub>), and the class information of the ith landmark, for example, Ä<sub>i</sub>, in which N denotes an integer greater than or equal to 1, and i denotes an integer greater than or equal to 1 and less than or equal to N. The class information Ä<sub>i </sub>indicates a class indicating a type, or a class as described herein, to which the i<sup>th </sup>landmark belongs among various classes of landmark.</div>
    <div id="p-0061" num="0060" class="description-paragraph">A landmark in an input image including various objects will be described hereinafter with reference to <figref idrefs="DRAWINGS">FIG. <b>3</b> </figref>.</div>
    <div id="p-0062" num="0061" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>3</b> </figref> is a diagram illustrating examples of objects in an image.</div>
    <div id="p-0063" num="0062" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>3</b> </figref>, an <figure-callout id="301" label="input image" filenames="US11544507-20230103-D00003.png" state="{{state}}">input image</figure-callout> <b>301</b> includes various objects. As illustrated, the objects include a traveling <figure-callout id="381" label="object" filenames="US11544507-20230103-D00003.png" state="{{state}}">object</figure-callout> <b>381</b> that travels on a road, and a vehicle may be the traveling <figure-callout id="381" label="object" filenames="US11544507-20230103-D00003.png" state="{{state}}">object</figure-callout> <b>381</b>. The objects also include a traveling <figure-callout id="382" label="object" filenames="US11544507-20230103-D00003.png" state="{{state}}">object</figure-callout> <b>382</b> that moves on a sidewalk, and a human being and an animal may be the traveling <figure-callout id="382" label="object" filenames="US11544507-20230103-D00003.png" state="{{state}}">object</figure-callout> <b>382</b>. A <figure-callout id="390" label="landmark object" filenames="US11544507-20230103-D00003.png" state="{{state}}">landmark object</figure-callout> <b>390</b>, which is simply referred to as a landmark herein, may represent an object fixed at a geographical location to provide a driver with information needed to drive the road. For example, the <figure-callout id="390" label="landmark" filenames="US11544507-20230103-D00003.png" state="{{state}}">landmark</figure-callout> <b>390</b> may include a road sign, a traffic light, and the like.</div>
    <div id="p-0064" num="0063" class="description-paragraph">In an example, the <figure-callout id="390" label="landmark" filenames="US11544507-20230103-D00003.png" state="{{state}}">landmark</figure-callout> <b>390</b> may be installed at a main point of traffic or may be disposed at a set location, and provide useful information for autonomous driving. Thus, an image recognition apparatus may classify a class of the <figure-callout id="390" label="landmark" filenames="US11544507-20230103-D00003.png" state="{{state}}">landmark</figure-callout> <b>390</b> and estimate a location of a reference point of the <figure-callout id="390" label="landmark" filenames="US11544507-20230103-D00003.png" state="{{state}}">landmark</figure-callout> <b>390</b> based on an image recognition model. A training apparatus may train the image recognition model such that the image recognition model may classify the class of the <figure-callout id="390" label="landmark" filenames="US11544507-20230103-D00003.png" state="{{state}}">landmark</figure-callout> <b>390</b> in the <figure-callout id="301" label="input image" filenames="US11544507-20230103-D00003.png" state="{{state}}">input image</figure-callout> <b>301</b>, and estimate the location of the reference point of the <figure-callout id="390" label="landmark" filenames="US11544507-20230103-D00003.png" state="{{state}}">landmark</figure-callout> <b>390</b>.</div>
    <div id="p-0065" num="0064" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. <b>4</b>A through <b>4</b>F</figref> are diagrams illustrating examples of various landmarks.</div>
    <div id="p-0066" num="0065" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. <b>4</b>A through <b>4</b>F</figref> illustrate various classes of landmark. In an example, according to a Korean road traffic act, landmarks may be classified into a total of six classes. For example, landmarks may be classified into a <figure-callout id="411" label="warning sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">warning sign</figure-callout> <b>411</b>, a regulating <figure-callout id="412" label="sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">sign</figure-callout> <b>412</b>, an indicating <figure-callout id="413" label="sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">sign</figure-callout> <b>413</b>, an <figure-callout id="414" label="auxiliary sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">auxiliary sign</figure-callout> <b>414</b>, a <figure-callout id="415" label="signal" filenames="US11544507-20230103-D00006.png" state="{{state}}">signal</figure-callout> <b>415</b>, and a road marking <b>416</b> as illustrated in <figref idrefs="DRAWINGS">FIGS. <b>4</b>A through <b>4</b>F</figref>.</div>
    <div id="p-0067" num="0066" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>4</b>A</figref> illustrates an example of the <figure-callout id="411" label="warning sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">warning sign</figure-callout> <b>411</b>. The <figure-callout id="411" label="warning sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">warning sign</figure-callout> <b>411</b> indicates a signal that informs a user of a road of a potentially dangerous road condition or a dangerous object nearby such that the user may take safety measures, if needed. In an example, a <figure-callout id="421" label="reference point" filenames="US11544507-20230103-D00004.png" state="{{state}}">reference point</figure-callout> <b>421</b> of a landmark belonging to a class of the <figure-callout id="411" label="warning sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">warning sign</figure-callout> <b>411</b> may be a center point of a bounding box surrounding the landmark, for example, a two-dimensional (2D) bounding box on an image.</div>
    <div id="p-0068" num="0067" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>4</b>B</figref> illustrates an example of the regulating <figure-callout id="412" label="sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">sign</figure-callout> <b>412</b>. The regulating <figure-callout id="412" label="sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">sign</figure-callout> <b>412</b> informs a user of a road of various regulations such as limits, restrictions, prohibitions for road traffic safety. A <figure-callout id="422" label="reference point" filenames="US11544507-20230103-D00004.png" state="{{state}}">reference point</figure-callout> <b>422</b> of a landmark belonging to a class of the regulating <figure-callout id="412" label="sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">sign</figure-callout> <b>412</b> may be a center point of a bounding box surrounding the landmark.</div>
    <div id="p-0069" num="0068" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>4</b>C</figref> illustrates an example of the indicating <figure-callout id="413" label="sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">sign</figure-callout> <b>413</b>. The indicating <figure-callout id="413" label="sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">sign</figure-callout> <b>413</b> indicates a sign that informs a user of a road of indications or instructions for road traffic safety, such as, for example, a method for passage or a passage classification. A <figure-callout id="423" label="reference point" filenames="US11544507-20230103-D00005.png" state="{{state}}">reference point</figure-callout> <b>423</b> of a landmark belonging to a class of the indicating <figure-callout id="413" label="sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">sign</figure-callout> <b>413</b> may be a center point of a bounding box surrounding the landmark.</div>
    <div id="p-0070" num="0069" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>4</b>D</figref> illustrates an example of the <figure-callout id="414" label="auxiliary sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">auxiliary sign</figure-callout> <b>414</b>. The <figure-callout id="414" label="auxiliary sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">auxiliary sign</figure-callout> <b>414</b> indicates an additional sign that is provided in addition to main functions of the <figure-callout id="411" label="warning sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">warning sign</figure-callout> <b>411</b>, the regulating <figure-callout id="412" label="sign" filenames="US11544507-20230103-D00004.png" state="{{state}}">sign</figure-callout> <b>412</b>, and the indicating <figure-callout id="413" label="sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">sign</figure-callout> <b>413</b>. A <figure-callout id="424" label="reference point" filenames="US11544507-20230103-D00005.png" state="{{state}}">reference point</figure-callout> <b>424</b> of a landmark belonging to a class of the <figure-callout id="414" label="auxiliary sign" filenames="US11544507-20230103-D00005.png" state="{{state}}">auxiliary sign</figure-callout> <b>414</b> may be a center point of a bounding box surrounding the landmark.</div>
    <div id="p-0071" num="0070" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>4</b>E</figref> illustrates an example of the <figure-callout id="415" label="signal" filenames="US11544507-20230103-D00006.png" state="{{state}}">signal</figure-callout> <b>415</b>. The <figure-callout id="415" label="signal" filenames="US11544507-20230103-D00006.png" state="{{state}}">signal</figure-callout> <b>415</b> indicates an installation performing a function of assigning priority to various objects in traffic by displaying, lighting, or blinking a character, a sign, and the like to inform a user of a road of progresses, stops, switches, cautions, and the like in road traffic. A <figure-callout id="425" label="reference point" filenames="US11544507-20230103-D00006.png" state="{{state}}">reference point</figure-callout> <b>425</b> of a landmark belonging to a class of the <figure-callout id="415" label="signal" filenames="US11544507-20230103-D00006.png" state="{{state}}">signal</figure-callout> <b>415</b> may be a center point of a bounding box surrounding the <figure-callout id="415" label="signal" filenames="US11544507-20230103-D00006.png" state="{{state}}">signal</figure-callout> <b>415</b>.</div>
    <div id="p-0072" num="0071" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>4</b>F</figref> illustrates an example of the road marking <b>416</b>. The road marking <b>416</b> indicates a mark on a road that informs a user of the road of details including, for example, various cautions, regulations, and indications, through a character, a sign, a line, and the like for road traffic safety. A <figure-callout id="426" label="reference point" filenames="US11544507-20230103-D00006.png" state="{{state}}">reference point</figure-callout> <b>426</b> of a landmark belonging to a class of the road marking <b>416</b> may be a lower right end point of the landmark, i.e., a point at a lower right end of the landmark.</div>
    <div id="p-0073" num="0072" class="description-paragraph">Various examples of landmark have been described above. However, a class of a landmark and a set location of a reference point of the landmark are not limited to the illustrated examples. A class of a landmark may vary depending on a country, and a location of a reference point may vary depending on a class.</div>
    <div id="p-0074" num="0073" class="description-paragraph">An image recognition model described herein may be configured to more accurately estimate a location of a reference point that may vary for each class as described above. Other types of classification of the landmarks, such as, for example, Manual on Uniform Traffic Control Devices (MUTCD), Standard Highway Signs (SHS), and Vienna Convention on Road Signs and Signals standards, may be used without departing from the spirit and scope of the illustrative examples described.</div>
    <div id="p-0075" num="0074" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref> is a diagram illustrating an example of training an image recognition model.</div>
    <div id="p-0076" num="0075" class="description-paragraph">A training apparatus may train an <figure-callout id="510" label="image recognition model" filenames="US11544507-20230103-D00007.png" state="{{state}}">image recognition model</figure-callout> <b>510</b> based on training data. The training data may include a pair of a training input and a training output corresponding to the training input. The training input may be a <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b> as illustrated in <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref>. The training output may be ground truth (GT) <figure-callout id="508" label="data" filenames="US11544507-20230103-D00007.png" state="{{state}}">data</figure-callout> <b>508</b> that is provided for the training input as illustrated in <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref>. In an example, the <figure-callout id="508" label="GT data" filenames="US11544507-20230103-D00007.png" state="{{state}}">GT data</figure-callout> <b>508</b> includes a GT class of a landmark in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b> and GT reference point coordinates.</div>
    <div id="p-0077" num="0076" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref>, the training apparatus calculates a <figure-callout id="509" label="temporary output" filenames="US11544507-20230103-D00007.png" state="{{state}}">temporary output</figure-callout> <b>509</b> from the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b> based on the <figure-callout id="510" label="image recognition model" filenames="US11544507-20230103-D00007.png" state="{{state}}">image recognition model</figure-callout> <b>510</b>. In an example, the <figure-callout id="510" label="image recognition model" filenames="US11544507-20230103-D00007.png" state="{{state}}">image recognition model</figure-callout> <b>510</b> for which training is not completed may be referred to as a temporary model. In addition, an output of the temporary model may also be referred to as the <figure-callout id="509" label="temporary output" filenames="US11544507-20230103-D00007.png" state="{{state}}">temporary output</figure-callout> <b>509</b>. As illustrated, the training apparatus calculates, as the <figure-callout id="509" label="temporary output" filenames="US11544507-20230103-D00007.png" state="{{state}}">temporary output</figure-callout> <b>509</b>, reference point information ({circumflex over (x)}<sub>i</sub>, Å·<sub>i</sub>) of the landmark in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b> and class information Ä<sub>i </sub>of the landmark in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b>.</div>
    <div id="p-0078" num="0077" class="description-paragraph">The training apparatus calculates a loss based on the calculated <figure-callout id="509" label="temporary output" filenames="US11544507-20230103-D00007.png" state="{{state}}">temporary output</figure-callout> <b>509</b> and the <figure-callout id="508" label="GT data" filenames="US11544507-20230103-D00007.png" state="{{state}}">GT data</figure-callout> <b>508</b>. For example, the training apparatus may calculate a class loss based on temporary class information and GT class information, and calculate a localization loss based on temporary reference point coordinates and GT reference point coordinates. In an example, a <figure-callout id="507" label="total loss" filenames="US11544507-20230103-D00007.png" state="{{state}}">total loss</figure-callout> <b>507</b> of the temporary model with respect to the landmark in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b> is represented by <figure-callout id="1" label="Equation" filenames="US11544507-20230103-D00010.png" state="{{state}}">Equation</figure-callout> 1.
<br/>
<i>L</i> <sub>total</sub>(<i>C,r,{circumflex over (r)}</i>)=<i>L</i> <sub>cls</sub>(<i>C</i>)+Î»[<i>Câ¥</i>1]<i>L</i> <sub>loc</sub>(<i>r,{circumflex over (r)},C</i>)ââ[Equation 1]
</div>
    <div id="p-0079" num="0078" class="description-paragraph">In <figure-callout id="1" label="Equation" filenames="US11544507-20230103-D00010.png" state="{{state}}">Equation</figure-callout> 1, L<sub>total </sub>denotes the <figure-callout id="507" label="total loss" filenames="US11544507-20230103-D00007.png" state="{{state}}">total loss</figure-callout> <b>507</b>, and L<sub>cls </sub>and L<sub>loc </sub>denote the class loss and the localization loss, respectively. C denotes the GT class provided to the landmark in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b>. r denotes the GT reference point coordinates (x, y) provided to the landmark in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b>, and {circumflex over (r)} denotes the temporary reference point coordinates ({circumflex over (x)}, Å·) calculated by the temporary model. L<sub>cls</sub>(C) denotes the class loss between the GT class information and the temporary class information of the landmark in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b> estimated based on the temporary model. L<sub>loc</sub>(r, {circumflex over (r)}, C) denotes the localization loss between the GT reference point information and the temporary reference point information of the landmark in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b> that is estimated based on the temporary model. Î»[Câ¥1] denotes a weight to be set by a user with respect to the localization loss. For example, the localization loss may be excluded from the <figure-callout id="507" label="total loss" filenames="US11544507-20230103-D00007.png" state="{{state}}">total loss</figure-callout> <b>507</b> when a class corresponding to a background, not a landmark, is included in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b>, for example, when C=0, but the localization loss may be included in the <figure-callout id="507" label="total loss" filenames="US11544507-20230103-D00007.png" state="{{state}}">total loss</figure-callout> <b>507</b> only when a landmark is included in the <figure-callout id="501" label="training image" filenames="US11544507-20230103-D00007.png" state="{{state}}">training image</figure-callout> <b>501</b>.</div>
    <div id="p-0080" num="0079" class="description-paragraph">In an example, the training apparatus may train the <figure-callout id="510" label="image recognition model" filenames="US11544507-20230103-D00007.png" state="{{state}}">image recognition model</figure-callout> <b>510</b> such that the <figure-callout id="507" label="total loss" filenames="US11544507-20230103-D00007.png" state="{{state}}">total loss</figure-callout> <b>507</b> calculated as described above is reduced, or alternatively, minimized. For example, the training apparatus may repetitively update a parameter of the <figure-callout id="510" label="image recognition model" filenames="US11544507-20230103-D00007.png" state="{{state}}">image recognition model</figure-callout> <b>510</b> until the <figure-callout id="507" label="total loss" filenames="US11544507-20230103-D00007.png" state="{{state}}">total loss</figure-callout> <b>507</b> converges.</div>
    <div id="p-0081" num="0080" class="description-paragraph">The localization loss is partially dependent on a class as represented by <figure-callout id="1" label="Equation" filenames="US11544507-20230103-D00010.png" state="{{state}}">Equation</figure-callout> 1 above, and thus, the training apparatus may train the <figure-callout id="510" label="image recognition model" filenames="US11544507-20230103-D00007.png" state="{{state}}">image recognition model</figure-callout> <b>510</b> such that accuracy in estimating a location, or in localization, increases as accuracy in estimating a class increases.</div>
    <div id="p-0082" num="0081" class="description-paragraph">The calculation of a total loss based on <figure-callout id="1" label="Equation" filenames="US11544507-20230103-D00010.png" state="{{state}}">Equation</figure-callout> 1, and a training process will be described in greater detail with reference to <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref>.</div>
    <div id="p-0083" num="0082" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> is a diagram illustrating an example of how an image recognition model is trained. The operations in <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> may be performed in parallel or concurrently. One or more blocks of <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref>, and combinations of the blocks, can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions. In addition to the description of <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref> below, the descriptions of <figref idrefs="DRAWINGS">FIG. <b>1</b>-<b>5</b> </figref> are also applicable to <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref>, and are incorporated herein by reference. Thus, the above description may not be repeated here.</div>
    <div id="p-0084" num="0083" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref>, in <figure-callout id="601" label="operation" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">operation</figure-callout> <b>601</b>, a training apparatus obtains a <figure-callout id="601" label="training input" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">training input</figure-callout> <b>601</b> from <figure-callout id="690" label="training data" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">training data</figure-callout> <b>690</b>.</div>
    <div id="p-0085" num="0084" class="description-paragraph">The training apparatus calculates a temporary output from the <figure-callout id="601" label="training input" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">training input</figure-callout> <b>601</b> based on an <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b>. In an example, the <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b> includes a <figure-callout id="611" label="DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">DNN</figure-callout> <b>611</b>. The training apparatus calculates, as the temporary output, temporary <figure-callout id="681" label="class information DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">class information DNN<sub> </sub> </figure-callout> <sub>cls </sub> <b>681</b> and temporary reference <figure-callout id="682" label="point information DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">point information DNN<sub> </sub> </figure-callout> <sub>loc </sub> <b>682</b>.</div>
    <div id="p-0086" num="0085" class="description-paragraph">The training apparatus calculates a <figure-callout id="671" label="class loss L" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">class loss L<sub> </sub> </figure-callout> <sub>cls </sub> <b>671</b> for the temporary <figure-callout id="681" label="class information DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">class information DNN<sub> </sub> </figure-callout> <sub>cls </sub> <b>681</b> as represented by Equation 2.
<br/>
<i>L</i> <sub>cls</sub>(<i>C</i>)=âlog <i>P</i> <sup>C</sup>ââ[Equation 2]
</div>
    <div id="p-0087" num="0086" class="description-paragraph">In Equation 2, L<sub>cls </sub>denotes a class loss with respect to a landmark included in a training image. However, Equation 2 is provided as an example loss function, and thus the class loss is not limited thereto. P<sup>C </sup>denotes a class-based weight, which may be represented by Equation 3, for example.</div>
    <div id="p-0088" num="0087" class="description-paragraph">
      <maths id="MATH-US-00001" num="00001">
        <math overflow="scroll">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <msup>
                    <mi>P</mi>
                    <mi>C</mi>
                  </msup>
                  <mo>=</mo>
                  <mfrac>
                    <mrow>
                      <mi>exp</mi>
                      <mo>â¡</mo>
                      <mrow>
                        <mo>(</mo>
                        <msup>
                          <mi>p</mi>
                          <mi>C</mi>
                        </msup>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mrow>
                      <munder>
                        <mo>â</mo>
                        <mi>c</mi>
                      </munder>
                      <mo>â¢</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"> </mspace>
                      </mstyle>
                      <mo>â¢</mo>
                      <mrow>
                        <mi>exp</mi>
                        <mo>â¡</mo>
                        <mrow>
                          <mo>(</mo>
                          <msup>
                            <mi>p</mi>
                            <mi>c</mi>
                          </msup>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                  </mfrac>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>Equation</mi>
                    <mo>â¢</mo>
                    <mstyle>
                      <mspace width="0.8em" height="0.8ex"> </mspace>
                    </mstyle>
                    <mo>â¢</mo>
                    <mn>3</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </maths>
    </div>
    <div id="p-0089" num="0088" class="description-paragraph">For example, when landmarks are classified into a total of M classes, the temporary <figure-callout id="681" label="class information DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">class information DNN<sub> </sub> </figure-callout> <sub>cls </sub> <b>681</b> may include a probability that a landmark in a training image belongs to a zeroth class through an Mâ1th class. For example, the temporary <figure-callout id="681" label="class information DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">class information DNN<sub> </sub> </figure-callout> <sub>cls </sub> <b>681</b> may be indicated as a class probability vector, for example, [p<sup>c</sup> <sup> <sub2>0</sub2> </sup>, . . . , p<sup>c</sup> <sup> <sub2>M-1</sub2> </sup>], in which c<sub>j </sub>denotes a j<sup>th </sup>class from among the classes and C denotes a GT class provided to a corresponding landmark. Î£<sub>c </sub>exp(p<sup>C</sup>) denotes an exponential sum of probabilities of a landmark belonging to each of the classes. In the temporary <figure-callout id="681" label="class information DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">class information DNN<sub> </sub> </figure-callout> <sub>cls </sub> <b>681</b>, p<sup>C </sup>denotes a probability of a landmark belonging to a GT class C. Thus, a class-based weight P<sup>C </sup>may indicate a quantified value of the probability estimated for the GT class C for the landmark among probabilities estimated for the classes based on the <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b>. The training apparatus may obtain the GT class C from the <figure-callout id="690" label="training data" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">training data</figure-callout> <b>690</b>.</div>
    <div id="p-0090" num="0089" class="description-paragraph">In addition, the training apparatus calculates a <figure-callout id="672" label="localization loss L" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">localization loss L<sub> </sub> </figure-callout> <sub>loc </sub> <b>672</b> for the temporary <figure-callout id="682" label="localization information DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">localization information DNN<sub> </sub> </figure-callout> <sub>loc </sub> <b>682</b> as represented by Equation 4.</div>
    <div id="p-0091" num="0090" class="description-paragraph">
      <maths id="MATH-US-00002" num="00002">
        <math overflow="scroll">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mrow>
                    <msub>
                      <mi>L</mi>
                      <mi>loc</mi>
                    </msub>
                    <mo>â¡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>r</mi>
                        <mo>,</mo>
                        <mover>
                          <mi>r</mi>
                          <mo>^</mo>
                        </mover>
                        <mo>,</mo>
                        <mi>C</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>=</mo>
                  <mrow>
                    <msup>
                      <mi>P</mi>
                      <mi>C</mi>
                    </msup>
                    <mo>â¢</mo>
                    <mrow>
                      <munder>
                        <mo>â</mo>
                        <mrow>
                          <mi>m</mi>
                          <mo>â</mo>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <mi>x</mi>
                              <mo>,</mo>
                              <mi>y</mi>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </munder>
                      <mo>â¢</mo>
                      <mrow>
                        <msub>
                          <mi> <figure-callout id="1" label="smooth L" filenames="US11544507-20230103-D00010.png" state="{{state}}">smooth</figure-callout> </mi> <figure-callout id="1" label="smooth L" filenames="US11544507-20230103-D00010.png" state="{{state}}">
                          <msub>
                            <mi>L</mi> </msub> </figure-callout> <msub> <mi> </mi>
                            <mn>1</mn>
                          </msub>
                        </msub>
                        <mo>â¡</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>m</mi>
                            <mo>-</mo>
                            <mover>
                              <mi>m</mi>
                              <mo>^</mo>
                            </mover>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>Equation</mi>
                    <mo>â¢</mo>
                    <mstyle>
                      <mspace width="0.8em" height="0.8ex"> </mspace>
                    </mstyle>
                    <mo>â¢</mo>
                    <mn>4</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </maths>
    </div>
    <div id="p-0092" num="0091" class="description-paragraph">In Equation 4, L<sub>loc </sub>denotes a localization loss for a landmark in a training image. m denotes GT reference point coordinates of the landmark, and {circumflex over (m)} denotes reference point coordinates estimated based on the <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b>. The training apparatus obtains the GT reference point coordinates from the <figure-callout id="690" label="training data" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">training data</figure-callout> <b>690</b>. In an example, smooth<sub>L</sub> <sub> <sub2>1 </sub2> </sub>denotes a function in which an L1 loss function and an L2 loss function are mixed, and indicates a type of Euclidean distance functions. However, a distance function is not limited to the example function described in the foregoing.</div>
    <div id="p-0093" num="0092" class="description-paragraph">As represented by Equation 4 above, the training apparatus determines a class-dependent localization loss by applying the class-based weight P<sup>C </sup>to a difference, for example, a value of smooth<sub>L</sub> <sub> <sub2>1</sub2> </sub>, between the temporary reference <figure-callout id="682" label="point information DNN" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">point information DNN<sub> </sub> </figure-callout> <sub>loc </sub> <b>682</b> and the GT reference point information. Thus, in an example, the <figure-callout id="672" label="localization loss L" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">localization loss L<sub> </sub> </figure-callout> <sub>loc </sub> <b>672</b> is a loss dependent on the class-based weight P<sup>C</sup>. Based on the <figure-callout id="672" label="localization loss L" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">localization loss L<sub> </sub> </figure-callout> <sub>loc </sub> <b>672</b> as represented by Equation 4, the training apparatus may reduce a magnitude of the <figure-callout id="672" label="localization loss L" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">localization loss L<sub> </sub> </figure-callout> <sub>loc </sub> <b>672</b> in a <figure-callout id="679" label="total loss L" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">total loss L<sub> </sub> </figure-callout> <sub>total </sub> <b>679</b> when accuracy of the <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b> in classification is less than a threshold accuracy, and thereby train first a portion of the <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b> corresponding to the classification. The training apparatus may increase the magnitude of the <figure-callout id="672" label="localization loss L" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">localization loss L<sub> </sub> </figure-callout> <sub>loc </sub> <b>672</b> in the <figure-callout id="679" label="total loss L" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">total loss L<sub> </sub> </figure-callout> <sub>total </sub> <b>679</b> when the accuracy in the classification is greater than or equal to the threshold accuracy, and thus train a portion of the <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b> corresponding to localization. Thus, the training apparatus may first increase a performance of the <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b> in classification and then increase a performance in localization, and train the <figure-callout id="610" label="image recognition model" filenames="US11544507-20230103-D00000.png,US11544507-20230103-D00008.png" state="{{state}}">image recognition model</figure-callout> <b>610</b> to perform class-based recognizing localization.</div>
    <div id="p-0094" num="0093" class="description-paragraph">For example, when a training image is divided into a plurality of subregions, the training apparatus may calculate a partial class loss for each of the subregions based on Equation 2 above, and calculate a total class loss for the training image based on a sum of partial class losses. In addition, the training apparatus may calculate a partial localization loss, and calculate a total localization loss for the training image based on a sum of partial localization losses. The calculation of a loss for each subregion will be described hereinafter with reference to <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref>.</div>
    <div id="p-0095" num="0094" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref> is a diagram illustrating an example of calculating a loss for each subregion during training.</div>
    <div id="p-0096" num="0095" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref>, a training apparatus calculates a <figure-callout id="780" label="temporary output" filenames="US11544507-20230103-D00009.png" state="{{state}}">temporary output</figure-callout> <b>780</b> for each subregion from a <figure-callout id="701" label="training image" filenames="US11544507-20230103-D00009.png" state="{{state}}">training image</figure-callout> <b>701</b> based on an <figure-callout id="710" label="image recognition model" filenames="US11544507-20230103-D00009.png" state="{{state}}">image recognition model</figure-callout> <b>710</b>. For example, the training apparatus may calculate temporary class information and temporary reference point information for each subregion of an input training image. In an example, the training apparatus may calculate, for each subregion of the input training image, a partial class loss between GT class information and the temporary class information calculated for each of the subregion. The training apparatus may determine, to be a class loss, a sum of partial class losses calculated for the subregions, for example, 20 subregions as illustrated in <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref>. However, the determining of a class loss is not limited to the illustrated example.</div>
    <div id="p-0097" num="0096" class="description-paragraph">For example, as illustrated in <figref idrefs="DRAWINGS">FIG. <b>7</b> </figref>, the training apparatus selects subregions corresponding to a <figure-callout id="781" label="GT landmark portion" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT landmark portion</figure-callout> <b>781</b>, for example, four subregions, from among the subregions of the input training image. The training apparatus calculates a partial class loss between GT class information and temporary class information calculated for each of the selected subregions. The training apparatus determines, to be the class loss, a sum of the partial class losses calculated for the selected subregions. In addition, the training apparatus further selects subregions corresponding to a <figure-callout id="782" label="GT background portion" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT background portion</figure-callout> <b>782</b>, for example, four subregions, from among the subregions of the input training image. For a balance in training of classification of classes, the training apparatus may determine the number of the subregions corresponding to the <figure-callout id="781" label="GT landmark portion" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT landmark portion</figure-callout> <b>781</b> and the number of the subregions corresponding to the <figure-callout id="782" label="GT background portion" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT background portion</figure-callout> <b>782</b> to be similar.</div>
    <div id="p-0098" num="0097" class="description-paragraph">In addition, the training apparatus calculates a partial localization loss between the GT reference point information and the temporary reference point information calculated for each of the subregions of the input training image. The training apparatus determines, to be a localization loss, a sum of partial localization losses calculated for the subregions. The training apparatus selects subregions corresponding to the <figure-callout id="781" label="GT landmark portion" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT landmark portion</figure-callout> <b>781</b> from among the subregions of the input training image. The training apparatus calculates a partial localization loss between the calculated reference point information and the GT reference point information for each of the selected subregions. The training apparatus determines, to be a localization loss, a sum of the partial localization losses calculated for the selected subregions. The training apparatus performs the calculation without a calculation of a partial localization loss for the <figure-callout id="782" label="GT background portion" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT background portion</figure-callout> <b>782</b>, and this is because the <figure-callout id="782" label="GT background portion" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT background portion</figure-callout> <b>782</b> does not include a landmark, and thus there is no need to calculate a localization loss.</div>
    <div id="p-0099" num="0098" class="description-paragraph">In an example, the training apparatus transforms <figure-callout id="791" label="map data" filenames="US11544507-20230103-D00009.png" state="{{state}}">map data</figure-callout> <b>791</b> to generate a <figure-callout id="792" label="GT output" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT output</figure-callout> <b>792</b>. In an example, the <figure-callout id="792" label="GT output" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT output</figure-callout> <b>792</b> may include GT class information and GT reference point information. In an example, the training apparatus transforms the <figure-callout id="791" label="map data" filenames="US11544507-20230103-D00009.png" state="{{state}}">map data</figure-callout> <b>791</b> to generate GT reference point information of the <figure-callout id="781" label="GT landmark portion" filenames="US11544507-20230103-D00009.png" state="{{state}}">GT landmark portion</figure-callout> <b>781</b>. For example, the <figure-callout id="791" label="map data" filenames="US11544507-20230103-D00009.png" state="{{state}}">map data</figure-callout> <b>791</b> may include information associated with three-dimensional (3D) coordinates at which a landmark is located. In an example, the training apparatus transforms 3D coordinates of a landmark in the <figure-callout id="701" label="training image" filenames="US11544507-20230103-D00009.png" state="{{state}}">training image</figure-callout> <b>701</b> into 2D coordinates based on a location and a posture, for example, a posture of a vehicle, in the <figure-callout id="791" label="map data" filenames="US11544507-20230103-D00009.png" state="{{state}}">map data</figure-callout> <b>791</b> at which the <figure-callout id="701" label="training image" filenames="US11544507-20230103-D00009.png" state="{{state}}">training image</figure-callout> <b>701</b> is captured, and a viewing angle of an image sensor capturing the <figure-callout id="701" label="training image" filenames="US11544507-20230103-D00009.png" state="{{state}}">training image</figure-callout> <b>701</b>. The training apparatus calculates a partial localization loss based on a difference between temporary reference point coordinates calculated for a subregion and GT reference point coordinates <b>793</b> transformed from the <figure-callout id="791" label="map data" filenames="US11544507-20230103-D00009.png" state="{{state}}">map data</figure-callout> <b>791</b>.</div>
    <div id="p-0100" num="0099" class="description-paragraph">The training apparatus calculates a <figure-callout id="770" label="total loss" filenames="US11544507-20230103-D00009.png" state="{{state}}">total loss</figure-callout> <b>770</b> based on the class loss, which is a sum of the partial class losses for the subregions, and the localization loss, which is a sum of the partial localization losses for the subregions.</div>
    <div id="p-0101" num="0100" class="description-paragraph">Hereinafter, an anchor node used to calculate a partial class loss and a partial localization loss for each subregion will be described with reference to <figref idrefs="DRAWINGS">FIG. <b>8</b> </figref>.</div>
    <div id="p-0102" num="0101" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>8</b> </figref> is a diagram illustrating an example calculating a loss for each anchor node in each subregion during training.</div>
    <div id="p-0103" num="0102" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>8</b> </figref>, a training apparatus calculates temporary class information and temporary reference point information for each of <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b> that is set for each subregion. For example, the training apparatus may set five <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b> for each subregion as illustrated.</div>
    <div id="p-0104" num="0103" class="description-paragraph">For example, the training apparatus calculates temporary localization coordinates <b>871</b> ({circumflex over (x)}, Å·) for each of the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b> in a <figure-callout id="880" label="subregion" filenames="US11544507-20230103-D00010.png" state="{{state}}">subregion</figure-callout> <b>880</b> of a training image. The training apparatus calculates a difference between the temporary localization coordinates <b>871</b> ({circumflex over (x)}, Å·) and GT coordinates (x, y) <b>872</b> of a <figure-callout id="891" label="GT reference point" filenames="US11544507-20230103-D00010.png" state="{{state}}">GT reference point</figure-callout> <b>891</b> included in a <figure-callout id="890" label="subregion" filenames="US11544507-20230103-D00010.png" state="{{state}}">subregion</figure-callout> <b>890</b> corresponding to a reference output. The training apparatus calculates a <figure-callout id="879" label="partial localization loss" filenames="US11544507-20230103-D00010.png" state="{{state}}">partial localization loss</figure-callout> <b>879</b> from a sum of differences between the GT coordinates (x, y) <b>872</b> and the temporary localization coordinates ({circumflex over (x)}, Å·) <b>871</b> calculated for the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b>. In this example, the calculating of the temporary localization coordinates ({circumflex over (x)}, Å·) <b>871</b> is described above for convenience of description. However, examples are not limited to the example described in the foregoing. For example, the training apparatus may calculate, as temporary reference point information, an offset from each of the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b> to a reference point, for each of the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b>. In this example, the offset may indicate an amount of positional change from a pixel location of each of the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b> to the reference point.</div>
    <div id="p-0105" num="0104" class="description-paragraph">In addition, the training apparatus calculates temporary <figure-callout id="861" label="class information Ä" filenames="US11544507-20230103-D00010.png" state="{{state}}">class information Ä</figure-callout> <b>861</b> for each of the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b> in the <figure-callout id="880" label="subregion" filenames="US11544507-20230103-D00010.png" state="{{state}}">subregion</figure-callout> <b>880</b> of the training image. The training apparatus calculates a <figure-callout id="869" label="partial class loss" filenames="US11544507-20230103-D00010.png" state="{{state}}">partial class loss</figure-callout> <b>869</b> from GT <figure-callout id="862" label="class information c" filenames="US11544507-20230103-D00010.png" state="{{state}}">class information c</figure-callout> <b>862</b> included in the <figure-callout id="890" label="subregion" filenames="US11544507-20230103-D00010.png" state="{{state}}">subregion</figure-callout> <b>890</b> corresponding to the reference output and the temporary <figure-callout id="861" label="class information Ä" filenames="US11544507-20230103-D00010.png" state="{{state}}">class information Ä</figure-callout> <b>861</b>.</div>
    <div id="p-0106" num="0105" class="description-paragraph">The training apparatus calculates the <figure-callout id="879" label="partial localization loss" filenames="US11544507-20230103-D00010.png" state="{{state}}">partial localization loss</figure-callout> <b>879</b> and the <figure-callout id="869" label="partial class loss" filenames="US11544507-20230103-D00010.png" state="{{state}}">partial class loss</figure-callout> <b>869</b> for a subregion by adding losses calculated for the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b>. Thus, the training apparatus calculates a total loss by adding partial losses for a plurality of subregions.</div>
    <div id="p-0107" num="0106" class="description-paragraph">Although the calculation of a loss using all the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b> is described above, examples are not limited to the example described in the foregoing. For example, the training apparatus may calculate temporary class information and temporary reference point information for an anchor node having a top confidence level among the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b>, based on a confidence level calculated for each of the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b>. In an example, the training apparatus may select K anchor nodes having top confidence levels in sequential order from among the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b>, and calculate temporary class information and temporary reference point information for each of the selected K anchor nodes. Based on the K anchor nodes selected from a subregion, the training apparatus may calculate a partial loss for the subregion. In this example, K denotes an integer greater than or equal to 1. In an example, the training apparatus may perform the calculation, without a calculation for an anchor node having a confidence level less than a threshold confidence level, based on a confidence level calculated for each of the <figure-callout id="881" label="anchor nodes" filenames="US11544507-20230103-D00010.png" state="{{state}}">anchor nodes</figure-callout> <b>881</b>. That is, the training apparatus may not calculate a loss for the anchor node having the confidence level less than the threshold confidence level among the selected K anchor nodes in the subregion. Thus, the training apparatus may calculate a loss only using an anchor node that satisfies the threshold confidence level among the K anchor nodes having the top confidence levels.</div>
    <div id="p-0108" num="0107" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>9</b> </figref> is a diagram illustrating an example of a training apparatus.</div>
    <div id="p-0109" num="0108" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>9</b> </figref>, a <figure-callout id="900" label="training apparatus" filenames="US11544507-20230103-D00011.png" state="{{state}}">training apparatus</figure-callout> <b>900</b> includes a <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> and a <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b>.</div>
    <div id="p-0110" num="0109" class="description-paragraph">The <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> may calculate a class loss and a class-dependent localization loss from training data based on an <figure-callout id="921" label="image recognition model" filenames="US11544507-20230103-D00011.png" state="{{state}}">image recognition model</figure-callout> <b>921</b>. The <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> may train the <figure-callout id="921" label="image recognition model" filenames="US11544507-20230103-D00011.png" state="{{state}}">image recognition model</figure-callout> <b>921</b> using a total loss calculated based on the class loss and the localization loss. However, operations of the <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> are not limited to what is described in the forgoing, and the <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> may perform at least one method described above with reference to <figref idrefs="DRAWINGS">FIGS. <b>1</b> to <b>8</b> </figref> or an algorithm corresponding thereto.</div>
    <div id="p-0111" num="0110" class="description-paragraph">The <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> refers to a data processing device configured as hardware with a circuitry in a physical structure to execute desired operations. For example, the desired operations may include codes or instructions included in a program. For example, the data processing device configured as hardware may include a microprocessor, a central processing unit (CPU), a processor core, a multicore processor, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA). The <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> executes the program and controls the image recognition model. In an example, the <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> may be a graphics processor unit (GPU), reconfigurable processor, or have any other type of multi- or single-processor configuration. The program code executed by the <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> is stored in the <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b>. Further details regarding the <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> is provided below.</div>
    <div id="p-0112" num="0111" class="description-paragraph">The <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b> may store the <figure-callout id="921" label="image recognition model" filenames="US11544507-20230103-D00011.png" state="{{state}}">image recognition model</figure-callout> <b>921</b>. The <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b> may also store the training data. The <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b> stores a result of evaluating, by the <figure-callout id="921" label="image recognition model" filenames="US11544507-20230103-D00011.png" state="{{state}}">image recognition model</figure-callout> <b>921</b>, of the training data. The training data may include a pair of a training input and a training output. The training input may be a <figure-callout id="991" label="training image" filenames="US11544507-20230103-D00011.png" state="{{state}}">training image</figure-callout> <b>991</b> and the training output may be <figure-callout id="992" label="map data" filenames="US11544507-20230103-D00011.png" state="{{state}}">map data</figure-callout> <b>992</b>, as illustrated. The <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b> stores a variety of information generated during the processing at the <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b>. In addition, a variety of data and programs may be stored in the <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b>. The <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b> may include, for example, a volatile memory or a non-volatile memory. The <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b> may include a mass storage medium, such as a hard disk, to store a variety of data. Further details regarding the <figure-callout id="920" label="memory" filenames="US11544507-20230103-D00011.png" state="{{state}}">memory</figure-callout> <b>920</b> is provided below.</div>
    <div id="p-0113" num="0112" class="description-paragraph">The <figure-callout id="900" label="training apparatus" filenames="US11544507-20230103-D00011.png" state="{{state}}">training apparatus</figure-callout> <b>900</b> may obtain GT reference point information and GT class information by transforming the <figure-callout id="992" label="map data" filenames="US11544507-20230103-D00011.png" state="{{state}}">map data</figure-callout> <b>992</b>. For example, the <figure-callout id="910" label="processor" filenames="US11544507-20230103-D00011.png" state="{{state}}">processor</figure-callout> <b>910</b> may extract a landmark that may be captured by an image sensor from among landmarks included in the <figure-callout id="992" label="map data" filenames="US11544507-20230103-D00011.png" state="{{state}}">map data</figure-callout> <b>992</b>, based on a location, a posture, and a viewing angle of the image sensor capturing the <figure-callout id="991" label="training image" filenames="US11544507-20230103-D00011.png" state="{{state}}">training image</figure-callout> <b>991</b>, and transform 3D coordinates of the extracted landmark into 2D coordinates on an image.</div>
    <div id="p-0114" num="0113" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref> is a diagram illustrating an example of a training method. The operations in <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref> may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref> may be performed in parallel or concurrently. One or more blocks of <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref>, and combinations of the blocks, can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions. In addition to the description of <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref> below, the descriptions of <figref idrefs="DRAWINGS">FIG. <b>1</b>-<b>9</b> </figref> are also applicable to <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref>, and are incorporated herein by reference. Thus, the above description may not be repeated here.</div>
    <div id="p-0115" num="0114" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>10</b> </figref>, in <figure-callout id="1010" label="operation" filenames="US11544507-20230103-D00012.png" state="{{state}}">operation</figure-callout> <b>1010</b>, a training apparatus calculates a class loss and a class-dependent localization loss from training data based on an image recognition model. For example, the training apparatus may calculate temporary class information and temporary reference point information from an input training image based on the image recognition model. The training apparatus may calculate the class loss based on the temporary class information and GT class information. The training apparatus may calculate the localization loss based on the temporary reference point information and GT reference point information.</div>
    <div id="p-0116" num="0115" class="description-paragraph">In an example, the training apparatus may calculate a class-based weight based on the temporary class information. The training apparatus may determine the class-dependent localization loss based on the class-based weight, the temporary reference point information, and the GT reference point information. The determining of a class-dependent localization loss is described above with reference to <figref idrefs="DRAWINGS">FIG. <b>6</b> </figref>, and thus a more detailed and repeated description is omitted here for brevity.</div>
    <div id="p-0117" num="0116" class="description-paragraph">In <figure-callout id="1020" label="operation" filenames="US11544507-20230103-D00012.png" state="{{state}}">operation</figure-callout> <b>1020</b>, the training apparatus trains the image recognition model using a total loss calculated based on the class loss and the localization loss. For example, the training apparatus may update a parameter of the image recognition model such that the total loss is minimized. The training apparatus may repetitively update the parameter of the image recognition model until the total loss converges. Thus, the training apparatus may update the parameter such that the class loss is minimized first before the localization loss is minimized.</div>
    <div id="p-0118" num="0117" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. <b>11</b>A through <b>11</b>C</figref> are diagrams illustrating examples of reducing a loss through training for each loss function.</div>
    <div id="p-0119" num="0118" class="description-paragraph">In <figref idrefs="DRAWINGS">FIGS. <b>11</b>A through <b>11</b>C</figref>, a vertical axis indicates a magnitude of a class loss, and a horizontal axis indicates a magnitude of a localization loss.</div>
    <div id="p-0120" num="0119" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>11</b>A</figref> illustrates an example of how training progresses based on a loss function as represented by Equation 5.
<br/>
<i>L</i> <sub>total</sub>(<i>C,r,{circumflex over (r)}</i>)=<i>L</i> <sub>cls</sub>(<i>C</i>)+<i>L</i> <sub>loc</sub>(<i>r,{circumflex over (r)}</i>)ââ[Equation 5]
</div>
    <div id="p-0121" num="0120" class="description-paragraph">In Equation 5, L<sub>loc</sub>(r, {circumflex over (r)}) denotes a localization loss irrespective of a class. As the loss function is defined as represented by Equation 5, a class loss and a localization loss may linearly and equally decrease with respect to each other.</div>
    <div id="p-0122" num="0121" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>11</b>B</figref> illustrates an example of how training progresses based on a loss function as represented by Equation 6. <figref idrefs="DRAWINGS">FIG. <b>11</b>C</figref> illustrates an example of how training progresses based on a loss function as represented by <figure-callout id="7" label="Equation" filenames="US11544507-20230103-D00006.png" state="{{state}}">Equation</figure-callout> 7.
<br/>
<i>L</i> <sub>total</sub>(<i>C,r,{circumflex over (r)}</i>)=<i>L</i> <sub>cls</sub>(<i>C</i>)+exp(<i>âL</i> <sub>cls</sub>(<i>C</i>))*(<i>L</i> <sub>loc</sub>(<i>r,{circumflex over (r)}</i>)ââ[Equation 6]
<br/>
<i>L</i> <sub>total</sub>(<i>C,r,{circumflex over (r)}</i>)=<i>L</i> <sub>cls</sub>(<i>C</i>)+exp(<i>âL</i> <sub>cls</sub>(<i>C</i>))*(<i>L</i> <sub>loc</sub>(<i>r,{circumflex over (r)}</i>)<sup>2</sup>ââ[Equation 7]
</div>
    <div id="p-0123" num="0122" class="description-paragraph">In <figure-callout id="7" label="Equations" filenames="US11544507-20230103-D00006.png" state="{{state}}">Equations</figure-callout> 6 and 7, a localization loss L<sub>loc</sub>(r, {circumflex over (r)}) and a class loss L<sub>cls</sub>(C) may be associated with each other. Thus, when loss functions are defined as represented by <figure-callout id="7" label="Equations" filenames="US11544507-20230103-D00006.png" state="{{state}}">Equations</figure-callout> 6 and 7 as illustrated in <figref idrefs="DRAWINGS">FIGS. <b>11</b>B and <b>11</b>C</figref>, a class loss may be reduced first and then a localization loss may be reduced.</div>
    <div id="p-0124" num="0123" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. <b>12</b> </figref> is a diagram illustrating an example of an image recognition apparatus.</div>
    <div id="p-0125" num="0124" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. <b>12</b> </figref>, an <figure-callout id="1200" label="image recognition apparatus" filenames="US11544507-20230103-D00016.png" state="{{state}}">image recognition apparatus</figure-callout> <b>1200</b> includes an <figure-callout id="1210" label="image acquirer" filenames="US11544507-20230103-D00016.png" state="{{state}}">image acquirer</figure-callout> <b>1210</b>, a <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b>, a <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b>, a UI or a <figure-callout id="1240" label="display" filenames="US11544507-20230103-D00016.png" state="{{state}}">display</figure-callout> <b>1240</b>, and a <figure-callout id="1250" label="communication interface" filenames="US11544507-20230103-D00016.png" state="{{state}}">communication interface</figure-callout> <b>1250</b>. The <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b>, the <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b>, the <figure-callout id="1210" label="image acquirer" filenames="US11544507-20230103-D00016.png" state="{{state}}">image acquirer</figure-callout> <b>1210</b>, the UI or the <figure-callout id="1240" label="display" filenames="US11544507-20230103-D00016.png" state="{{state}}">display</figure-callout> <b>1240</b>, and the <figure-callout id="1250" label="communication interface" filenames="US11544507-20230103-D00016.png" state="{{state}}">communication interface</figure-callout> <b>1250</b> communicate with each other through a <figure-callout id="1205" label="communication bus" filenames="US11544507-20230103-D00016.png" state="{{state}}">communication bus</figure-callout> <b>1205</b>.</div>
    <div id="p-0126" num="0125" class="description-paragraph">The <figure-callout id="1210" label="image acquirer" filenames="US11544507-20230103-D00016.png" state="{{state}}">image acquirer</figure-callout> <b>1210</b> may obtain an input image. For example, the <figure-callout id="1210" label="image acquirer" filenames="US11544507-20230103-D00016.png" state="{{state}}">image acquirer</figure-callout> <b>1210</b> may include an image sensor configured to capture an image. The image sensor may be embodied by, for example, a color camera, a depth sensor, an infrared sensor, a thermal image sensor, a radio detection and ranging (RADAR) sensor, a light detection and ranging (LiDAR) sensor, and the like. However, examples of the image sensor are not limited to the examples described in the foregoing.</div>
    <div id="p-0127" num="0126" class="description-paragraph">The <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b> may estimate a class of a landmark in the input image and a reference point of the landmark based on an image recognition model. For example, the <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b> may output a class of each of landmarks in the input image, and coordinates of a reference point of each of the landmarks in the input image. Also, the <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b> performs at least one method described above with reference to <figref idrefs="DRAWINGS">FIGS. <b>1</b> to <b>11</b> </figref> or an algorithm corresponding thereto.</div>
    <div id="p-0128" num="0127" class="description-paragraph">The <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b> refers to a data processing device configured as hardware with a circuitry in a physical structure to execute desired operations. For example, the desired operations may include codes or instructions included in a program. For example, the data processing device configured as hardware may include a microprocessor, a central processing unit (CPU), a processor core, a multicore processor, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA). The <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b> executes the program and controls the image recognition model. In an example, the <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b> may be a graphics processor unit (GPU), reconfigurable processor, or have any other type of multi- or single-processor configuration. The program code executed by the <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b> is stored in the <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b>. Further details regarding the processor <b>1030</b> is provided below.</div>
    <div id="p-0129" num="0128" class="description-paragraph">The <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b> may store the image recognition model for which training is completed. For example, the image recognition model may indicate a model having a parameter updated through a training process described above with reference to <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> through <figref idrefs="DRAWINGS">FIG. <b>11</b> </figref>. However, the parameter of the image recognition model may not be set, but the <figure-callout id="1200" label="image recognition apparatus" filenames="US11544507-20230103-D00016.png" state="{{state}}">image recognition apparatus</figure-callout> <b>1200</b> may later update the parameter of the image recognition model in a real-time recognition process.</div>
    <div id="p-0130" num="0129" class="description-paragraph">The <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b> stores a variety of information generated during the processing at the <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b>. In addition, a variety of data and programs may be stored in the <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b>. The <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b> may include, for example, a volatile memory or a non-volatile memory. The <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b> may include a mass storage medium, such as a hard disk, to store a variety of data. Further details regarding the <figure-callout id="1230" label="memory" filenames="US11544507-20230103-D00016.png" state="{{state}}">memory</figure-callout> <b>1230</b> is provided below.</div>
    <div id="p-0131" num="0130" class="description-paragraph">In an example, the <figure-callout id="1200" label="image recognition apparatus" filenames="US11544507-20230103-D00016.png" state="{{state}}">image recognition apparatus</figure-callout> <b>1200</b> may accurately estimate a location of a landmark using an image recognition model trained based on a class-dependent localization loss function as described above with reference to <figref idrefs="DRAWINGS">FIGS. <b>1</b> through <b>11</b> </figref>.</div>
    <div id="p-0132" num="0131" class="description-paragraph">The UI or a <figure-callout id="1240" label="display" filenames="US11544507-20230103-D00016.png" state="{{state}}">display</figure-callout> <b>1240</b> outputs the location of a landmark estimated by the <figure-callout id="1220" label="processor" filenames="US11544507-20230103-D00016.png" state="{{state}}">processor</figure-callout> <b>1220</b>, or displays a virtual object indicating the landmark on the map data based on the accurately estimated location of the landmark. The UI or a <figure-callout id="1240" label="display" filenames="US11544507-20230103-D00016.png" state="{{state}}">display</figure-callout> <b>1240</b> is a physical structure that includes one or more hardware components that provide the ability to render a user interface, render a display, and/or receive user input. However, the UI or a <figure-callout id="1240" label="display" filenames="US11544507-20230103-D00016.png" state="{{state}}">display</figure-callout> <b>1240</b> is not limited to the example described above, and any other displays, such as, for example, smart phone and eye glass display (EGD) that are operatively connected to the <figure-callout id="1200" label="image recognition apparatus" filenames="US11544507-20230103-D00016.png" state="{{state}}">image recognition apparatus</figure-callout> <b>1200</b> may be used without departing from the spirit and scope of the illustrative examples described.</div>
    <div id="p-0133" num="0132" class="description-paragraph">The <figure-callout id="1200" label="image recognition apparatus" filenames="US11544507-20230103-D00016.png" state="{{state}}">image recognition apparatus</figure-callout> <b>1200</b> may perform localization on a vehicle or a mobile terminal. The vehicle described herein refers to any mode of transportation, delivery, or communication such as, for example, an automobile, a truck, a tractor, a scooter, a motorcycle, a cycle, an amphibious vehicle, a snowmobile, a boat, a public transit vehicle, a bus, a monorail, a train, a tram, an autonomous or automated driving vehicle, an intelligent vehicle, a self-driving vehicle, an unmanned aerial vehicle, an electric vehicle (EV), a hybrid vehicle, a smart mobility device, an intelligent vehicle with an advanced driver assistance system (ADAS), or a drone. In an example, the smart mobility device includes mobility devices such as, for example, electric wheels, electric kickboard, and electric bike. In an example, vehicles include motorized and non-motorized vehicles, for example, a vehicle with a power engine (for example, a cultivator or a motorcycle), a bicycle or a handcart.</div>
    <div id="p-0134" num="0133" class="description-paragraph">In addition to the vehicle described herein, <figure-callout id="1200" label="image recognition apparatus" filenames="US11544507-20230103-D00016.png" state="{{state}}">image recognition apparatus</figure-callout> <b>1200</b> may be included in various other devices, such as, for example, a smart phone, a walking assistance device, a wearable device, a security device, a robot, a mobile terminal, and various Internet of Things (IoT) devices.</div>
    <div id="p-0135" num="0134" class="description-paragraph">The <figure-callout id="1200" label="image recognition apparatus" filenames="US11544507-20230103-D00016.png" state="{{state}}">image recognition apparatus</figure-callout> <b>1200</b> may estimate an accurate location of a reference point set for each class of landmarks in an input image, and thus be used to estimate a location and a direction of the vehicle or the mobile terminal. An accurate location of a reference point of a landmark may be stored in map data, for example, a high-definition 3D map, and thus the location and the direction of the vehicle may be estimated based on the accurate location.</div>
    <div id="p-0136" num="0135" class="description-paragraph">The <figure-callout id="1200" label="image recognition apparatus" filenames="US11544507-20230103-D00016.png" state="{{state}}">image recognition apparatus</figure-callout> <b>1200</b>, image recognition apparatus, <figure-callout id="900" label="training apparatus" filenames="US11544507-20230103-D00011.png" state="{{state}}">training apparatus</figure-callout> <b>900</b>, training apparatus, <figure-callout id="1210" label="image acquirer" filenames="US11544507-20230103-D00016.png" state="{{state}}">image acquirer</figure-callout> <b>1210</b>, and other apparatuses, units, modules, devices, and other components described herein with respect to <figref idrefs="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, <b>5</b>, <b>7</b>, <b>9</b>, and <b>12</b> </figref> are implemented by hardware components. Examples of hardware components that may be used to perform the operations described in this application where appropriate include controllers, sensors, generators, drivers, memories, comparators, arithmetic logic units, adders, subtractors, multipliers, dividers, integrators, and any other electronic components configured to perform the operations described in this application. In other examples, one or more of the hardware components that perform the operations described in this application are implemented by computing hardware, for example, by one or more processors or computers. A processor or computer may be implemented by one or more processing elements, such as an array of logic gates, a controller and an arithmetic logic unit, a digital signal processor, a microcomputer, a programmable logic controller, a field-programmable gate array, a programmable logic array, a microprocessor, or any other device or combination of devices that is configured to respond to and execute instructions in a defined manner to achieve a desired result. In one example, a processor or computer includes, or is connected to, one or more memories storing instructions or software that are executed by the processor or computer. Hardware components implemented by a processor or computer may execute instructions or software, such as an operating system (OS) and one or more software applications that run on the OS, to perform the operations described in this application. The hardware components may also access, manipulate, process, create, and store data in response to execution of the instructions or software. For simplicity, the singular term âprocessorâ or âcomputerâ may be used in the description of the examples described in this application, but in other examples multiple processors or computers may be used, or a processor or computer may include multiple processing elements, or multiple types of processing elements, or both. For example, a single hardware component or two or more hardware components may be implemented by a single processor, or two or more processors, or a processor and a controller. One or more hardware components may be implemented by one or more processors, or a processor and a controller, and one or more other hardware components may be implemented by one or more other processors, or another processor and another controller. One or more processors, or a processor and a controller, may implement a single hardware component, or two or more hardware components. A hardware component may have any one or more of different processing configurations, examples of which include a single processor, independent processors, parallel processors, single-instruction single-data (SISD) multiprocessing, single-instruction multiple-data (SIMD) multiprocessing, multiple-instruction single-data (MISD) multiprocessing, and multiple-instruction multiple-data (MIMD) multiprocessing.</div>
    <div id="p-0137" num="0136" class="description-paragraph">The methods illustrated in <figref idrefs="DRAWINGS">FIGS. <b>5</b>, <b>6</b>, <b>7</b>, <b>8</b>, and <b>10</b> </figref> that perform the operations described in this application are performed by computing hardware, for example, by one or more processors or computers, implemented as described above executing instructions or software to perform the operations described in this application that are performed by the methods. For example, a single operation or two or more operations may be performed by a single processor, or two or more processors, or a processor and a controller. One or more operations may be performed by one or more processors, or a processor and a controller, and one or more other operations may be performed by one or more other processors, or another processor and another controller. One or more processors, or a processor and a controller, may perform a single operation, or two or more operations.</div>
    <div id="p-0138" num="0137" class="description-paragraph">Instructions or software to control a processor or computer to implement the hardware components and perform the methods as described above are written as computer programs, code segments, instructions or any combination thereof, for individually or collectively instructing or configuring the processor or computer to operate as a machine or special-purpose computer to perform the operations performed by the hardware components and the methods as described above. In an example, the instructions or software includes at least one of an applet, a dynamic link library (DLL), middleware, firmware, a device driver, an application program storing the method of outputting the state information. In one example, the instructions or software include machine code that is directly executed by the processor or computer, such as machine code produced by a compiler. In another example, the instructions or software include higher-level code that is executed by the processor or computer using an interpreter. Programmers of ordinary skill in the art can readily write the instructions or software based on the block diagrams and the flow charts illustrated in the drawings and the corresponding descriptions in the specification, which disclose algorithms for performing the operations performed by the hardware components and the methods as described above.</div>
    <div id="p-0139" num="0138" class="description-paragraph">The instructions or software to control computing hardware, for example, one or more processors or computers, to implement the hardware components and perform the methods as described above, and any associated data, data files, and data structures, may be recorded, stored, or fixed in or on one or more non-transitory computer-readable storage media. Examples of a non-transitory computer-readable storage medium include read-only memory (ROM), random-access programmable read only memory (PROM), electrically erasable programmable read-only memory (EEPROM), random-access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), flash memory, non-volatile memory, CD-ROMs, CD-Rs, CD+Rs, CD-RWs, CD+RWs, DVD-ROMs, DVD-Rs, DVD+Rs, DVD-RWs, DVD+RWs, DVD-RAMs, BD-ROMs, BD-Rs, BD-R LTHs, BD-REs, blue-ray or optical disk storage, hard disk drive (HDD), solid state drive (SSD), flash memory, card type memory such as multimedia card, secure digital (SD) card, or extreme digital (XD) card, magnetic tapes, floppy disks, magneto-optical data storage devices, optical data storage devices, hard disks, solid-state disks, and any other device that is configured to store the instructions or software and any associated data, data files, and data structures in a non-transitory manner and providing the instructions or software and any associated data, data files, and data structures to a processor or computer so that the processor or computer can execute the instructions. In one example, the instructions or software and any associated data, data files, and data structures are distributed over network-coupled computer systems so that the instructions and software and any associated data, data files, and data structures are stored, accessed, and executed in a distributed fashion by the one or more processors or computers.</div>
    <div id="p-0140" num="0139" class="description-paragraph">While this disclosure includes specific examples, it will be apparent after an understanding of the disclosure of this application that various changes in form and details may be made in these examples without departing from the spirit and scope of the claims and their equivalents. The examples described herein are to be considered in a descriptive sense only, and not for purposes of limitation. Descriptions of features or aspects in each example are to be considered as being applicable to similar features or aspects in other examples. Suitable results may be achieved if the described techniques are performed in a different order, and/or if components in a described system, architecture, device, or circuit are combined in a different manner, and/or replaced or supplemented by other components or their equivalents. Therefore, the scope of the disclosure is defined not by the detailed description, but by the claims and their equivalents, and all variations within the scope of the claims and their equivalents are to be construed as being included in the disclosure.</div>
    
  </div>
  </div>
  </section>

  <section itemprop="claims" itemscope>
    <h2>Claims (<span itemprop="count">14</span>)</h2>
    
    <div itemprop="content" html><div mxw-id="PCLM368003460" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement>
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A method of training an image recognition model, comprising:
<div class="claim-text">dividing an input training image into a plurality of subregions, wherein the divided subregions are non-overlapping;</div>
<div class="claim-text">selecting subregions corresponding to a landmark portion from among the divided subregions of the input training image from training data by excluding at least one subregion corresponding to a background from among the divided subregions of the input training image;</div>
<div class="claim-text">calculating a class loss and a class-dependent localization loss for the selected subregions based on an image recognition model, the calculating of the class-dependent localization loss including transforming coordinates of the landmark portion based on a viewing angle of an image sensor capturing the input training image and a posture associated with the image sensor; and</div>
<div class="claim-text">training the image recognition model using a total loss comprising the class loss and the class-dependent localization loss,</div>
<div class="claim-text">wherein the calculating of the total loss comprises:
<div class="claim-text">after the selecting the subregions corresponding to the landmark portion, setting a plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">calculating temporary reference point information for each of the plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">for each of the selected subregions, calculating a partial localization loss as a sum of differences between ground truth reference point information and the temporary reference point information for each of the plurality of anchor nodes;</div>
<div class="claim-text">determining a sum of partial localization losses calculated for each of the selected subregions of the input training image to be the class-dependent localization loss;</div>
<div class="claim-text">calculating temporary class information for each of the plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">calculating a partial class loss between ground truth class information and the temporary class information calculated for each of the anchor nodes for the each of the selected subregions of the input training image; and</div>
<div class="claim-text">determining a sum of partial class losses calculated for the each of the selected subregions of the input training image to be the class loss.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the calculating of the class loss comprises:
<div class="claim-text">selecting subregions corresponding to a ground truth landmark portion from among the divided subregions of the input training image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the selecting of the subregions comprises:
<div class="claim-text">further selecting a subregion corresponding a ground truth background portion from among the divided subregions of the input training image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the calculating of the partial localization loss comprises:
<div class="claim-text">excluding a subregion with a ground truth background portion from the selected subregions.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the calculating, of the temporary class information and the temporary reference point information for the each of the anchor nodes comprises:
<div class="claim-text">calculating temporary class information and temporary reference point information for an anchor node having a highest confidence level from among confidence levels calculated for each of the anchor nodes.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the calculating of the temporary class information and the temporary reference point information for each of the anchor nodes comprises:
<div class="claim-text">excluding an anchor node having a confidence level less than a threshold from among confidence levels calculated for each of the anchor nodes.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the calculating of the class loss and the class-dependent localization loss comprises:
<div class="claim-text">calculating a class-based weight based on the temporary class information; and</div>
<div class="claim-text">determining the class-dependent localization loss based on the class-based weight, the temporary reference point information, and the ground truth reference point information.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the determining of the class-dependent localization loss comprises:
<div class="claim-text">determining the class-dependent localization loss by applying the class-based weight to a difference between the temporary reference point information and the ground truth reference point information.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the training comprises:
<div class="claim-text">updating a parameter of the image recognition model to minimize the total loss.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the updating of the parameter comprises:
<div class="claim-text">repeating the updating of the parameter of the image recognition model to converge the total loss.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the updating of the parameter comprises:
<div class="claim-text">updating the parameter such that the class loss is minimized before the localization loss is minimized.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. A non-transitory computer-readable storage medium storing instructions that, when executed by a processor, cause the processor to perform the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. A training apparatus comprising:
<div class="claim-text">a memory configured to store an image recognition model; and</div>
<div class="claim-text">a processor configured to:
<div class="claim-text">divide an input training image into a plurality of subregions, wherein the divided subregions are non-overlapping;</div>
<div class="claim-text">select subregions corresponding to a landmark portion from among the divided subregions of the input training image from training data by excluding at least one subregion corresponding to a background from among the divided subregions of the input training image;</div>
<div class="claim-text">calculate a class loss and a class-dependent localization loss for the selected subregions based on the image recognition model, the calculating of the class-dependent localization loss including transforming coordinates of the landmark portion based on a viewing angle of an image sensor capturing the input training image and a posture associated with the image sensor; and</div>
<div class="claim-text">train the image recognition model using a total loss comprising the class loss and the class-dependent localization loss,</div>
</div>
<div class="claim-text">wherein the processor is further configured to:
<div class="claim-text">after the selecting the subregions corresponding to the landmark portion, set a plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">calculate temporary reference point information for each of the plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">for each of the selected subregions, calculate a partial localization loss as a sum of differences between ground truth reference point information and the temporary reference point information for each of the plurality of anchor nodes;</div>
<div class="claim-text">determine a sum of partial localization losses calculated for each of the selected subregions of the input training image to be the class-dependent localization loss;</div>
<div class="claim-text">calculate temporary class information for each of the plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">calculate a partial class loss between ground truth class information and temporary class information calculated for each of the anchor nodes for the each of the selected subregions of the input training image; and</div>
<div class="claim-text">determine a sum of partial class losses calculated for the each of the selected subregions of the input training image to be the class loss.</div>
</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. An image recognition method comprising:
<div class="claim-text">obtaining an input image; and</div>
<div class="claim-text">estimating, from the input image, a class of a landmark in the input image and a reference point of the landmark, based on an image recognition model,</div>
<div class="claim-text">wherein the image recognition model is trained using a total loss comprising class loss and class-dependent localization loss being calculated by:
<div class="claim-text">dividing an input training image into a plurality of subregions, wherein the divided subregions are non-overlapping,</div>
<div class="claim-text">selecting subregions corresponding to a landmark portion from among the divided subregions of the input training image from training data and excluding at least one subregion corresponding to a background from among the divided subregions of the input training image,</div>
<div class="claim-text">calculating the class-dependent localization loss for the selected subregions based on the image recognition model by transforming coordinates of the landmark portion based on a viewing angle of an image sensor capturing the input training image and a posture associated with the image sensor, and</div>
</div>
<div class="claim-text">wherein the total loss is calculated by:
<div class="claim-text">after the selecting the subregions corresponding to the landmark portion, setting a plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">calculating temporary reference point information for each of the plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">for each of the selected subregions, calculating a partial localization loss as a sum of differences between ground truth reference point information and the temporary reference point information for each of the plurality of anchor nodes;</div>
<div class="claim-text">determining a sum of partial localization losses calculated for each of the selected subregions of the input training image to be the class-dependent localization loss;</div>
<div class="claim-text">calculating temporary class information for each of the plurality of anchor nodes for each of the selected subregions;</div>
<div class="claim-text">calculating a partial class loss between ground truth class information and the temporary class information calculated for each of the anchor nodes for the each of the selected subregions of the input training image; and</div>
<div class="claim-text">determining a sum of partial class losses calculated for the each of the selected subregions of the input training image to be the class loss.</div>
</div>
</div>
    </div>
  </div> </div>
  </div>
  </section>

  <section itemprop="application" itemscope>

    <section itemprop="metadata" itemscope>
      <span itemprop="applicationNumber">US16/367,358</span>
      <span itemprop="priorityDate">2018-10-17</span>
      <span itemprop="filingDate">2019-03-28</span>
      <span itemprop="title">Method and apparatus to train image recognition model, and image recognition method and apparatus 
       </span>
      <span itemprop="ifiStatus">Active</span>
      <span itemprop="ifiExpiration">2040-01-28</span>
      <a href="/patent/US11544507B2/en">
        <span itemprop="representativePublication">US11544507B2</span>
        (<span itemprop="primaryLanguage">en</span>)
      </a>
    </section>

    

    <h2>Applications Claiming Priority (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">KR10-2018-0123487</span>
            
          </td>
          <td itemprop="priorityDate"></td>
          <td itemprop="filingDate">2018-10-17</td>
          <td itemprop="title"></td>
        </tr>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">KR1020180123487A</span>
            <a href="/patent/KR20200043005A/en">
              <span itemprop="representativePublication">KR20200043005A</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-10-17</td>
          <td itemprop="filingDate">2018-10-17</td>
          <td itemprop="title">Method and device to train image recognition model and to recognize image 
       </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Publications (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Publication Number</th>
          <th>Publication Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="pubs" itemscope repeat>
          <td>
            <span itemprop="publicationNumber">US20200125899A1</span>
            
            <a href="/patent/US20200125899A1/en">
              US20200125899A1
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-23</td>
        </tr>
        <tr itemprop="pubs" itemscope repeat>
          <td>
            <span itemprop="publicationNumber">US11544507B2</span>
            
            <span itemprop="thisPatent">true</span>
            <a href="/patent/US11544507B2/en">
              US11544507B2
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-01-03</td>
        </tr>
      </tbody>
    </table>

  </section>

  <section itemprop="family" itemscope>
    <h1>Family</h1>
    <h2>ID=66751948</h2>

    <h2>Family Applications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Title</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="applications" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US16/367,358</span>
            <span itemprop="ifiStatus">Active</span>
            <span itemprop="ifiExpiration">2040-01-28</span>
            <a href="/patent/US11544507B2/en">
              <span itemprop="representativePublication">US11544507B2</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-10-17</td>
          <td itemprop="filingDate">2019-03-28</td>
          <td itemprop="title">Method and apparatus to train image recognition model, and image recognition method and apparatus 
       </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Country Status (5)</h2>
    <table>
      <thead>
        <tr>
          <th>Country</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">US</span>
            (<span itemprop="num">1</span>)
            <meta itemprop="thisCountry" content="true">
          </td>
          <td>
            <a href="/patent/US11544507B2/en">
              <span itemprop="representativePublication">US11544507B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">EP</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/EP3640846B1/en">
              <span itemprop="representativePublication">EP3640846B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">JP</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/JP7421889B2/en">
              <span itemprop="representativePublication">JP7421889B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">KR</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/KR20200043005A/en">
              <span itemprop="representativePublication">KR20200043005A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">CN</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/CN111062405A/en">
              <span itemprop="representativePublication">CN111062405A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
      </tbody>
    </table>

    

    <h2>Families Citing this family (6)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/US11354820B2/en">
              <span itemprop="publicationNumber">US11354820B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-11-17</td>
          <td itemprop="publicationDate">2022-06-07</td>
          <td><span itemprop="assigneeOriginal">Uatc, Llc</span></td>
          <td itemprop="title">Image based localization system 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/KR20210141123A/en">
              <span itemprop="publicationNumber">KR20210141123A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-05-15</td>
          <td itemprop="publicationDate">2021-11-23</td>
          <td><span itemprop="assigneeOriginal">íêµ­ì ìíµì ì°êµ¬ì</span></td>
          <td itemprop="title">Method for partial training of artificial intelligence and apparatus for the same 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN112987765B/en">
              <span itemprop="publicationNumber">CN112987765B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-03-05</td>
          <td itemprop="publicationDate">2022-03-15</td>
          <td><span itemprop="assigneeOriginal">åäº¬èªç©ºèªå¤©å¤§å­¦</span></td>
          <td itemprop="title">Precise autonomous take-off and landing method of unmanned aerial vehicle/boat simulating attention distribution of prey birds 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN113359135B/en">
              <span itemprop="publicationNumber">CN113359135B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-07-07</td>
          <td itemprop="publicationDate">2023-08-22</td>
          <td><span itemprop="assigneeOriginal">ä¸­å½äººæ°è§£æ¾åç©ºåå·¥ç¨å¤§å­¦</span></td>
          <td itemprop="title">Training method, application method, device and medium for imaging and recognition model 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN115690526A/en">
              <span itemprop="publicationNumber">CN115690526A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-07-29</td>
          <td itemprop="publicationDate">2023-02-03</td>
          <td><span itemprop="assigneeOriginal">ä¸æçµå­æ ªå¼ä¼ç¤¾</span></td>
          <td itemprop="title">Apparatus and method with object detection 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN117037895B/en">
              <span itemprop="publicationNumber">CN117037895B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2023-10-09</td>
          <td itemprop="publicationDate">2024-02-20</td>
          <td><span itemprop="assigneeOriginal">èå·åèæºè½ç§ææéå¬å¸</span></td>
          <td itemprop="title">Model training method and device, electronic equipment and storage medium 
       </td>
        </tr>
      </tbody>
    </table>

    <h2>Citations (12)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20090080780A1/en">
              <span itemprop="publicationNumber">US20090080780A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2005-07-19</td>
          <td itemprop="publicationDate">2009-03-26</td>
          <td>
            <span itemprop="assigneeOriginal">Nec Corporation</span>
          </td>
          <td itemprop="title">Articulated Object Position and Posture Estimation Device, Method and Program 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20100232727A1/en">
              <span itemprop="publicationNumber">US20100232727A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2007-05-22</td>
          <td itemprop="publicationDate">2010-09-16</td>
          <td>
            <span itemprop="assigneeOriginal">Metaio Gmbh</span>
          </td>
          <td itemprop="title">Camera pose estimation apparatus and method for augmented reality imaging 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20140343842A1/en">
              <span itemprop="publicationNumber">US20140343842A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2013-05-17</td>
          <td itemprop="publicationDate">2014-11-20</td>
          <td>
            <span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span>
          </td>
          <td itemprop="title">Localization using road markings 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/KR20150103979A/en">
              <span itemprop="publicationNumber">KR20150103979A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2014-03-04</td>
          <td itemprop="publicationDate">2015-09-14</td>
          <td>
            <span itemprop="assigneeOriginal">êµ­ë°©ê³¼íì°êµ¬ì</span>
          </td>
          <td itemprop="title">System and method for estimating position of autonomous vehicle using position information of geographic feature 
     </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20150371397A1/en">
              <span itemprop="publicationNumber">US20150371397A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2014-06-20</td>
          <td itemprop="publicationDate">2015-12-24</td>
          <td>
            <span itemprop="assigneeOriginal">Nec Laboratories America, Inc.</span>
          </td>
          <td itemprop="title">Object Detection with Regionlets Re-localization 
     </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20160283864A1/en">
              <span itemprop="publicationNumber">US20160283864A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-03-27</td>
          <td itemprop="publicationDate">2016-09-29</td>
          <td>
            <span itemprop="assigneeOriginal">Qualcomm Incorporated</span>
          </td>
          <td itemprop="title">Sequential image sampling and storage of fine-tuned features 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20170147905A1/en">
              <span itemprop="publicationNumber">US20170147905A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-11-25</td>
          <td itemprop="publicationDate">2017-05-25</td>
          <td>
            <span itemprop="assigneeOriginal">Baidu Usa Llc</span>
          </td>
          <td itemprop="title">Systems and methods for end-to-end object detection 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US9863775B2/en">
              <span itemprop="publicationNumber">US9863775B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2014-04-11</td>
          <td itemprop="publicationDate">2018-01-09</td>
          <td>
            <span itemprop="assigneeOriginal">Nissan North America, Inc.</span>
          </td>
          <td itemprop="title">Vehicle localization system 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/KR20180069501A/en">
              <span itemprop="publicationNumber">KR20180069501A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2016-12-15</td>
          <td itemprop="publicationDate">2018-06-25</td>
          <td>
            <span itemprop="assigneeOriginal">íëìëì°¨ì£¼ìíì¬</span>
          </td>
          <td itemprop="title">Apparatus for estimating location of vehicle, method for thereof, apparatus for constructing map thereof, and method for constructing map 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20180190046A1/en">
              <span itemprop="publicationNumber">US20180190046A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-11-04</td>
          <td itemprop="publicationDate">2018-07-05</td>
          <td>
            <span itemprop="assigneeOriginal">Zoox, Inc.</span>
          </td>
          <td itemprop="title">Calibration for autonomous vehicle operation 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20190377949A1/en">
              <span itemprop="publicationNumber">US20190377949A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-06-08</td>
          <td itemprop="publicationDate">2019-12-12</td>
          <td>
            <span itemprop="assigneeOriginal">Guangdong Oppo Mobile Telecommunications Corp., Ltd.</span>
          </td>
          <td itemprop="title">Image Processing Method, Electronic Device and Computer Readable Storage Medium 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20200117991A1/en">
              <span itemprop="publicationNumber">US20200117991A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-10-12</td>
          <td itemprop="publicationDate">2020-04-16</td>
          <td>
            <span itemprop="assigneeOriginal">Fujitsu Limited</span>
          </td>
          <td itemprop="title">Learning apparatus, detecting apparatus, learning method, and detecting method 
     </td>
        </tr>
      </tbody>
    </table>

    <h2>Family Cites Families (6)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/US10871536B2/en">
              <span itemprop="publicationNumber">US10871536B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-11-29</td>
          <td itemprop="publicationDate">2020-12-22</td>
          <td><span itemprop="assigneeOriginal">Arterys Inc.</span></td>
          <td itemprop="title">Automated cardiac volume segmentation 
       </td>
        </tr>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/JP6829575B2/en">
              <span itemprop="publicationNumber">JP6829575B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2016-10-03</td>
          <td itemprop="publicationDate">2021-02-10</td>
          <td><span itemprop="assigneeOriginal">ã°ã­ã¼ãªã¼æ ªå¼ä¼ç¤¾</span></td>
          <td itemprop="title">
  Image processing equipment, image processing system and image processing method
 
     </td>
        </tr>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/KR102308871B1/en">
              <span itemprop="publicationNumber">KR102308871B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2016-11-02</td>
          <td itemprop="publicationDate">2021-10-05</td>
          <td><span itemprop="assigneeOriginal">ì¼ì±ì ìì£¼ìíì¬</span></td>
          <td itemprop="title">Device and method to train and recognize object based on attribute of object 
       </td>
        </tr>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN106934355A/en">
              <span itemprop="publicationNumber">CN106934355A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-02-28</td>
          <td itemprop="publicationDate">2017-07-07</td>
          <td><span itemprop="assigneeOriginal">è¥¿äº¤å©ç©æµ¦å¤§å­¦</span></td>
          <td itemprop="title">In-car hand detection method based on depth convolutional neural networks 
       </td>
        </tr>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/US10657376B2/en">
              <span itemprop="publicationNumber">US10657376B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2017-03-17</td>
          <td itemprop="publicationDate">2020-05-19</td>
          <td><span itemprop="assigneeOriginal">Magic Leap, Inc.</span></td>
          <td itemprop="title">Room layout estimation methods and techniques 
       </td>
        </tr>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN108009526A/en">
              <span itemprop="publicationNumber">CN108009526A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-12-25</td>
          <td itemprop="publicationDate">2018-05-08</td>
          <td><span itemprop="assigneeOriginal">è¥¿åå·¥ä¸å¤§å­¦</span></td>
          <td itemprop="title">A kind of vehicle identification and detection method based on convolutional neural networks 
       </td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2018</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2018-10-17</span>
            <span itemprop="countryCode">KR</span>
            <span itemprop="applicationNumber">KR1020180123487A</span>
            <a href="/patent/KR20200043005A/en"><span itemprop="documentId">patent/KR20200043005A/en</span></a>
            <span itemprop="legalStatusCat">not_active</span>
            <span itemprop="legalStatus">Application Discontinuation</span>
            
          </li>
        </ul>
      </li>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2019</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-03-28</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/367,358</span>
            <a href="/patent/US11544507B2/en"><span itemprop="documentId">patent/US11544507B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            <span itemprop="thisApp" content="true" bool></span>
          </li>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-06-04</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP19178016.2A</span>
            <a href="/patent/EP3640846B1/en"><span itemprop="documentId">patent/EP3640846B1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-06-28</span>
            <span itemprop="countryCode">CN</span>
            <span itemprop="applicationNumber">CN201910583821.5A</span>
            <a href="/patent/CN111062405A/en"><span itemprop="documentId">patent/CN111062405A/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-09-20</span>
            <span itemprop="countryCode">JP</span>
            <span itemprop="applicationNumber">JP2019171731A</span>
            <a href="/patent/JP7421889B2/en"><span itemprop="documentId">patent/JP7421889B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
          </li>
        </ul>
      </li>
    </ul>

    </section>

  <section>
    <h2>Patent Citations (12)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20090080780A1/en">
              <span itemprop="publicationNumber">US20090080780A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2005-07-19</td>
          <td itemprop="publicationDate">2009-03-26</td>
          <td><span itemprop="assigneeOriginal">Nec Corporation</span></td>
          <td itemprop="title">Articulated Object Position and Posture Estimation Device, Method and Program 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20100232727A1/en">
              <span itemprop="publicationNumber">US20100232727A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2007-05-22</td>
          <td itemprop="publicationDate">2010-09-16</td>
          <td><span itemprop="assigneeOriginal">Metaio Gmbh</span></td>
          <td itemprop="title">Camera pose estimation apparatus and method for augmented reality imaging 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20140343842A1/en">
              <span itemprop="publicationNumber">US20140343842A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2013-05-17</td>
          <td itemprop="publicationDate">2014-11-20</td>
          <td><span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span></td>
          <td itemprop="title">Localization using road markings 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/KR20150103979A/en">
              <span itemprop="publicationNumber">KR20150103979A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2014-03-04</td>
          <td itemprop="publicationDate">2015-09-14</td>
          <td><span itemprop="assigneeOriginal">êµ­ë°©ê³¼íì°êµ¬ì</span></td>
          <td itemprop="title">System and method for estimating position of autonomous vehicle using position information of geographic feature 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US9863775B2/en">
              <span itemprop="publicationNumber">US9863775B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2014-04-11</td>
          <td itemprop="publicationDate">2018-01-09</td>
          <td><span itemprop="assigneeOriginal">Nissan North America, Inc.</span></td>
          <td itemprop="title">Vehicle localization system 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20150371397A1/en">
              <span itemprop="publicationNumber">US20150371397A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2014-06-20</td>
          <td itemprop="publicationDate">2015-12-24</td>
          <td><span itemprop="assigneeOriginal">Nec Laboratories America, Inc.</span></td>
          <td itemprop="title">Object Detection with Regionlets Re-localization 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20160283864A1/en">
              <span itemprop="publicationNumber">US20160283864A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-03-27</td>
          <td itemprop="publicationDate">2016-09-29</td>
          <td><span itemprop="assigneeOriginal">Qualcomm Incorporated</span></td>
          <td itemprop="title">Sequential image sampling and storage of fine-tuned features 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20180190046A1/en">
              <span itemprop="publicationNumber">US20180190046A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-11-04</td>
          <td itemprop="publicationDate">2018-07-05</td>
          <td><span itemprop="assigneeOriginal">Zoox, Inc.</span></td>
          <td itemprop="title">Calibration for autonomous vehicle operation 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20170147905A1/en">
              <span itemprop="publicationNumber">US20170147905A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-11-25</td>
          <td itemprop="publicationDate">2017-05-25</td>
          <td><span itemprop="assigneeOriginal">Baidu Usa Llc</span></td>
          <td itemprop="title">Systems and methods for end-to-end object detection 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/KR20180069501A/en">
              <span itemprop="publicationNumber">KR20180069501A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2016-12-15</td>
          <td itemprop="publicationDate">2018-06-25</td>
          <td><span itemprop="assigneeOriginal">íëìëì°¨ì£¼ìíì¬</span></td>
          <td itemprop="title">Apparatus for estimating location of vehicle, method for thereof, apparatus for constructing map thereof, and method for constructing map 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20190377949A1/en">
              <span itemprop="publicationNumber">US20190377949A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-06-08</td>
          <td itemprop="publicationDate">2019-12-12</td>
          <td><span itemprop="assigneeOriginal">Guangdong Oppo Mobile Telecommunications Corp., Ltd.</span></td>
          <td itemprop="title">Image Processing Method, Electronic Device and Computer Readable Storage Medium 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20200117991A1/en">
              <span itemprop="publicationNumber">US20200117991A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-10-12</td>
          <td itemprop="publicationDate">2020-04-16</td>
          <td><span itemprop="assigneeOriginal">Fujitsu Limited</span></td>
          <td itemprop="title">Learning apparatus, detecting apparatus, learning method, and detecting method 
     </td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Non-Patent Citations (15)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Cao, Yuhang, et al. "<a href='http://scholar.google.com/scholar?q="Prime+sample+attention+in+object+detection."'>Prime sample attention in object detection.</a>" arXiv preprint arXiv:1904.04821, 2019 (pp. 1-10).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Extended European Search Report dated Dec. 6, 2019 in counterpart European Application No. 19178016.2 ( 8 pages in English).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Girshick, Ross, "<a href='http://scholar.google.com/scholar?q="Fast+R-CNN"'>Fast R-CNN</a>", 2015 IEEE International Conference on Computer Vision (ICCV), 2015 (pp. 1440-1448).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">He, Wenhao et al., "<a href='http://scholar.google.com/scholar?q="Deep+Direct+Regression+for+Multi-oriented+Scene+Text+Detection"'>Deep Direct Regression for Multi-oriented Scene Text Detection</a>", 2017 IEEE International Conference on Computer Vision (ICCV), 2017 (pp. 1-9).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Hwang, Sangheum et al., "<a href='http://scholar.google.com/scholar?q="Self-Transfer+Learning+for+Fully+Weakly+Supervised+Object+Localization"'>Self-Transfer Learning for Fully Weakly Supervised Object Localization</a>", arXiv:1602.01625, Feb. 4, 2016 (pp. 1-9).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Jung, Heechul, et al. "<a href='http://scholar.google.com/scholar?q="ResNet-based+vehicle+classification+and+localization+in+traffic+surveillance+systems."'>ResNet-based vehicle classification and localization in traffic surveillance systems.</a>" Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017. (Year: 2017).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Lin, Di et al., "<a href='http://scholar.google.com/scholar?q="Deep+LAC%3A+Deep+Localization%2C+Alignment+and+Classification+for+Fine-grained+Recognition"'>Deep LAC: Deep Localization, Alignment and Classification for Fine-grained Recognition</a>", 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015 (pp. 1666-1674).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Lin, Tsung-Yi et al., "<a href='http://scholar.google.com/scholar?q="Focal+Loss+for+Dense+Object+Detection"'>Focal Loss for Dense Object Detection</a>", 2017 IEEE International Conference on Computer Vision (ICCV), Feb. 7, 2018 (pp. 1-10).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Liu, Wei, et al., "<a href='http://scholar.google.com/scholar?q="SSD%3A+Single+Shot+MultiBox+Detector"'>SSD: Single Shot MultiBox Detector</a>", Proceedings of the European Conference on Computer Vision, Dec. 29, 2016 (17 pages in English).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Ma, Jianqi, et al. "<a href='http://scholar.google.com/scholar?q="Arbitrary-Oriented+Scene+Text+Detection+via+Rotation+Proposals."'>Arbitrary-Oriented Scene Text Detection via Rotation Proposals.</a>" (Mar. 2018). (Year: 2018).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Redmon, Joseph, et al. "<a href='http://scholar.google.com/scholar?q="You+only+look+once%3A+Unified%2C+real-time+object+detection."'>You only look once: Unified, real-time object detection.</a>" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. (Year: 2016).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Ren, Shaoqing, et al. "<a href='http://scholar.google.com/scholar?q="Faster+R-CNN%3A+Towards+Real-Time+Object+Detection+with+Region+Proposal+Networks"'>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>", IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, Issue 6, 2017 (pp. 1-14).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Shou, Zheng et al., "<a href='http://scholar.google.com/scholar?q="Temporal+Action+Localization+in+Untrimmed+Videos+via+Multi-stage+CNNs"'>Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs</a>", 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016 (pp. 1049-1058).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Thi, Tuan Hue, et al., "<a href='http://scholar.google.com/scholar?q="Structured+learning+of+local+features+for+human+action+classification+and+localization"'>Structured learning of local features for human action classification and localization</a>", Image and Vision Computing, vol. 30, Issue 1, Jan. 2012 (pp. 1-14).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Xu, Hongyu, et al. "<a href='http://scholar.google.com/scholar?q="Deep+regionlets+for+object+detection."'>Deep regionlets for object detection.</a>" Proceedings of the European Conference on Computer Vision (ECCV). 2018. (Year: 2018).</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr>
      </tbody>
    </table>
  </section>

  

  <section>
    <h2>Also Published As</h2>
    <table>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Publication date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/JP2020064619A/en">
              <span itemprop="publicationNumber">JP2020064619A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-23</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/EP3640846B1/en">
              <span itemprop="publicationNumber">EP3640846B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2024-02-21</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/JP7421889B2/en">
              <span itemprop="publicationNumber">JP7421889B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2024-01-25</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/CN111062405A/en">
              <span itemprop="publicationNumber">CN111062405A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-24</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/KR20200043005A/en">
              <span itemprop="publicationNumber">KR20200043005A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-27</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/EP3640846A1/en">
              <span itemprop="publicationNumber">EP3640846A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-22</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/US20200125899A1/en">
              <span itemprop="publicationNumber">US20200125899A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-23</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Similar Documents</h2>
    <table>
      <thead>
        <tr>
          <th>Publication</th>
          <th>Publication Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11544507B2/en">
                <span itemprop="publicationNumber">US11544507B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-01-03">2023-01-03</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus to train image recognition model, and image recognition method and apparatus 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230127115A1/en">
                <span itemprop="publicationNumber">US20230127115A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-04-27">2023-04-27</time>
            
            
          </td>
          <td itemprop="title">Three-Dimensional Object Detection 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10599930B2/en">
                <span itemprop="publicationNumber">US10599930B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-03-24">2020-03-24</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus of detecting object of interest 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11842447B2/en">
                <span itemprop="publicationNumber">US11842447B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-12-12">2023-12-12</time>
            
            
          </td>
          <td itemprop="title">Localization method and apparatus of displaying virtual object in augmented reality 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10579058B2/en">
                <span itemprop="publicationNumber">US10579058B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-03-03">2020-03-03</time>
            
            
          </td>
          <td itemprop="title">Apparatus and method for generating training data to train neural network determining information associated with road included in image 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11482014B2/en">
                <span itemprop="publicationNumber">US11482014B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-10-25">2022-10-25</time>
            
            
          </td>
          <td itemprop="title">3D auto-labeling with structural and physical constraints 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/EP3441909B1/en">
                <span itemprop="publicationNumber">EP3441909B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-11-08">2023-11-08</time>
            
            
          </td>
          <td itemprop="title">Lane detection method and apparatus 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20210174537A1/en">
                <span itemprop="publicationNumber">US20210174537A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-06-10">2021-06-10</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for detecting target object in image 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11768292B2/en">
                <span itemprop="publicationNumber">US11768292B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-09-26">2023-09-26</time>
            
            
          </td>
          <td itemprop="title">Three-dimensional object detection 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20190371052A1/en">
                <span itemprop="publicationNumber">US20190371052A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2019-12-05">2019-12-05</time>
            
            
          </td>
          <td itemprop="title">Inferring locations of 3d objects in a spatial environment 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11176719B2/en">
                <span itemprop="publicationNumber">US11176719B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-11-16">2021-11-16</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for localization based on images and map data 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN111860227B/en">
                <span itemprop="publicationNumber">CN111860227B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2024-03-08">2024-03-08</time>
            
            
          </td>
          <td itemprop="title">Method, apparatus and computer storage medium for training trajectory planning model 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11453337B2/en">
                <span itemprop="publicationNumber">US11453337B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-09-27">2022-09-27</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus with three-dimensional object display 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN114127810A/en">
                <span itemprop="publicationNumber">CN114127810A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-03-01">2022-03-01</time>
            
            
          </td>
          <td itemprop="title">Vehicle autonomous level function 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230311932A1/en">
                <span itemprop="publicationNumber">US20230311932A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-10-05">2023-10-05</time>
            
            
          </td>
          <td itemprop="title">Merging object and background radar data for autonomous driving simulations 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11803977B2/en">
                <span itemprop="publicationNumber">US11803977B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-10-31">2023-10-31</time>
            
            
          </td>
          <td itemprop="title">LIDAR point cloud alignment validator in HD mapping 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230311930A1/en">
                <span itemprop="publicationNumber">US20230311930A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-10-05">2023-10-05</time>
            
            
          </td>
          <td itemprop="title">Capturing and simulating radar data for autonomous driving systems 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230031919A1/en">
                <span itemprop="publicationNumber">US20230031919A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-02-02">2023-02-02</time>
            
            
          </td>
          <td itemprop="title">Image segmentation method and device 
     </td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Legal Events</h2>
    <table>
      <thead>
        <tr>
          <th>Date</th>
          <th>Code</th>
          <th>Title</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2019-03-28">2019-03-28</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">SAMSUNG ELECTRONCIS CO., LTD., KOREA, REPUBLIC OF</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LEE, WONHEE;SON, MINJUNG;JUNG, KYUNGBOO;AND OTHERS;REEL/FRAME:048722/0554</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Effective date</strong>:
              <span itemprop="value">20190326</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2019-03-28">2019-03-28</time></td>
          <td itemprop="code">FEPP</td>
          <td itemprop="title">Fee payment procedure</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-03-26">2021-03-26</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NON FINAL ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-07-14">2021-07-14</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">FINAL REJECTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-09-21">2021-09-21</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-09-29">2021-09-29</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ADVISORY ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-10-15">2021-10-15</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">DOCKETED NEW CASE - READY FOR EXAMINATION</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-11-22">2021-11-22</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NON FINAL ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-03-02">2022-03-02</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-05-04">2022-05-04</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">FINAL REJECTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-07-15">2022-07-15</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ADVISORY ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-08-04">2022-08-04</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">DOCKETED NEW CASE - READY FOR EXAMINATION</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-09-06">2022-09-06</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-12-01">2022-12-01</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-12-14">2022-12-14</time></td>
          <td itemprop="code">STCF</td>
          <td itemprop="title">Information on status: patent grant</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">PATENTED CASE</span>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </section>

</article>

    </search-app>
    <script type="text/javascript" src="//www.gstatic.com/feedback/api.js"></script>
    <script type="text/javascript" src="//www.gstatic.com/feedback/js/help/prod/service/lazy.min.js"></script>
    <script type="text/javascript">
      if (window.help && window.help.service) {
        helpApi = window.help.service.Lazy.create(0, {apiKey: 'AIzaSyDTEI_0tLX4varJ7bwK8aT-eOI5qr3BmyI', locale: 'en-US'});
        window.requestedSurveys = new Set();
        window.requestSurvey = function(triggerId) {
          if (window.requestedSurveys.has(triggerId)) {
            return;
          }
          window.requestedSurveys.add(triggerId);
          helpApi.requestSurvey({
            triggerId: triggerId,
            enableTestingMode: false,
            callback: (requestSurveyCallbackParam) => {
              if (!requestSurveyCallbackParam.surveyData) {
                return;
              }
              helpApi.presentSurvey({
                productData: {
                  productVersion: window.version,
                  customData: {
                    "experiments": "72459301,72474719",
                  },
                },
                surveyData: requestSurveyCallbackParam.surveyData,
                colorScheme: 1,
                customZIndex: 10000,
              });
            }
          });
        };

        window.requestSurvey('YXTwAsvoW0kedxbuTdH0RArc9VhT');
      }
    </script>
    <script src="/sw/null_loader.js"></script>
  </body>
</html>
