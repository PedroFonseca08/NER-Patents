<!DOCTYPE html>
<html lang="en">
  <head>
    <title>US20200041276A1 - End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
      - Google Patents</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <meta name="referrer" content="origin-when-crossorigin">
    <link rel="canonical" href="https://patents.google.com/patent/US20200041276A1/en">
    <meta name="description" content="
     The disclosure relates to systems, methods, and devices for simultaneous localization and mapping of a robot in an environment utilizing a variational autoencoder generative adversarial network (VAE-GAN). A method includes receiving an image from a camera of a vehicle and providing the image to a VAE-GAN. The method includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image. The method includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map. The method is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs. 
   
   ">
    <meta name="DC.type" content="patent">
    <meta name="DC.title" content="End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
     ">
    <meta name="DC.date" content="2018-08-03" scheme="dateSubmitted">
    <meta name="DC.description" content="
     The disclosure relates to systems, methods, and devices for simultaneous localization and mapping of a robot in an environment utilizing a variational autoencoder generative adversarial network (VAE-GAN). A method includes receiving an image from a camera of a vehicle and providing the image to a VAE-GAN. The method includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image. The method includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map. The method is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs. 
   
   ">
    <meta name="citation_patent_application_number" content="US:16/054,694">
    <meta name="citation_pdf_url" content="https://patentimages.storage.googleapis.com/66/af/65/a9f3b2cdca63c0/US20200041276A1.pdf">
    <meta name="citation_patent_publication_number" content="US:20200041276:A1">
    <meta name="DC.date" content="2020-02-06">
    <meta name="DC.contributor" content="Punarjay Chakravarty" scheme="inventor">
    <meta name="DC.contributor" content="Praveen Narayanan" scheme="inventor">
    <meta name="DC.contributor" content="Ford Global Technologies LLC" scheme="assignee">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Product+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700">

    <style>
      body { transition: none; }
    </style>

    <script>
      window.version = 'patent-search.search_20240108_RC01';

      function sendFeedback() {
        userfeedback.api.startFeedback({
          'productId': '713680',
          'bucket': 'patent-search-web',
          'productVersion': window.version,
        });
      }

      window.experiments = {};
      window.experiments.patentCountries = "ae,ag,al,am,ao,ap,ar,at,au,aw,az,ba,bb,bd,be,bf,bg,bh,bj,bn,bo,br,bw,bx,by,bz,ca,cf,cg,ch,ci,cl,cm,cn,co,cr,cs,cu,cy,cz,dd,de,dj,dk,dm,do,dz,ea,ec,ee,eg,em,ep,es,fi,fr,ga,gb,gc,gd,ge,gh,gm,gn,gq,gr,gt,gw,hk,hn,hr,hu,ib,id,ie,il,in,ir,is,it,jo,jp,ke,kg,kh,km,kn,kp,kr,kw,kz,la,lc,li,lk,lr,ls,lt,lu,lv,ly,ma,mc,md,me,mg,mk,ml,mn,mo,mr,mt,mw,mx,my,mz,na,ne,ng,ni,nl,no,nz,oa,om,pa,pe,pg,ph,pl,pt,py,qa,ro,rs,ru,rw,sa,sc,sd,se,sg,si,sk,sl,sm,sn,st,su,sv,sy,sz,td,tg,th,tj,tm,tn,tr,tt,tw,tz,ua,ug,us,uy,uz,vc,ve,vn,wo,yu,za,zm,zw";
      
      
      window.experiments.keywordWizard = true;
      
      
      
      window.experiments.definitions = true;

      window.Polymer = {
        dom: 'shady',
        lazyRegister: true,
      };
    </script>

    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/webcomponentsjs/webcomponents-lite.min.js"></script>
    <link rel="import" href="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.html">
  </head>
  <body unresolved>
    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.js"></script>
    <search-app>
      
      

      <article class="result" itemscope itemtype="http://schema.org/ScholarlyArticle">
  <h1 itemprop="pageTitle">US20200041276A1 - End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
      - Google Patents</h1>
  <span itemprop="title">End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
     </span>

  <meta itemprop="type" content="patent">
  <a href="https://patentimages.storage.googleapis.com/66/af/65/a9f3b2cdca63c0/US20200041276A1.pdf" itemprop="pdfLink">Download PDF</a>
  <h2>Info</h2>

  <dl>
    <dt>Publication number</dt>
    <dd itemprop="publicationNumber">US20200041276A1</dd>
    <meta itemprop="numberWithoutCodes" content="20200041276">
    <meta itemprop="kindCode" content="A1">
    <meta itemprop="publicationDescription" content="Patent application publication">
    <span>US20200041276A1</span>
    <span>US16/054,694</span>
    <span>US201816054694A</span>
    <span>US2020041276A1</span>
    <span>US 20200041276 A1</span>
    <span>US20200041276 A1</span>
    <span>US 20200041276A1</span>
    <span>  </span>
    <span> </span>
    <span> </span>
    <span>US 201816054694 A</span>
    <span>US201816054694 A</span>
    <span>US 201816054694A</span>
    <span>US 2020041276 A1</span>
    <span>US2020041276 A1</span>
    <span>US 2020041276A1</span>

    <dt>Authority</dt>
    <dd itemprop="countryCode">US</dd>
    <dd itemprop="countryName">United States</dd>

    <dt>Prior art keywords</dt>
    <dd itemprop="priorArtKeywords" repeat>gan</dd>
    <dd itemprop="priorArtKeywords" repeat>image</dd>
    <dd itemprop="priorArtKeywords" repeat>vae</dd>
    <dd itemprop="priorArtKeywords" repeat>training</dd>
    <dd itemprop="priorArtKeywords" repeat>pose</dd>

    <dt>Prior art date</dt>
    <dd><time itemprop="priorArtDate" datetime="2018-08-03">2018-08-03</time></dd>

    <dt>Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)</dt>
    <dd itemprop="legalStatusIfi" itemscope>
      <span itemprop="status">Abandoned</span>
    </dd>
  </dl>

  <dt>Application number</dt>
  <dd itemprop="applicationNumber">US16/054,694</dd>

  

  

  <dt>Inventor</dt>
  <dd itemprop="inventor" repeat>Punarjay Chakravarty</dd>
  <dd itemprop="inventor" repeat>Praveen Narayanan</dd>

  <dt>Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)</dt>
  <dd itemprop="assigneeCurrent" repeat>
    Ford Global Technologies LLC
  </dd>

  <dt>Original Assignee</dt>
  <dd itemprop="assigneeOriginal" repeat>Ford Global Technologies LLC</dd>

  <dt>Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)</dt>
  <dd><time itemprop="priorityDate" datetime="2018-08-03">2018-08-03</time></dd>

  <dt>Filing date</dt>
  <dd><time itemprop="filingDate" datetime="2018-08-03">2018-08-03</time></dd>

  <dt>Publication date</dt>
  <dd><time itemprop="publicationDate" datetime="2020-02-06">2020-02-06</time></dd>

  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2018-08-03">2018-08-03</time>
    <span itemprop="title">Application filed by Ford Global Technologies LLC</span>
    <span itemprop="type">filed</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    <span itemprop="assigneeSearch">Ford Global Technologies LLC</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2018-08-03">2018-08-03</time>
    <span itemprop="title">Priority to US16/054,694</span>
    <span itemprop="type">priority</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US20200041276A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2018-08-03">2018-08-03</time>
    <span itemprop="title">Assigned to FORD GLOBAL TECHNOLOGIES, LLC</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">FORD GLOBAL TECHNOLOGIES, LLC</span>
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    <span itemprop="description" repeat>Assignors: Chakravarty, Punarjay, Narayanan, Praveen</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2019-08-01">2019-08-01</time>
    <span itemprop="title">Priority to DE102019120880.7A</span>
    <span itemprop="type">priority</span>
    
    
    
    <span itemprop="documentId">patent/DE102019120880A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2019-08-05">2019-08-05</time>
    <span itemprop="title">Priority to CN201910716773.2A</span>
    <span itemprop="type">priority</span>
    
    
    
    <span itemprop="documentId">patent/CN110796692A/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2020-02-06">2020-02-06</time>
    <span itemprop="title">Publication of US20200041276A1</span>
    <span itemprop="type">publication</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US20200041276A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date">Status</time>
    <span itemprop="title">Abandoned</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    <span itemprop="current" content="true" bool>Current</span>
    
    
    
  </dd>

  <h2>Links</h2>
  <ul>
    <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoLink">
          <a href="https://appft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&p=1&u=/netahtml/PTO/srchnum.html&r=1&f=G&l=50&d=PG01&s1=20200041276.PGNR." itemprop="url" target="_blank"><span itemprop="text">USPTO</span></a>
        </li>
        
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoPatentCenterLink">
          <a href="https://patentcenter.uspto.gov/applications/16054694" itemprop="url" target="_blank"><span itemprop="text">USPTO PatentCenter</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoAssignmentLink">
          <a href="https://assignment.uspto.gov/patent/index.html#/patent/search/resultFilter?searchInput=20200041276" itemprop="url" target="_blank"><span itemprop="text">USPTO Assignment</span></a>
        </li>

    <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="espacenetLink">
        <a href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=2020041276A1&amp;KC=A1&amp;FT=D" itemprop="url" target="_blank"><span itemprop="text">Espacenet</span></a>
      </li>
      

    

    
      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="globalDossierLink">
        <a href="https://globaldossier.uspto.gov/#/result/publication/US/20200041276/1" itemprop="url" target="_blank"><span itemprop="text">Global Dossier</span></a>
      </li>

      

      

      

      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="stackexchangeLink">
        <a href="https://patents.stackexchange.com/questions/tagged/US20200041276" itemprop="url"><span itemprop="text">Discuss</span></a>
      </li>
      
  </ul>

  <ul itemprop="concept" itemscope>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013507</span>
      <span itemprop="name">mapping</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>title</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">36</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004807</span>
      <span itemprop="name">localization</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>title</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">34</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000000034</span>
      <span itemprop="name">method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">83</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012549</span>
      <span itemprop="name">training</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">145</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000007</span>
      <span itemprop="name">visual effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">11</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004891</span>
      <span itemprop="name">communication</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">6</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010586</span>
      <span itemprop="name">diagram</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">15</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004422</span>
      <span itemprop="name">calculation algorithm</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">10</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006870</span>
      <span itemprop="name">function</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">9</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000008447</span>
      <span itemprop="name">perception</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">8</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000008569</span>
      <span itemprop="name">process</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">8</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013527</span>
      <span itemprop="name">convolutional neural network</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013528</span>
      <span itemprop="name">artificial neural network</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">4</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000005540</span>
      <span itemprop="name">biological transmission</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">4</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001133</span>
      <span itemprop="name">acceleration</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000009826</span>
      <span itemprop="name">distribution</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000463</span>
      <span itemprop="name">material</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010606</span>
      <span itemprop="name">normalization</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003287</span>
      <span itemprop="name">optical effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002093</span>
      <span itemprop="name">peripheral effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012545</span>
      <span itemprop="name">processing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002123</span>
      <span itemprop="name">temporal effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012360</span>
      <span itemprop="name">testing method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">241000699670</span>
      <span itemprop="name">Mus sp.</span>
      <span itemprop="domain">Species</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">235000019892</span>
      <span itemprop="name">Stellar</span>
      <span itemprop="domain">Nutrition</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006399</span>
      <span itemprop="name">behavior</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004364</span>
      <span itemprop="name">calculation method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004590</span>
      <span itemprop="name">computer program</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013480</span>
      <span itemprop="name">data collection</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000001514</span>
      <span itemprop="name">detection method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000009472</span>
      <span itemprop="name">formulation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003993</span>
      <span itemprop="name">interaction</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000002372</span>
      <span itemprop="name">labelling</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005259</span>
      <span itemprop="name">measurement</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000005055</span>
      <span itemprop="name">memory storage</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000203</span>
      <span itemprop="name">mixture</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012986</span>
      <span itemprop="name">modification</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004048</span>
      <span itemprop="name">modification</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012544</span>
      <span itemprop="name">monitoring process</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">229920001690</span>
      <span itemprop="name">polydopamine</span>
      <span itemprop="domain">Polymers</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000009877</span>
      <span itemprop="name">rendering</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005070</span>
      <span itemprop="name">sampling</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000011218</span>
      <span itemprop="name">segmentation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000007787</span>
      <span itemprop="name">solid</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013403</span>
      <span itemprop="name">standard screening design</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000009466</span>
      <span itemprop="name">transformation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013519</span>
      <span itemprop="name">translation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000002604</span>
      <span itemprop="name">ultrasonography</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
  </ul>

  <section>
    <h2>Images</h2>
    <ul>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/5c/4a/92/0e47f6fed7ce74/US20200041276A1-20200206-D00000.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/78/a2/54/4466279fa7bdbf/US20200041276A1-20200206-D00000.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="301">
            <meta itemprop="label" content="GAN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1533">
              <meta itemprop="top" content="66">
              <meta itemprop="right" content="1586">
              <meta itemprop="bottom" content="93">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="302">
            <meta itemprop="label" content="RGB image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="129">
              <meta itemprop="top" content="344">
              <meta itemprop="right" content="185">
              <meta itemprop="bottom" content="371">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="304">
            <meta itemprop="label" content="image encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="533">
              <meta itemprop="top" content="375">
              <meta itemprop="right" content="590">
              <meta itemprop="bottom" content="402">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="306">
            <meta itemprop="label" content="image decoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1250">
              <meta itemprop="top" content="380">
              <meta itemprop="right" content="1310">
              <meta itemprop="bottom" content="406">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="312">
            <meta itemprop="label" content="pose encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="534">
              <meta itemprop="top" content="738">
              <meta itemprop="right" content="593">
              <meta itemprop="bottom" content="768">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="314">
            <meta itemprop="label" content="pose decoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1249">
              <meta itemprop="top" content="737">
              <meta itemprop="right" content="1310">
              <meta itemprop="bottom" content="767">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="316">
            <meta itemprop="label" content="vector data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1690">
              <meta itemprop="top" content="758">
              <meta itemprop="right" content="1747">
              <meta itemprop="bottom" content="785">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="322">
            <meta itemprop="label" content="depth encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="534">
              <meta itemprop="top" content="1134">
              <meta itemprop="right" content="591">
              <meta itemprop="bottom" content="1159">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="324">
            <meta itemprop="label" content="depth decoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1260">
              <meta itemprop="top" content="1134">
              <meta itemprop="right" content="1318">
              <meta itemprop="bottom" content="1160">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="326">
            <meta itemprop="label" content="depth map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1691">
              <meta itemprop="top" content="1136">
              <meta itemprop="right" content="1746">
              <meta itemprop="bottom" content="1161">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="330">
            <meta itemprop="label" content="latent space">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="894">
              <meta itemprop="top" content="750">
              <meta itemprop="right" content="949">
              <meta itemprop="bottom" content="776">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/c0/29/01/646957f18683eb/US20200041276A1-20200206-D00001.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/a7/53/17/3c625a889340d7/US20200041276A1-20200206-D00001.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="100">
            <meta itemprop="label" content="vehicle control system">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1331">
              <meta itemprop="top" content="19">
              <meta itemprop="right" content="1409">
              <meta itemprop="bottom" content="55">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="102">
            <meta itemprop="label" content="assistance system">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="782">
              <meta itemprop="top" content="627">
              <meta itemprop="right" content="862">
              <meta itemprop="bottom" content="664">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="106">
            <meta itemprop="label" content="more radar systems">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="149">
              <meta itemprop="top" content="576">
              <meta itemprop="right" content="229">
              <meta itemprop="bottom" content="615">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="108">
            <meta itemprop="label" content="more LIDAR systems">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="149">
              <meta itemprop="top" content="860">
              <meta itemprop="right" content="230">
              <meta itemprop="bottom" content="896">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="110">
            <meta itemprop="label" content="more camera systems">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="149">
              <meta itemprop="top" content="1143">
              <meta itemprop="right" content="232">
              <meta itemprop="bottom" content="1180">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="114">
            <meta itemprop="label" content="more ultrasound systems">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="150">
              <meta itemprop="top" content="1708">
              <meta itemprop="right" content="232">
              <meta itemprop="bottom" content="1745">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="116">
            <meta itemprop="label" content="data store">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="537">
              <meta itemprop="top" content="1187">
              <meta itemprop="right" content="619">
              <meta itemprop="bottom" content="1223">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="118">
            <meta itemprop="label" content="transceiver">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="782">
              <meta itemprop="top" content="230">
              <meta itemprop="right" content="861">
              <meta itemprop="bottom" content="265">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="120">
            <meta itemprop="label" content="vehicle control actuators">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1390">
              <meta itemprop="top" content="671">
              <meta itemprop="right" content="1470">
              <meta itemprop="bottom" content="707">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="122">
            <meta itemprop="label" content="more displays">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1389">
              <meta itemprop="top" content="958">
              <meta itemprop="right" content="1470">
              <meta itemprop="bottom" content="994">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="124">
            <meta itemprop="label" content="speakers">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1390">
              <meta itemprop="top" content="1238">
              <meta itemprop="right" content="1469">
              <meta itemprop="bottom" content="1276">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/b3/ab/cf/2619179f20d3fb/US20200041276A1-20200206-D00002.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/0a/64/41/52c34188f146e1/US20200041276A1-20200206-D00002.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="training phase">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="86">
              <meta itemprop="top" content="2443">
              <meta itemprop="right" content="122">
              <meta itemprop="bottom" content="2525">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="201">
            <meta itemprop="label" content="GAN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="85">
              <meta itemprop="top" content="438">
              <meta itemprop="right" content="121">
              <meta itemprop="bottom" content="515">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="202">
            <meta itemprop="label" content="training image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="542">
              <meta itemprop="top" content="2382">
              <meta itemprop="right" content="580">
              <meta itemprop="bottom" content="2464">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="204">
            <meta itemprop="label" content="image encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="585">
              <meta itemprop="top" content="1675">
              <meta itemprop="right" content="625">
              <meta itemprop="bottom" content="1758">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="206">
            <meta itemprop="label" content="image decoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="559">
              <meta itemprop="top" content="886">
              <meta itemprop="right" content="596">
              <meta itemprop="bottom" content="969">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="208">
            <meta itemprop="label" content="reconstructed image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="541">
              <meta itemprop="top" content="206">
              <meta itemprop="right" content="579">
              <meta itemprop="bottom" content="288">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="210">
            <meta itemprop="label" content="vector data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1159">
              <meta itemprop="top" content="2379">
              <meta itemprop="right" content="1199">
              <meta itemprop="bottom" content="2463">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="212">
            <meta itemprop="label" content="pose encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1175">
              <meta itemprop="top" content="1673">
              <meta itemprop="right" content="1215">
              <meta itemprop="bottom" content="1760">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="216">
            <meta itemprop="label" content="pose vector data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1148">
              <meta itemprop="top" content="205">
              <meta itemprop="right" content="1188">
              <meta itemprop="bottom" content="289">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="220">
            <meta itemprop="label" content="training depth map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1807">
              <meta itemprop="top" content="2361">
              <meta itemprop="right" content="1844">
              <meta itemprop="bottom" content="2444">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="222">
            <meta itemprop="label" content="depth encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1827">
              <meta itemprop="top" content="1675">
              <meta itemprop="right" content="1861">
              <meta itemprop="bottom" content="1759">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="224">
            <meta itemprop="label" content="depth decoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1826">
              <meta itemprop="top" content="871">
              <meta itemprop="right" content="1865">
              <meta itemprop="bottom" content="956">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="226">
            <meta itemprop="label" content="depth map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1810">
              <meta itemprop="top" content="206">
              <meta itemprop="right" content="1847">
              <meta itemprop="bottom" content="291">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="230">
            <meta itemprop="label" content="latent space">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1197">
              <meta itemprop="top" content="1274">
              <meta itemprop="right" content="1235">
              <meta itemprop="bottom" content="1357">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/5b/07/bc/920dde70a10575/US20200041276A1-20200206-D00003.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/cd/70/6e/4c842f3d045134/US20200041276A1-20200206-D00003.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="301">
            <meta itemprop="label" content="GAN">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="99">
              <meta itemprop="top" content="427">
              <meta itemprop="right" content="134">
              <meta itemprop="bottom" content="504">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="302">
            <meta itemprop="label" content="RGB image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="509">
              <meta itemprop="top" content="2499">
              <meta itemprop="right" content="547">
              <meta itemprop="bottom" content="2579">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="304">
            <meta itemprop="label" content="image encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="553">
              <meta itemprop="top" content="1899">
              <meta itemprop="right" content="592">
              <meta itemprop="bottom" content="1981">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="306">
            <meta itemprop="label" content="image decoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="560">
              <meta itemprop="top" content="839">
              <meta itemprop="right" content="598">
              <meta itemprop="bottom" content="921">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="312">
            <meta itemprop="label" content="pose encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1094">
              <meta itemprop="top" content="1895">
              <meta itemprop="right" content="1133">
              <meta itemprop="bottom" content="1981">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="314">
            <meta itemprop="label" content="pose decoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1092">
              <meta itemprop="top" content="838">
              <meta itemprop="right" content="1131">
              <meta itemprop="bottom" content="922">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="316">
            <meta itemprop="label" content="vector data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1120">
              <meta itemprop="top" content="189">
              <meta itemprop="right" content="1161">
              <meta itemprop="bottom" content="273">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="322">
            <meta itemprop="label" content="depth encoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1674">
              <meta itemprop="top" content="1898">
              <meta itemprop="right" content="1711">
              <meta itemprop="bottom" content="1981">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="324">
            <meta itemprop="label" content="depth decoder">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1676">
              <meta itemprop="top" content="824">
              <meta itemprop="right" content="1715">
              <meta itemprop="bottom" content="908">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="326">
            <meta itemprop="label" content="depth map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1678">
              <meta itemprop="top" content="188">
              <meta itemprop="right" content="1714">
              <meta itemprop="bottom" content="274">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="3">
            <meta itemprop="id" content="330">
            <meta itemprop="label" content="latent space">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1108">
              <meta itemprop="top" content="1367">
              <meta itemprop="right" content="1146">
              <meta itemprop="bottom" content="1452">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/6f/b4/62/34097cd99a3158/US20200041276A1-20200206-D00004.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/ea/be/65/fc26cc3b4ce795/US20200041276A1-20200206-D00004.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="400">
            <meta itemprop="label" content="process">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="322">
              <meta itemprop="top" content="49">
              <meta itemprop="right" content="403">
              <meta itemprop="bottom" content="85">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="402">
            <meta itemprop="label" content="RGB image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="255">
              <meta itemprop="top" content="557">
              <meta itemprop="right" content="334">
              <meta itemprop="bottom" content="595">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="404">
            <meta itemprop="label" content="GAN generator">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1372">
              <meta itemprop="top" content="182">
              <meta itemprop="right" content="1454">
              <meta itemprop="bottom" content="219">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="406">
            <meta itemprop="label" content="depth map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1361">
              <meta itemprop="top" content="1558">
              <meta itemprop="right" content="1441">
              <meta itemprop="bottom" content="1596">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="4">
            <meta itemprop="id" content="408">
            <meta itemprop="label" content="GAN discriminator">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="393">
              <meta itemprop="top" content="1533">
              <meta itemprop="right" content="476">
              <meta itemprop="bottom" content="1574">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/fd/18/ae/e74de3753fe7f9/US20200041276A1-20200206-D00005.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/92/0f/16/36ca3eb2091869/US20200041276A1-20200206-D00005.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="500">
            <meta itemprop="label" content="method">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="72">
              <meta itemprop="top" content="19">
              <meta itemprop="right" content="152">
              <meta itemprop="bottom" content="54">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/8f/4e/ac/8f8b7a9cf46587/US20200041276A1-20200206-D00006.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/70/7d/e3/3195ae0dc51119/US20200041276A1-20200206-D00006.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="600">
            <meta itemprop="label" content="method">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="99">
              <meta itemprop="top" content="19">
              <meta itemprop="right" content="183">
              <meta itemprop="bottom" content="54">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/31/dd/fc/5dc979a473d5b6/US20200041276A1-20200206-D00007.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/97/cb/bd/09cdec7b06f61f/US20200041276A1-20200206-D00007.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="700">
            <meta itemprop="label" content="method">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="101">
              <meta itemprop="top" content="19">
              <meta itemprop="right" content="182">
              <meta itemprop="bottom" content="55">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/a0/cc/a8/bfaadf85ab520d/US20200041276A1-20200206-D00008.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/ea/68/92/19a12d392f5658/US20200041276A1-20200206-D00008.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="800">
            <meta itemprop="label" content="computing device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1122">
              <meta itemprop="top" content="28">
              <meta itemprop="right" content="1203">
              <meta itemprop="bottom" content="63">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="812">
            <meta itemprop="label" content="Bus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="614">
              <meta itemprop="top" content="81">
              <meta itemprop="right" content="694">
              <meta itemprop="bottom" content="118">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="822">
            <meta itemprop="label" content="peripheral device interface">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="248">
              <meta itemprop="top" content="2088">
              <meta itemprop="right" content="330">
              <meta itemprop="bottom" content="2126">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="824">
            <meta itemprop="label" content="hard disk drive">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1135">
              <meta itemprop="top" content="709">
              <meta itemprop="right" content="1219">
              <meta itemprop="bottom" content="747">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="830">
            <meta itemprop="label" content="display device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1306">
              <meta itemprop="top" content="1654">
              <meta itemprop="right" content="1389">
              <meta itemprop="bottom" content="1704">
            </span>
          </li>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>Classifications</h2>
    <ul>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/50</span>&mdash;<span itemprop="Description">Depth or shape recovery</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01</span>&mdash;<span itemprop="Description">MEASURING; TESTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C</span>&mdash;<span itemprop="Description">MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/00</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/26</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/28</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network with correlation of data from several navigational instruments</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/30</span>&mdash;<span itemprop="Description">Map- or contour-matching</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/32</span>&mdash;<span itemprop="Description">Structuring or formatting of map data</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01</span>&mdash;<span itemprop="Description">MEASURING; TESTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C</span>&mdash;<span itemprop="Description">MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/00</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/38</span>&mdash;<span itemprop="Description">Electronic maps specially adapted for navigation; Updating thereof</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/3804</span>&mdash;<span itemprop="Description">Creation or updating of map data</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/3833</span>&mdash;<span itemprop="Description">Creation or updating of map data characterised by the source of data</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/3848</span>&mdash;<span itemprop="Description">Data obtained from both position sensors and additional sensors</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/50</span>&mdash;<span itemprop="Description">Depth or shape recovery</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/55</span>&mdash;<span itemprop="Description">Depth or shape recovery from multiple images</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F</span>&mdash;<span itemprop="Description">ELECTRIC DIGITAL DATA PROCESSING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/00</span>&mdash;<span itemprop="Description">Information retrieval; Database structures therefor; File system structures therefor</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/20</span>&mdash;<span itemprop="Description">Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/29</span>&mdash;<span itemprop="Description">Geographical information databases</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F</span>&mdash;<span itemprop="Description">ELECTRIC DIGITAL DATA PROCESSING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/00</span>&mdash;<span itemprop="Description">Information retrieval; Database structures therefor; File system structures therefor</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/50</span>&mdash;<span itemprop="Description">Information retrieval; Database structures therefor; File system structures therefor of still image data</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/56</span>&mdash;<span itemprop="Description">Information retrieval; Database structures therefor; File system structures therefor of still image data having vectorial format</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F17/30241</span>&mdash;<span itemprop="Description"></span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/00</span>&mdash;<span itemprop="Description">Computing arrangements based on biological models</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/02</span>&mdash;<span itemprop="Description">Neural networks</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/04</span>&mdash;<span itemprop="Description">Architecture, e.g. interconnection topology</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/045</span>&mdash;<span itemprop="Description">Combinations of networks</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/0454</span>&mdash;<span itemprop="Description"></span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/00</span>&mdash;<span itemprop="Description">Computing arrangements based on biological models</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/02</span>&mdash;<span itemprop="Description">Neural networks</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/04</span>&mdash;<span itemprop="Description">Architecture, e.g. interconnection topology</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/047</span>&mdash;<span itemprop="Description">Probabilistic or stochastic networks</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/00</span>&mdash;<span itemprop="Description">Computing arrangements based on biological models</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/02</span>&mdash;<span itemprop="Description">Neural networks</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/08</span>&mdash;<span itemprop="Description">Learning methods</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/00</span>&mdash;<span itemprop="Description">Computing arrangements based on biological models</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/02</span>&mdash;<span itemprop="Description">Neural networks</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/08</span>&mdash;<span itemprop="Description">Learning methods</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/082</span>&mdash;<span itemprop="Description">Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/00</span>&mdash;<span itemprop="Description">Computing arrangements based on biological models</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/02</span>&mdash;<span itemprop="Description">Neural networks</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/08</span>&mdash;<span itemprop="Description">Learning methods</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/088</span>&mdash;<span itemprop="Description">Non-supervised learning, e.g. competitive learning</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/70</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/73</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras using feature-based methods</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/74</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras using feature-based methods involving reference images or patches</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H</span>&mdash;<span itemprop="Description">ELECTRICITY</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04</span>&mdash;<span itemprop="Description">ELECTRIC COMMUNICATION TECHNIQUE</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N</span>&mdash;<span itemprop="Description">PICTORIAL COMMUNICATION, e.g. TELEVISION</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N19/00</span>&mdash;<span itemprop="Description">Methods or arrangements for coding, decoding, compressing or decompressing digital video signals</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N19/10</span>&mdash;<span itemprop="Description">Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N19/134</span>&mdash;<span itemprop="Description">Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N19/136</span>&mdash;<span itemprop="Description">Incoming video signal characteristics or properties</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N19/137</span>&mdash;<span itemprop="Description">Motion inside a coding unit, e.g. average field, frame or block difference</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N19/139</span>&mdash;<span itemprop="Description">Analysis of motion vectors, e.g. their magnitude, direction, variance or reliability</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H</span>&mdash;<span itemprop="Description">ELECTRICITY</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04</span>&mdash;<span itemprop="Description">ELECTRIC COMMUNICATION TECHNIQUE</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N</span>&mdash;<span itemprop="Description">PICTORIAL COMMUNICATION, e.g. TELEVISION</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N19/00</span>&mdash;<span itemprop="Description">Methods or arrangements for coding, decoding, compressing or decompressing digital video signals</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">H04N19/44</span>&mdash;<span itemprop="Description">Decoders specially adapted therefor, e.g. video decoders which are asymmetric with respect to the encoder</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10</span>&mdash;<span itemprop="Description">Image acquisition modality</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10016</span>&mdash;<span itemprop="Description">Video; Image sequence</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10</span>&mdash;<span itemprop="Description">Image acquisition modality</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/10028</span>&mdash;<span itemprop="Description">Range image; Depth image; 3D point clouds</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20</span>&mdash;<span itemprop="Description">Special algorithmic details</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20081</span>&mdash;<span itemprop="Description">Training; Learning</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20</span>&mdash;<span itemprop="Description">Special algorithmic details</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20084</span>&mdash;<span itemprop="Description">Artificial neural networks [ANN]</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30</span>&mdash;<span itemprop="Description">Subject of image; Context of image processing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30244</span>&mdash;<span itemprop="Description">Camera pose</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30</span>&mdash;<span itemprop="Description">Subject of image; Context of image processing</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30248</span>&mdash;<span itemprop="Description">Vehicle exterior or interior</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/30252</span>&mdash;<span itemprop="Description">Vehicle exterior; Vicinity of vehicle</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
    </ul>
  </section>

  

  

  <section>
    <h2>Definitions</h2>
    <ul>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the present disclosure</span>
        <span itemprop="definition">relates to methods, systems, and apparatuses for simultaneous localization and mapping of an apparatus in an environment, and particularly relates to simultaneous localization and mapping of a vehicle using a variational autoencoder generative adversarial network.</span>
        <meta itemprop="num_attr" content="0001">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Localization, mapping, and depth perception in real-time</span>
        <span itemprop="definition">are requirements for certain autonomous systems, including autonomous driving systems or mobile robotics systems.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each of localization, mapping, and depth perception</span>
        <span itemprop="definition">are key components for carrying out certain tasks such as obstacle avoidance, route planning, mapping, localization, pedestrian detection, and human-robot interaction.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Depth perception and localization</span>
        <span itemprop="definition">are traditionally performed by expensive active sensing systems such as LIDAR sensors or passive sensing systems such as binocular vision or stereo cameras.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Systems, methods, and devices for computing localization, mapping, and depth perception</span>
        <span itemprop="definition">can be integrated in automobiles such as autonomous vehicles and driving assistance systems.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such systems</span>
        <span itemprop="definition">are currently being developed and deployed to provide safety features, reduce an amount of user input required, or even eliminate user involvement entirely.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">some driving assistance systems</span>
        <span itemprop="definition">such as crash avoidance systems, may monitor driving, positions, and a velocity of the vehicle and other objects while a human is driving. When the system detects that a crash or impact is imminent the crash avoidance system may intervene and apply a brake, steer the vehicle, or perform other avoidance or safety maneuvers.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">autonomous vehicles</span>
        <span itemprop="definition">may drive, navigate, and/or park a vehicle with little or no user input.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">autonomous vehicles and driving assistance systems</span>
        <span itemprop="definition">operate safely and are able to accurately navigate roads in a variety of different driving environments.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">is a schematic block diagram illustrating an example vehicle control system or autonomous vehicle system, according to one embodiment</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">is a schematic block diagram of a variational autoencoder generative adversarial network in a training phase, according to one embodiment</span>
        <meta itemprop="num_attr" content="0006">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3</span>
        <span itemprop="definition">is a schematic block diagram of a variational autoencoder generative adversarial network in a computation phase, according to one embodiment</span>
        <meta itemprop="num_attr" content="0007">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">is a schematic block diagram illustrating a process for determining a depth map of an environment, according to one embodiment</span>
        <meta itemprop="num_attr" content="0008">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a schematic flow chart diagram of a method for utilizing simultaneous localization and mapping of a vehicle in an environment, according to one embodiment</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">is a schematic flow chart diagram of a method for utilizing simultaneous localization and mapping of a vehicle in an environment, according to one embodiment</span>
        <meta itemprop="num_attr" content="0010">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">is a schematic flow chart diagram of a method for training a variational autoencoder generative adversarial network, according to one embodiment.</span>
        <meta itemprop="num_attr" content="0011">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 8</span>
        <span itemprop="definition">is a schematic block diagram illustrating an example computing system, according to one embodiment.</span>
        <meta itemprop="num_attr" content="0012">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Localization of a vehicle along with mapping and depth perception of drivable surfaces or regions</span>
        <span itemprop="definition">is an important aspect of allowing for and improving operation of autonomous vehicle or driver assistance features. For example, a vehicle must know precisely where obstacles or drivable surfaces are located to navigate safely around objects.</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Simultaneous Localization and Mapping</span>
        <span itemprop="definition">forms the basis for operational functionality of mobile robots, including autonomous vehicles and other mobile robots.</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">robots</span>
        <span itemprop="definition">include an indoor mobile robot configured for delivering items in a warehouse or an autonomous drone configured for traversing a building or other environment in a disaster scenario.</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SLAM</span>
        <span itemprop="definition">is directed to sensing the robot&#39;s environment and building a map of its surroundings as the robot moves through its environment.</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SLAM</span>
        <span itemprop="definition">is further directed to simultaneously localizing the robot within its environment by extracting pose vector data, including six Degree of Freedom (DoF) poses relative to a starting point of the robot.</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DoF</span>
        <span itemprop="definition">Degree of Freedom</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SLAM</span>
        <span itemprop="definition">thus incrementally generates a map of the robot&#39;s environment. In the case of a robot repeating a route that it has previously mapped, the robot can solve for the localization subset of the problem without generating a new map. The generating of building a map of a new area necessitates SLAM.</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SLAM</span>
        <span itemprop="definition">is commonly implemented utilizing a depth sensor, such as a LIDAR sensor or a stereo camera.</span>
        <meta itemprop="num_attr" content="0015">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SLAM</span>
        <span itemprop="definition">normally necessitates such devices for enabling the SLAM process to measure the depth and distance of three-dimensional landmarks and to calculate the robot&#39;s position in relation to those landmarks.</span>
        <meta itemprop="num_attr" content="0015">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SLAM</span>
        <span itemprop="definition">may also be implemented using monocular vision, but the depth recovered through triangulation of landmarks from a moving camera over time is up to scale only, such that relative depths of objects in the scene are recovered without absolute depth measurements.</span>
        <meta itemprop="num_attr" content="0015">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Robots</span>
        <span itemprop="definition">must know how far an object is from the robot such that the robot can determine a collision-free path around the object.</span>
        <meta itemprop="num_attr" content="0016">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Robots</span>
        <span itemprop="definition">utilize LIDAR sensors and stereo camera to determine a dense depth-map of obstacles around the robot. Some of the same obstacles determined through this process may be utilized as three-dimensional landmarks in the SLAM implementation.</span>
        <meta itemprop="num_attr" content="0016">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Applicant</span>
        <span itemprop="definition">has developed systems, methods, and devices for improving operations in both SLAM and obstacle avoidance.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Applicant</span>
        <span itemprop="definition">presents systems, methods, and devices for generating a dense depth map for obstacle avoidance, determining a robot&#39;s location, and determining pose vector data as a robot traverses its environment.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the systems, methods, and devices of the present disclosure</span>
        <span itemprop="definition">utilize a monocular camera and do not necessitate the use of expensive LIDAR sensors or stereo cameras that further require intensive computing resources.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the disclosure</span>
        <span itemprop="definition">presents lightweight, inexpensive, and low-computing methods for sensing a robot&#39;s surrounding, localizing a robot within its environment, and enabling the robot to generate obstacle avoidance movement procedures.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such systems, methods, and devices of the present disclosure</span>
        <span itemprop="definition">may be implemented on any suitable robotics system, including for example, an autonomous vehicle, a mobile robot, and/or a drone or smart mobility vehicle.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAEs</span>
        <span itemprop="definition">Variational autoencoders</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a VAE</span>
        <span itemprop="definition">can serve as an autoencoder while further serving as a generative model from which new data can be generated by sampling from a latent manifold.</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE</span>
        <span itemprop="definition">consists of an encoder, which maps the input to a compressed latent representation.</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE</span>
        <span itemprop="definition">further includes a decoder configured to decode the latent vector back to an output.</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the entire VAE system</span>
        <span itemprop="definition">may be trained end to end as a deep neural network.</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE</span>
        <span itemprop="definition">may be configured to encode meaningful information about various data attributes in its latent manifold which can then be exploited to carry out pertinent tasks.</span>
        <meta itemprop="num_attr" content="0019">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Applicant</span>
        <span itemprop="definition">presents utilizing a shared latent space assumption of a VAE between an image, pose vector data of the image, and a depth map of the image, to facilitate the use of SLAM in conjunction with the VAE.</span>
        <meta itemprop="num_attr" content="0019">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Generative adversarial networks</span>
        <span itemprop="definition">are a class of generative models configured to produce high quality samples from probability distributions of interest.</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a GAN</span>
        <span itemprop="definition">may generate output samples of stellar artistic quality.</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training methodology for a GAN</span>
        <span itemprop="definition">is adversarial, in that the generator (the network that produces samples, often called âfakesâ) learns by fooling another network called the discriminator that decides whether the samples produced are real or fake.</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the generator network and the discriminator network</span>
        <span itemprop="definition">are trained in tandem, with the generator network eventually learning to produce samples that succeed in fooling the discriminator network. At such a point, the GAN is able to generate samples from the probability distribution underlying the generative process.</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAEs</span>
        <span itemprop="definition">confer advantages in providing latent representations of data for further us.</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">one drawback of the VAE</span>
        <span itemprop="definition">is the blurriness of the samples produced.</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GANs</span>
        <span itemprop="definition">on the other hand, produce excellent samples but do not have a useful latent representation available.</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">utilizes and combines each system such that one obtains a tractable VAE latent representation while also improving upon the quality of the samples by using a GAN as the generator in the decoder of the VAE. This results in crisper images than a VAE alone.</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the systems, methods, and devices of the present disclosure</span>
        <span itemprop="definition">utilize the VAE-GAN as the central machinery in the SLAM algorithm.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such systems, methods, and devices</span>
        <span itemprop="definition">receiving an input such as a red-green-blue (RGB) image and outputs corresponding depth maps and pose vector data for the camera that captured the RGB image.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the system</span>
        <span itemprop="definition">is trained using a regular stereo visual SLAM pipeline, where stereo visual simultaneous localization and mapping (vSLAM) receives a sequence of stereoscopic images, generates the depth maps and corresponding six Degree of Freedom poses as the stereo camera moves through space.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Stereo vSLAM</span>
        <span itemprop="definition">trains the VAE-GAN-SLAM algorithm using a sequence of RGB images, the corresponding depth maps for the images, and the corresponding pose vector data for the images.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">is trained to reconstruct the RGB image, the pose vector data for the image, and the depth map for the image while creating a shared latent space representation of the same.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the assumption</span>
        <span itemprop="definition">is that the RGB image, depth map of the image, and pose vector data of the image are sampled from places close together in the real world, are close together in the learnt shared latent space as well.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">take as its input an RGB image coming from a monocular camera moving through the same environment and produce both a depth map and pose vector data for the monocular camera.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the latent space representation of the VAE-GAN</span>
        <span itemprop="definition">also enables disentanglement and latent space arithmetic.</span>
        <meta itemprop="num_attr" content="0023">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An example of such an embodiment</span>
        <span itemprop="definition">would be to isolate a dimension in the latent vector responsible for a certain attribute of interest, such as a pose dimension, and create previously unseen view of a scene by changing the pose vector.</span>
        <meta itemprop="num_attr" content="0023">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Applicant</span>
        <span itemprop="definition">recognizes that the systems, methods, and devices disclosed herein enable the use of the system as a SLAM box for facilitating fast and easy single-image inference producing the pose of a robot and the positions of obstacles in the environment around the robot.</span>
        <meta itemprop="num_attr" content="0024">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GANs</span>
        <span itemprop="definition">Generative adversarial networks</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Applicant</span>
        <span itemprop="definition">presents systems, methods, and devices for depth estimation of a single image using a GAN.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such systems, methods, and devices</span>
        <span itemprop="definition">improve performance over known depth estimation systems, and further require a smaller number of training images.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the use of GAN as opposed to a regular convolutional neural network</span>
        <span itemprop="definition">enables the collection of a small amount of training data in each environment, typically in the hundreds of images as opposed to the hundreds of thousands of images required by convolutional neural networks.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such systems, methods, and devices</span>
        <span itemprop="definition">reduce the burden for data collection by an order of magnitude.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Applicant</span>
        <span itemprop="definition">further presents systems, methods, and devices for depth estimation utilizing visual simultaneous localization and mapping (vSLAM) methods for ensuring temporal consistency in the generated depth maps produced by the GAN as the camera moves through an environment.</span>
        <meta itemprop="num_attr" content="0026">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vSLAM module</span>
        <span itemprop="definition">provides pose information of the camera, e.g. how much the camera has moved between successive images. Such pose information is provided to the GAN as a temporal constraint on training the GAN to promote the GAN to generate consistent depth maps for successive images.</span>
        <meta itemprop="num_attr" content="0026">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">includes providing the image to a variational autoencoder generative adversarial network (VAE-GAN).</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">is such that the VAE-GAN comprises a single latent space for encoding a plurality of inputs.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">illustrates an example vehicle control system 100 that may be used for autonomous or assisted driving.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the automated driving/assistance system 102</span>
        <span itemprop="definition">may be used to automate or control operation of a vehicle or to aid a human driver.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the automated driving/assistance system 102</span>
        <span itemprop="definition">may control one or more of braking, steering, acceleration, lights, alerts, driver notifications, radio, or any other auxiliary systems of the vehicle.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the automated driving/assistance system 102</span>
        <span itemprop="definition">may not be able to provide any control of the driving (e.g., steering, acceleration, or braking), but may provide notifications and alerts to assist a human driver in driving safely.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the automated driving/assistance system 102</span>
        <span itemprop="definition">may use a neural network, or other model or algorithm to detect or localize objects based on perception data gathered by one or more sensors.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle control system 100</span>
        <span itemprop="definition">also includes one or more sensor systems/devices for detecting a presence of objects near or within a sensor range of a parent vehicle (e.g., a vehicle that includes the vehicle control system 100 ).</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle control system 100</span>
        <span itemprop="definition">may include one or more radar systems 106 , one or more LIDAR systems 108 , one or more camera systems 110 , a global positioning system (GPS) 112 , and/or one or more ultrasound systems 114 .</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle control system 100</span>
        <span itemprop="definition">may include a data store 116 for storing relevant or useful data for navigation and safety such as map data, driving history or other data.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle control system 100</span>
        <span itemprop="definition">may also include a transceiver 118 for wireless communication with a mobile or wireless network, other vehicles, infrastructure, or any other communication system.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle control system 100</span>
        <span itemprop="definition">may include vehicle control actuators 120 to control various aspects of the driving of the vehicle such as electric motors, switches or other actuators, to control braking, acceleration, steering or the like.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle control system 100</span>
        <span itemprop="definition">may also include one or more displays 122 , speakers 124 , or other devices so that notifications to a human driver or passenger may be provided.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a display 122</span>
        <span itemprop="definition">may include a heads-up display, dashboard display or indicator, a display screen, or any other visual indicator which may be seen by a driver or passenger of a vehicle.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a heads-up display</span>
        <span itemprop="definition">may be used to provide notifications or indicate locations of detected objects or overlay instructions or driving maneuvers for assisting a driver.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the speakers 124</span>
        <span itemprop="definition">may include one or more speakers of a sound system of a vehicle or may include a speaker dedicated to driver notification.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">is given by way of example only. Other embodiments may include fewer or additional components without departing from the scope of the disclosure. Additionally, illustrated components may be combined or included within other components without limitation.</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the automated driving/assistance system 102</span>
        <span itemprop="definition">is configured to control driving or navigation of a parent vehicle.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the automated driving/assistance system 102</span>
        <span itemprop="definition">may control the vehicle control actuators 120 to drive a path on a road, parking lot, driveway or other location.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the automated driving/assistance system 102</span>
        <span itemprop="definition">may determine a path based on information or perception data provided by any of the components 106 - 114 .</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the sensor systems/devices 106 - 114</span>
        <span itemprop="definition">may be used to obtain real-time sensor data so that the automated driving/assistance system 102 can assist a driver or drive a vehicle in real-time.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">illustrates a schematic block diagram of a training phase 200 of a variational autoencoder generative adversarial network (VAE-GAN) 201 .</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">includes an image encoder 204 and a corresponding image decoder 206 .</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">includes a pose encoder 212 and a corresponding pose decoder 214 .</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">includes a depth encoder 222 and a corresponding depth decoder 224 .</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each of the image decoder 206 , the pose decoder 214 , and the depth decoder 224</span>
        <span itemprop="definition">includes a generative adversarial network (GAN) that comprises a GAN generator (see e.g.</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">includes a latent space 230 that is shared by each of the image encoder 204 , the image decoder 206 , the pose encoder 212 , the pose decoder 214 , the depth encoder 222 , and the depth decoder 224 .</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">receives a training image 202 at the image encoder 204 and generates a reconstructed image 208 based on the training image 202 .</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">receives training pose vector data 210 that is based on the training image 202 at the pose encoder 212 and the VAE-GAN 201 generates reconstructed pose vector data 216 based on the training pose vector data 210 .</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">receives a training depth map 220 that is based on the training image 202 at the depth encoder 222 and outputs a reconstructed depth map 226 that is based on the training depth map 220 .</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">is the central machinery in the simultaneous localization and mapping (SLAM) algorithm of the present disclosure.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">is trained utilizing a regular stereo visual SLAM pipeline.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a stereo visual SLAM</span>
        <span itemprop="definition">takes a sequence of stereoscopic images and generates depth maps and corresponding six Degrees of Freedom poses for the stereo camera as the camera moves through space.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Stereo visual SLAM</span>
        <span itemprop="definition">trains the VAE-GAN-SLAM algorithm using a sequence of red-green-blue (RGB) images where only the left image of a stereo pair is used, along with the corresponding depth maps and six Degrees of Freedom pose vector data for the sequence of RGB images.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">RGB</span>
        <span itemprop="definition">red-green-blue</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">is trained under the assumption that the RGB image, the depth map of the image, and the pose vector data of the image are sampled from locations close together in the real world that are also close together in the learnt shared latent space 230 as well. After the networks are trained, the VAE-GAN 201 can take as its input an RGB image coming from a monocular camera moving through the same environment and produce both a depth map and a six Degree of Freedom pose vector data for the camera.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training image 202</span>
        <span itemprop="definition">is provided to the VAE-GAN 201 for training the VAE-GAN 201 to generate pose vector data and/or depth map data based on an image.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training image 202</span>
        <span itemprop="definition">is a red-blue-green (RGB) image captured by a monocular camera.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training image 202</span>
        <span itemprop="definition">is a single image of a stereo image pair captured by a stereo camera.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed image 208</span>
        <span itemprop="definition">is generated by the VAE-GAN 201 based on the training image 202 .</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image encoder 204 and the image decoder 206</span>
        <span itemprop="definition">are adversarial to one another and are configured to generate the reconstructed image 208 .</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image encoder 204</span>
        <span itemprop="definition">is configured to receiving the training image 202 and map the training image 202 to a compress latent representation in the latent space 230 .</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image decoder 206</span>
        <span itemprop="definition">comprises a GAN having a GAN generator and a GAN discriminator.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image decoder 206</span>
        <span itemprop="definition">is configured to decode the compressed latent representation of the training image 202 from the latent space 230 .</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN of the image decoder 206</span>
        <span itemprop="definition">is configured to generate the reconstructed image 208 .</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training pose vector data 210</span>
        <span itemprop="definition">is provided to the VAE-GAN 201 for training the VAE-GAN 201 to generate pose vector data of an image.</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training pose vector data 210</span>
        <span itemprop="definition">includes six Degree of Freedom pose data of a camera that captured the training image 202 , wherein the six Degree of Freedom pose data indicates a relative pose of the camera when the image was captured as the camera traversed an environment.</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed pose vector data 216</span>
        <span itemprop="definition">is generated by the VAE-GAN 201 based on the training pose vector data 210 .</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the pose encoder 212</span>
        <span itemprop="definition">is configured to receive the training pose vector data 210 and map the training pose vector data 210 to a compressed latent representation in the latent space 230 of the VEA-GAN 201 .</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the pose decoder 214</span>
        <span itemprop="definition">is configured to decode the compressed latent representation of the training pose vector data 210 from the latent space 230 .</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the pose decoder 214</span>
        <span itemprop="definition">comprises a GAN that comprises a GAN generator and a GAN discriminator.</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN of the pose decoder 214</span>
        <span itemprop="definition">is configured to generate the reconstructed pose vector data 216 based on the training pose vector data 210 .</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the training depth map 220</span>
        <span itemprop="definition">is provided to the VAE-GAN 201 for training the VAE-GAN 201 to generate a depth map of an image.</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the depth map 220</span>
        <span itemprop="definition">is based on the training image 202 and includes depth information for the training image 202 .</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed depth map 226</span>
        <span itemprop="definition">is generated by the VAE-GAN 201 based on the training depth map 220 .</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the depth encoder 222</span>
        <span itemprop="definition">is configured to receive the training depth map 220 and map the training depth map 220 to a compressed latent representation in the latent space 230 of the VAE-GAN 201 .</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the depth decoder 224</span>
        <span itemprop="definition">comprises a GAN that comprises a GAN generator and a GAN discriminator.</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the depth decoder 224</span>
        <span itemprop="definition">is configured to decode the compressed latent representation of the training depth map 220 from the latent space 230 .</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN of the depth decoder 224</span>
        <span itemprop="definition">is configured to generate the reconstructed depth map 226 based on the training depth map 220 .</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the latent space 230 of the VAE-GAN 201</span>
        <span itemprop="definition">is shared by each of the image encoder 204 , the image decoder 206 , the pose encoder 212 , the pose decoder 214 , the depth encoder 222 , and the depth decoder 224 .</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 201</span>
        <span itemprop="definition">is trained to generate each of the reconstructed image 208 , the reconstructed pose vector data 216 , and the reconstructed depth map 226 in tandem.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the latent space 230</span>
        <span itemprop="definition">includes an encoded latent space vector applicable to each of an image, pose vector data of an image, and a depth map of an image.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the latent space 230 representation of the VAE-GAN 201</span>
        <span itemprop="definition">enables disentanglement and latent space arithmetic.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An example of the disentanglement and latent space arithmetic</span>
        <span itemprop="definition">includes isolating a dimension in the latent space 230 responsible for a certain attribute of interest, such as a posed dimension. This may enable the creation of a previously unseen view of a scheme by changing the pose vector.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">training the latent space 230 simultaneously for all three attributes, namely the image, the pose vector data, and the depth map</span>
        <span itemprop="definition">forces the latent space 230 to be consistent for each of the attributes.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">This</span>
        <span itemprop="definition">provides an elegant formulation where the VAE-GAN 201 is not trained separately for each of an image, pose vector data, and a depth map.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the trained VAE-GAN 201</span>
        <span itemprop="definition">may receive an input image and generate any outer output such as pose vector data based on the input image or a depth map based on the input image.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3</span>
        <span itemprop="definition">illustrates a schematic block diagram of a computing phase 300 (alternatively may be referred to as a generative or execution phase) of a variational autoencoder generative adversarial network (VAE-GAN) 301 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">includes an image encoder 304 and a corresponding image decoder 306 , wherein the image decoder 306 comprises a GAN configured to generate a reconstructed image based on the RGB image 302 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the image encoder 304 and the image decoder 306</span>
        <span itemprop="definition">have been trained (see FIG. 2 ).</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">includes a pose encoder 312 and a corresponding pose decoder 314 , wherein the pose decoder 314 comprises a GAN configured to generate the reconstructed pose vector data 316 based on the RGB image 302 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the pose encoder 312 and the pose decoder 314</span>
        <span itemprop="definition">have been trained (see FIG. 2 ).</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">includes a depth encoder 322 and a corresponding depth decoder 324 , wherein the depth decoder 324 comprises a GAN configured to generate the reconstructed depth map 326 based on the RGB image 302 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">the depth decoder 324 comprises a GAN configured to generate the reconstructed depth map 326 based on the RGB image 302 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">includes a latent space 330 that is shared by the image encoder 304 , the image decoder 306 , the pose encoder 312 , the pose decoder 314 , the depth encoder 322 , and the depth decoder 324 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">receives an RGB image 302 at the image encoder 304 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">outputs reconstructed pose vector data 316 at the trained pose decoder 314 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">outputs a reconstructed depth map 326 at the trained depth decoder 324 .</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the RGB image 302</span>
        <span itemprop="definition">is a red-green-blue image captured by a monocular camera and provided to the VAE-GAN 301 after the VAE-GAN 301 has been trained.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the RGB image 302</span>
        <span itemprop="definition">is captured by a monocular camera of a vehicle, is provided to a vehicle controller, and is provided to the VAE-GAN 301 in real-time.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the RGB image 302</span>
        <span itemprop="definition">may provide a capture of an environment of the vehicle and may be utilized to determine depth perception for the vehicle surroundings.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle controller</span>
        <span itemprop="definition">may implement the result of the VAE-GAN 301 into a SLAM algorithm for computing simultaneous localization and mapping of the vehicle in real-time.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle controller</span>
        <span itemprop="definition">may further provide a notification to a driver, determine a driving maneuver, or execute a driving maneuver based on the results of the SLAM algorithm.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed pose vector data 316</span>
        <span itemprop="definition">is generated by a GAN embedded in the pose decoder 314 of the VAE-GAN 301 .</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">may be trained to generate the reconstructed pose vector data 316 based on a monocular image.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">includes a latent space 330 that is shared by each of an image encoder/decoder, a pose encoder/decoder, and a depth encoder/decoder.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the shared latent space 330</span>
        <span itemprop="definition">enables the VAE-GAN 301 to generate any trained output based on an RGB image 302 (or non-RGB image) as illustrated.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed pose vector data 316</span>
        <span itemprop="definition">includes six Degree of Freedom pose data for a monocular camera.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed pose vector data 316</span>
        <span itemprop="definition">may be utilized by a vehicle to determine a location of the vehicle in its environment and further utilized for simultaneous localization and mapping of the vehicle as it moves through space by implementing the data in a SLAM algorithm.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed depth map 326</span>
        <span itemprop="definition">is generated by a GAN embedded in the depth decoder 324 of the VAE-GAN 301 .</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">may be trained to generate the reconstructed depth map 326 based only on the RGB image 302 .</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed depth map 326</span>
        <span itemprop="definition">provides a dense depth map based on the RGB image 302 and may provide a dense depth map of a surrounding of a robot or autonomous vehicle.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructed depth map 326</span>
        <span itemprop="definition">may be provided to a SLAM algorithm for calculating simultaneous localization and mapping of a robot as the robot moves through its environment.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a vehicle controller</span>
        <span itemprop="definition">may then provide a notification to a driver, determine a driving maneuver, and/or execute a driving maneuver such as an obstacle avoidance maneuver based on the reconstructed depth map 326 and the result of the SLAM algorithm.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the latent space 330</span>
        <span itemprop="definition">is shared by each of the image encoder 304 , the image decoder 306 , the pose encoder 312 , the pose decoder 314 , the depth encoder 322 , and the depth decoder 324 .</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the latent space 330</span>
        <span itemprop="definition">comprises an encoded latent space vector that is utilized for each of an image, pose vector data of an image, and a depth map of an image.</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN 301</span>
        <span itemprop="definition">is capable of determining any suitable output e.g. reconstructed pose vector data 316 and/or a reconstructed depth map 326 based on an RGB image 302 input.</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each of the encoders</span>
        <span itemprop="definition">including the image encoder 304 , the pose encoder 312 , and the depth encoder 322 is configured to map an input into a compressed latent representation at the latent space 330 .</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">each of the decoders</span>
        <span itemprop="definition">including the image decoder 306 , the pose decoder 314 , and the depth decoder 324 are configured to decode the compressed latent representation of the input from the latent space 330 .</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the decoders of the VAE-GAN 301</span>
        <span itemprop="definition">further include a GAN that is configured to generate an output based on the decoded version of the input.</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">illustrates a schematic block diagram of a process 400 of determining a depth map of an environment, according to one embodiment.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the process 400</span>
        <span itemprop="definition">is implemented in a depth decoder 324 that comprises a GAN configured to generate a reconstructed depth map 326 .</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a similar process 400</span>
        <span itemprop="definition">may be implemented in a pose decoder 314 that comprises a GAN that is configured to generate reconstructed pose vector data 316 .</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the process 400</span>
        <span itemprop="definition">includes receiving an RGB image 402 and feeding the RGB image 402 to a generative adversarial network (hereinafter âGANâ) generator 404 .</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">generates a depth map 406 based on the RGB image 402 .</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GAN</span>
        <span itemprop="definition">generative adversarial network</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a generative adversarial network (âGANâ) discriminator 408</span>
        <span itemprop="definition">receives the RGB image 402 (i.e. the original image) and the depth map 406 generated by the GAN generator 404 .</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN discriminator 408</span>
        <span itemprop="definition">is configured to distinguish real and fake image pairs 410 , e.g. genuine images received from a camera versus depth map images generated by the GAN generator 404 .</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the RGB image 402</span>
        <span itemprop="definition">is received from a monocular camera and may be received from the monocular camera in real-time.</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the monocular camera</span>
        <span itemprop="definition">is attached to a moving device, such as a vehicle, and each RGB image 402 is captured when the monocular camera is in a unique position or is in a unique pose.</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the monocular camera</span>
        <span itemprop="definition">is attached to an exterior of a vehicle and provides the RGB image 402 to a vehicle controller, and the vehicle controller is in communication with the GAN generator 404 .</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN</span>
        <span itemprop="definition">(i.e. the combination of the GAN generator 404 and the GAN discriminator 408 ) comprises a deep neural network architecture comprising two adversarial nets in a zero-sum game framework.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">is configured to generate new data instances and the GAN discriminator 408 is configured to evaluate the new data instances for authenticity.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN discriminator 408</span>
        <span itemprop="definition">is configured to analyze the new data instances and determine whether each new data instance belongs to the actual training data sets or if it was generated artificially (see 410 ).</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">is configured to create new images that are passed to the GAN discriminator 408 and the GAN generator 404 is trained to generate images that fool the GAN discriminator 408 into determining that an artificial new data instance belongs to the actual training data.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">receives an RGB image 402 and returns a depth map 406 based on the RGB image 402 .</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the depth map 406</span>
        <span itemprop="definition">is fed to the GAN discriminator 408 alongside a stream of camera images from an actual dataset, and the GAN discriminator 408 determines a prediction of authenticity for each image, i.e. whether the image is a camera image from the actual dataset or a depth map 406 generated by the GAN generator 404 .</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN</span>
        <span itemprop="definition">includes a double feedback loop wherein the GAN discriminator 408 is in a feedback loop with the ground truth of the images and the GAN generator 404 is in a feedback loop with the GAN discriminator 408 .</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN discriminator 408</span>
        <span itemprop="definition">is a convolutional neural network configured to categorize images fed to it and the GAN generator 404 is an inverse convolutional neural network.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">both the GAN generator 404 and the GAN discriminator 408</span>
        <span itemprop="definition">are seeking to optimize a different and opposing objective function or loss function.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">changes its behavior, so does the GAN discriminator 408 , and vice versa.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the losses of the GAN generator 404 and the GAN discriminator 408</span>
        <span itemprop="definition">push against each other to improve the outputs of the GAN.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">is pretrained offline before the GAN generator 404 receives an RGB image 402 from a monocular camera.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN discriminator 408</span>
        <span itemprop="definition">is pretrained before the GAN generator 404 is trained and this may provide a clearer gradient.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">is trained using a known dataset as the initial training data for the GAN discriminator 408 .</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">may be seeded with a randomized input that is sampled from a predefined latent space, and thereafter, samples synthesized by the GAN generator 404 are evaluated by the GAN discriminator 408 .</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">circumvents the bottleneck for information commonly found in an encoder-decoder network known in the art.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">includes skip connections between each layer of the GAN generator 404 , wherein each skip connection concatenates all channels of the GAN generator 404 .</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">is optimized by alternating between one gradient descent step on the adversarial network then one step on the GAN generator 404 .</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the generator net</span>
        <span itemprop="definition">is run in the same manner as during the training phase.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">instance normalization</span>
        <span itemprop="definition">is applied to the GAN generator 404 , wherein dropout is applied at test time and batch normalization is applied using statistics of the test batch rather than aggregated statistics of the training batch.</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN</span>
        <span itemprop="definition">comprises an encoder-decoder architecture as illustrated in FIG. 4 .</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">receives the RGB image 402 and generates the depth map 406 .</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN discriminator 408</span>
        <span itemprop="definition">distinguishes between a pair comprising an RGB image 402 and a depth map 406 .</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404 and the GAN discriminator 408</span>
        <span itemprop="definition">are trained alternatively until the GAN discriminator 408 cannot tell the difference between an RGB image 402 and a depth map 406 . This can encourage the GAN generator 404 to generate depth maps that are as close to ground truth as possible.</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the depth map 406</span>
        <span itemprop="definition">constitute image-to-image translation that is carried out by the GAN generator 404 and based on the RGB image 402 .</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">learns a mapping from a random noise vector z to determine the depth map 406 output image.</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404</span>
        <span itemprop="definition">is trained to produce outputs that cannot be distinguished from real images by an adversarial GAN discriminator 408 .</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an adversarial GAN discriminator 408</span>
        <span itemprop="definition">learns to classify between an RGB image 402 and a depth map 406 , and the GAN generator 404 is trained to fool the adversarial GAN discriminator 408 .</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">both the adversarial GAN discriminator 408 and the GAN generator 404</span>
        <span itemprop="definition">observe the depth map 406 output images.</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the input images</span>
        <span itemprop="definition">i.e. the RGB image 402 and the output images, i.e. the depth map 406 differ in surface appearance but both include a rendering of the same underlying structure.</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">structure in the RGB image 402</span>
        <span itemprop="definition">is roughly aligned with structure in the depth map 406 .</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the GAN generator 404 architecture</span>
        <span itemprop="definition">is designed around this consideration.</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">illustrates a schematic flow chart diagram of a method 500 for localizing a vehicle in an environment and mapping the environment of the vehicle.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 500</span>
        <span itemprop="definition">may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system 102 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 500</span>
        <span itemprop="definition">begins and the computing device receives an image from a camera of a vehicle at 502 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">provides the image to a variational autoencoder generative adversarial network (VAE-GAN) at 504 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">receives from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image at 506 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">calculates simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map at 508 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs (see 510 ).</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">illustrates a schematic flow chart diagram of a method 600 for localizing a vehicle in an environment and mapping the environment of the vehicle.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 100</span>
        <span itemprop="definition">may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system 102 .</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 600</span>
        <span itemprop="definition">begins and the computing device receives an image from a camera of a vehicle at 602 .</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing devices</span>
        <span itemprop="definition">provides the image to a variational autoencoder generative adversarial network (VAE-GAN) at 604 .</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">is such that the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of an image encoder, an image decoder, a pose encoder, a pose decoder, a depth encoder, and a depth decoder are trained utilizing a single latent space of the VAE-GAN (see 606 ).</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">is such that the VEA-GAN comprises a trained image encoder configured to receive the image, a trained pose decoder comprising a GAN configured to generate reconstructed pose vector data based on the image, and a trained depth decoder comprising a GAN configured to generate a reconstructed depth map based on the image (see 608 ).</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">receives from the VAE-GAN the reconstructed pose vector data based on the image at 610 .</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">calculates simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map at 614 .</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">illustrates a schematic flow chart diagram of a method 700 for training a VAE-GAN.</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 700</span>
        <span itemprop="definition">may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/assistance system 102 .</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method 700</span>
        <span itemprop="definition">begins and the computing device provides a training image to an image encoder of a variational autoencoder generative adversarial network (VAE-GAN) at 702 .</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">provides training pose vector data based on the training image to a pose encoder of the VAE-GAN at 704 .</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing devices</span>
        <span itemprop="definition">provides a training depth map based on the training image to a depth encoder of the VAE-GAN at 706 .</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">is such that the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of the image encoder, the pose encoder, and the depth encoder are trained in tandem utilizing a latent space of the VAE-GAN (see 708 ).</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">is such that the VAE-GAN comprises an encoded latent space vector applicable to each of the training image, the training pose vector data, and the training depth map (see 710 ).</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computing device 800</span>
        <span itemprop="definition">may be used to perform various procedures, such as those discussed herein.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device 800</span>
        <span itemprop="definition">can function as a neural network such as a GAN generator 404 , a vehicle controller such as an autonomous driving/assistance system 102 , a VAE-GAN 201 , a server, and the like.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computing device 800</span>
        <span itemprop="definition">can perform various monitoring functions as discussed herein, and can execute one or more application programs, such as the application programs or functionality described herein.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computing device 800</span>
        <span itemprop="definition">can be any of a wide variety of computing devices, such as a desktop computer, in-dash computer, vehicle control system, a notebook computer, a server computer, a handheld computer, tablet computer and the like.</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computing device 800</span>
        <span itemprop="definition">includes one or more processor(s) 802 , one or more memory device(s) 804 , one or more interface(s) 806 , one or more mass storage device(s) 808 , one or more Input/output (I/O) device(s) 810 , and a display device 830 all of which are coupled to a bus 812 .</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Processor(s) 802</span>
        <span itemprop="definition">include one or more processors or controllers that execute instructions stored in memory device(s) 804 and/or mass storage device(s) 808 .</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Processor(s) 802</span>
        <span itemprop="definition">may also include various types of computer-readable media, such as cache memory.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Memory device(s) 804</span>
        <span itemprop="definition">include various computer-readable media, such as volatile memory (e.g., random access memory (RAM) 814 ) and/or nonvolatile memory (e.g., read-only memory (ROM) 816 ). Memory device(s) 804 may also include rewritable ROM, such as Flash memory.</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">volatile memory</span>
        <span itemprop="definition">e.g., random access memory (RAM) 814</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">nonvolatile memory</span>
        <span itemprop="definition">e.g., read-only memory (ROM) 816</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Memory device(s) 804</span>
        <span itemprop="definition">may also include rewritable ROM, such as Flash memory.</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Mass storage device(s) 808</span>
        <span itemprop="definition">include various computer readable media, such as magnetic tapes, magnetic disks, optical disks, solid-state memory (e.g., Flash memory), and so forth. As shown in FIG. 8 , a particular mass storage device is a hard disk drive 824 . Various drives may also be included in mass storage device(s) 808 to enable reading from and/or writing to the various computer readable media. Mass storage device(s) 808 include removable media 826 and/or non-removable media.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">I/O device(s) 810</span>
        <span itemprop="definition">include various devices that allow data and/or other information to be input to or retrieved from computing device 800 .</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example I/O device(s) 810</span>
        <span itemprop="definition">include cursor control devices, keyboards, keypads, microphones, monitors or other display devices, speakers, printers, network interface cards, modems, and the like.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Display device 830</span>
        <span itemprop="definition">includes any type of device capable of displaying information to one or more users of computing device 800 .</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Examples of display device 830</span>
        <span itemprop="definition">include a monitor, display terminal, video projection device, and the like.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Interface(s) 806</span>
        <span itemprop="definition">include various interfaces that allow computing device 800 to interact with other systems, devices, or computing environments.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example interface(s) 806</span>
        <span itemprop="definition">may include any number of different network interfaces 820 , such as interfaces to local area networks (LANs), wide area networks (WANs), wireless networks, and the Internet.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Other interface(s)</span>
        <span itemprop="definition">include user interface 818 and peripheral device interface 822 .</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the interface(s) 806</span>
        <span itemprop="definition">may also include one or more user interface elements 818 .</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the interface(s) 806</span>
        <span itemprop="definition">may also include one or more peripheral interfaces such as interfaces for printers, pointing devices (mice, track pad, or any suitable user interface now known to those of ordinary skill in the field, or later discovered), keyboards, and the like.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Bus 812</span>
        <span itemprop="definition">allows processor(s) 802 , memory device(s) 804 , interface(s) 806 , mass storage device(s) 808 , and I/O device(s) 810 to communicate with one another, as well as other devices or components coupled to bus 812 .</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Bus 812</span>
        <span itemprop="definition">represents one or more of several types of bus structures, such as a system bus, PCI bus, IEEE bus, USB bus, and so forth.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">programs and other executable program components</span>
        <span itemprop="definition">are shown herein as discrete blocks, although it is understood that such programs and components may reside at various times in different storage components of computing device 800 and are executed by processor(s) 802 .</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the systems and procedures described herein</span>
        <span itemprop="definition">can be implemented in hardware, or a combination of hardware, software, and/or firmware.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">one or more application specific integrated circuits (ASICs)</span>
        <span itemprop="definition">can be programmed to carry out one or more of the systems and procedures described herein.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 1</span>
        <span itemprop="definition">is a method for simultaneous localization and mapping of a robot in an environment.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">includes: receiving an image from a camera of a vehicle; providing the image to a variational autoencoder generative adversarial network (VAE-GAN); receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 2</span>
        <span itemprop="definition">is a method as in Example 1, further comprising training the VAE-GAN, wherein training the VAE-GAN comprises: providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation; providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to the compressed latent representation; and providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to the compressed latent representation.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 3</span>
        <span itemprop="definition">is a method as in any of Examples 1-2, wherein the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of: the image encoder and the image decoder; the pose encoder and the pose decoder; and the depth encoder and the depth decoder are trained in tandem utilizing the latent space of the VAE-GAN.</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 4</span>
        <span itemprop="definition">is a method as in any of Examples 1-3, wherein each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 5</span>
        <span itemprop="definition">is a method as in any of Examples 1-4, wherein the VAE-GAN comprises an encoded latent space vector that is applicable to each of the training image, the training pose vector data, and the training depth map.</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 6</span>
        <span itemprop="definition">is a method as in any of Examples 1-5, further comprising determining the training pose vector data based on the training image, wherein determining the training pose vector data comprises: receiving a plurality of stereo images forming a stereo image sequence; and calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry; wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 7</span>
        <span itemprop="definition">is a method as in any of Examples 1-6, wherein the camera of the vehicle comprises a monocular camera configured to capture a sequence of images of an environment of the vehicle, and wherein the image comprises a red-green-blue (RGB) image.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the camera of the vehicle</span>
        <span itemprop="definition">comprises a monocular camera configured to capture a sequence of images of an environment of the vehicle, and wherein the image comprises a red-green-blue (RGB) image.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">RGB</span>
        <span itemprop="definition">red-green-blue</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 8</span>
        <span itemprop="definition">is a method as in any of Examples 1-7, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GAN</span>
        <span itemprop="definition">generative adversarial network</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 9</span>
        <span itemprop="definition">is a method as in any of Examples 1-8, wherein the VAE-GAN comprises: a trained image encoder configured to receive the image; a trained pose decoder comprising a GAN configured to generate the reconstructed pose vector data based on the image; and a trained depth decoder comprising a GAN configured to generate the reconstructed depth map based on the image.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 10</span>
        <span itemprop="definition">is a method as in any of Examples 1-9, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 11</span>
        <span itemprop="definition">is a method as in any of Examples 1-10, wherein the latent space of the VAE-GAN comprises an encoded latent space vector utilized for each of the image encoder, the pose decoder, and the depth decoder.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 12</span>
        <span itemprop="definition">is a method as in any of Examples 1-11, wherein the reconstructed pose vector data comprises six Degree of Freedom pose data pertaining to the camera of the vehicle.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 13</span>
        <span itemprop="definition">is non-transitory computer-readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to: receive an image from a camera of a vehicle; provide the image to a variational autoencoder generative adversarial network (VAE-GAN); receive from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and calculate simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 14</span>
        <span itemprop="definition">is non-transitory computer-readable storage media as in Example 13, wherein the instructions further cause the one or more processors to train the VAE-GAN, wherein training the VAE-GAN comprises: providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation; providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to the compressed latent representation; and providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to the compressed latent representation.</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 15</span>
        <span itemprop="definition">is non-transitory computer-readable storage media as in any of Examples 13-14, wherein the instructions cause the one or more processors to train the VAE-GAN utilizing a plurality of inputs in tandem, such that each of: the image encoder and the image decoder; the pose encoder and the pose decoder; and the depth encoder and the depth decoder are trained in tandem such that each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</span>
        <meta itemprop="num_attr" content="0085">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 16</span>
        <span itemprop="definition">is non-transitory computer-readable storage media as in any of Examples 13-15, the instructions further cause the one or more processors to calculate the training pose vector data based on the training image, wherein calculating the training pose vector data comprises: receiving a plurality of stereo images forming a stereo image sequence; and calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry; wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 17</span>
        <span itemprop="definition">is non-transitory computer-readable storage media as in any of Examples 13-16, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GAN</span>
        <span itemprop="definition">generative adversarial network</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 18</span>
        <span itemprop="definition">is a system for simultaneous localization and mapping of a vehicle in an environment, the system comprising: a monocular camera of a vehicle; a vehicle controller in communication with the monocular camera, wherein the vehicle controller comprises non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to: receive an image from the monocular camera of the vehicle; provide the image to a variational autoencoder generative adversarial network (VAE-GAN); receive from the VAE-GAN reconstructed pose vector data based on the image; receive from the VAE-GAN a reconstructed depth map based on the image; and calculate simultaneous localization and mapping for the vehicle based on one or more of the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">VAE-GAN</span>
        <span itemprop="definition">variational autoencoder generative adversarial network</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 19</span>
        <span itemprop="definition">is a system as in Example 18, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 20</span>
        <span itemprop="definition">is a system as in any of Examples 18-19, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the VAE-GAN</span>
        <span itemprop="definition">comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Example 21</span>
        <span itemprop="definition">is a system or device that includes means for implementing a method, system, or device as in any of Examples 1-20.</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Implementations of the systems, devices, and methods disclosed herein</span>
        <span itemprop="definition">may comprise or utilize a special purpose or general-purpose computer including computer hardware, such as, for example, one or more processors and system memory, as discussed herein. Implementations within the scope of the present disclosure may also include physical and other computer-readable media for carrying or storing computer-executable instructions and/or data structures. Such computer-readable media can be any available media that can be accessed by a general purpose or special purpose computer system. Computer-readable media that store computer-executable instructions are computer storage media (devices). Computer-readable media that carry computer-executable instructions are transmission media. Thus, by way of example, and not limitation, implementations of the disclosure can comprise at least two distinctly different kinds of computer-readable media: computer storage media (devices) and transmission media.</span>
        <meta itemprop="num_attr" content="0093">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computer storage media</span>
        <span itemprop="definition">includes RAM, ROM, EEPROM, CD-ROM, solid state drives (âSSDsâ) (e.g., based on RAM), Flash memory, phase-change memory (âPCMâ), other types of memory, other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium, which can be used to store desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer.</span>
        <meta itemprop="num_attr" content="0094">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SSDs</span>
        <span itemprop="definition">solid state drives</span>
        <meta itemprop="num_attr" content="0094">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">PCM</span>
        <span itemprop="definition">phase-change memory</span>
        <meta itemprop="num_attr" content="0094">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An implementation of the devices, systems, and methods disclosed herein</span>
        <span itemprop="definition">may communicate over a computer network.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a ânetworkâ</span>
        <span itemprop="definition">is defined as one or more data links that enable the transport of electronic data between computer systems and/or modules and/or other electronic devices.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Transmissions media</span>
        <span itemprop="definition">can include a network and/or data links, which can be used to carry desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer. Combinations of the above should also be included within the scope of computer-readable media.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Computer-executable instructions</span>
        <span itemprop="definition">comprise, for example, instructions and data which, when executed at a processor, cause a general-purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computer executable instructions</span>
        <span itemprop="definition">may be, for example, binaries, intermediate format instructions such as assembly language, or even source code.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the disclosure</span>
        <span itemprop="definition">may be practiced in network computing environments with many types of computer system configurations, including, an in-dash vehicle computer, personal computers, desktop computers, laptop computers, message processors, hand-held devices, multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, mobile telephones, PDAs, tablets, pagers, routers, switches, various storage devices, and the like.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the disclosure</span>
        <span itemprop="definition">may also be practiced in distributed system environments where local and remote computer systems, which are linked (either by hardwired data links, wireless data links, or by a combination of hardwired and wireless data links) through a network, both perform tasks.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">program modules</span>
        <span itemprop="definition">may be located in both local and remote memory storage devices.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">functions described herein</span>
        <span itemprop="definition">can be performed in one or more of: hardware, software, firmware, digital components, or analog components.</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ASICs</span>
        <span itemprop="definition">application specific integrated circuits</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">modules</span>
        <span itemprop="definition">and âcomponentsâ are used in the names of certain components to reflect their implementation independence in software, hardware, circuitry, sensors, or the like. As one skilled in the art will appreciate, components may be referred to by different names. This document does not intend to distinguish between components that differ in name, but not function.</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a sensor</span>
        <span itemprop="definition">may include computer code configured to be executed in one or more processors and may include hardware logic/electrical circuitry controlled by the computer code.</span>
        <meta itemprop="num_attr" content="0099">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">At least some embodiments of the disclosure</span>
        <span itemprop="definition">have been directed to computer program products comprising such logic (e.g., in the form of software) stored on any computer useable medium.</span>
        <meta itemprop="num_attr" content="0100">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such software</span>
        <span itemprop="definition">when executed in one or more data processing devices, causes a device to operate as described herein.</span>
        <meta itemprop="num_attr" content="0100">
      </li>
    </ul>
  </section>

  


  <section itemprop="abstract" itemscope>
    <h2>Abstract</h2>
    
    <div itemprop="content" html><abstract mxw-id="PA368816195" lang="EN" source="national office" load-source="docdb">
    <div class="abstract">The disclosure relates to systems, methods, and devices for simultaneous localization and mapping of a robot in an environment utilizing a variational autoencoder generative adversarial network (VAE-GAN). A method includes receiving an image from a camera of a vehicle and providing the image to a VAE-GAN. The method includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image. The method includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map. The method is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs.</div>
  </abstract>
  </div>
  </section>

  <section itemprop="description" itemscope>
    <h2>Description</h2>
    
    <div itemprop="content" html><ul mxw-id="PDES239256841" lang="EN" load-source="patent-office" class="description">
    
    <heading id="h-0001">TECHNICAL FIELD</heading>
    <li> <para-num num="[0001]"> </para-num> <div id="p-0002" num="0001" class="description-line">The present disclosure relates to methods, systems, and apparatuses for simultaneous localization and mapping of an apparatus in an environment, and particularly relates to simultaneous localization and mapping of a vehicle using a variational autoencoder generative adversarial network.</div>
    </li> <heading id="h-0002">BACKGROUND</heading>
    <li> <para-num num="[0002]"> </para-num> <div id="p-0003" num="0002" class="description-line">Localization, mapping, and depth perception in real-time are requirements for certain autonomous systems, including autonomous driving systems or mobile robotics systems. Each of localization, mapping, and depth perception are key components for carrying out certain tasks such as obstacle avoidance, route planning, mapping, localization, pedestrian detection, and human-robot interaction. Depth perception and localization are traditionally performed by expensive active sensing systems such as LIDAR sensors or passive sensing systems such as binocular vision or stereo cameras.</div>
    </li> <li> <para-num num="[0003]"> </para-num> <div id="p-0004" num="0003" class="description-line">Systems, methods, and devices for computing localization, mapping, and depth perception can be integrated in automobiles such as autonomous vehicles and driving assistance systems. Such systems are currently being developed and deployed to provide safety features, reduce an amount of user input required, or even eliminate user involvement entirely. For example, some driving assistance systems, such as crash avoidance systems, may monitor driving, positions, and a velocity of the vehicle and other objects while a human is driving. When the system detects that a crash or impact is imminent the crash avoidance system may intervene and apply a brake, steer the vehicle, or perform other avoidance or safety maneuvers. As another example, autonomous vehicles may drive, navigate, and/or park a vehicle with little or no user input. However, due to the dangers involved in driving and the costs of vehicles, it is extremely important that autonomous vehicles and driving assistance systems operate safely and are able to accurately navigate roads in a variety of different driving environments.</div>
    
    
    </li> <description-of-drawings>
      <heading id="h-0003">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <li> <para-num num="[0004]"> </para-num> <div id="p-0005" num="0004" class="description-line">Non-limiting and non-exhaustive implementations of the present disclosure are described with reference to the following figures, wherein like reference numerals refer to like parts throughout the various views unless otherwise specified. Advantages of the present disclosure will become better understood with regard to the following description and accompanying drawings where:</div>
      </li> <li> <para-num num="[0005]"> </para-num> <div id="p-0006" num="0005" class="description-line"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a schematic block diagram illustrating an example vehicle control system or autonomous vehicle system, according to one embodiment;</div>
      </li> <li> <para-num num="[0006]"> </para-num> <div id="p-0007" num="0006" class="description-line"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a schematic block diagram of a variational autoencoder generative adversarial network in a training phase, according to one embodiment;</div>
      </li> <li> <para-num num="[0007]"> </para-num> <div id="p-0008" num="0007" class="description-line"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a schematic block diagram of a variational autoencoder generative adversarial network in a computation phase, according to one embodiment;</div>
      </li> <li> <para-num num="[0008]"> </para-num> <div id="p-0009" num="0008" class="description-line"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a schematic block diagram illustrating a process for determining a depth map of an environment, according to one embodiment;</div>
      </li> <li> <para-num num="[0009]"> </para-num> <div id="p-0010" num="0009" class="description-line"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a schematic flow chart diagram of a method for utilizing simultaneous localization and mapping of a vehicle in an environment, according to one embodiment;</div>
      </li> <li> <para-num num="[0010]"> </para-num> <div id="p-0011" num="0010" class="description-line"> <figref idrefs="DRAWINGS">FIG. 6</figref> is a schematic flow chart diagram of a method for utilizing simultaneous localization and mapping of a vehicle in an environment, according to one embodiment;</div>
      </li> <li> <para-num num="[0011]"> </para-num> <div id="p-0012" num="0011" class="description-line"> <figref idrefs="DRAWINGS">FIG. 7</figref> is a schematic flow chart diagram of a method for training a variational autoencoder generative adversarial network, according to one embodiment; and</div>
      </li> <li> <para-num num="[0012]"> </para-num> <div id="p-0013" num="0012" class="description-line"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a schematic block diagram illustrating an example computing system, according to one embodiment.</div>
    </li> </description-of-drawings>
    
    
    <heading id="h-0004">DETAILED DESCRIPTION</heading>
    <li> <para-num num="[0013]"> </para-num> <div id="p-0014" num="0013" class="description-line">Localization of a vehicle along with mapping and depth perception of drivable surfaces or regions is an important aspect of allowing for and improving operation of autonomous vehicle or driver assistance features. For example, a vehicle must know precisely where obstacles or drivable surfaces are located to navigate safely around objects.</div>
    </li> <li> <para-num num="[0014]"> </para-num> <div id="p-0015" num="0014" class="description-line">Simultaneous Localization and Mapping (SLAM) forms the basis for operational functionality of mobile robots, including autonomous vehicles and other mobile robots. Examples of such robots include an indoor mobile robot configured for delivering items in a warehouse or an autonomous drone configured for traversing a building or other environment in a disaster scenario. SLAM is directed to sensing the robot&#39;s environment and building a map of its surroundings as the robot moves through its environment. SLAM is further directed to simultaneously localizing the robot within its environment by extracting pose vector data, including six Degree of Freedom (DoF) poses relative to a starting point of the robot. SLAM thus incrementally generates a map of the robot&#39;s environment. In the case of a robot repeating a route that it has previously mapped, the robot can solve for the localization subset of the problem without generating a new map. The generating of building a map of a new area necessitates SLAM.</div>
    </li> <li> <para-num num="[0015]"> </para-num> <div id="p-0016" num="0015" class="description-line">SLAM is commonly implemented utilizing a depth sensor, such as a LIDAR sensor or a stereo camera. SLAM normally necessitates such devices for enabling the SLAM process to measure the depth and distance of three-dimensional landmarks and to calculate the robot&#39;s position in relation to those landmarks. SLAM may also be implemented using monocular vision, but the depth recovered through triangulation of landmarks from a moving camera over time is up to scale only, such that relative depths of objects in the scene are recovered without absolute depth measurements.</div>
    </li> <li> <para-num num="[0016]"> </para-num> <div id="p-0017" num="0016" class="description-line">Applicant recognizes than allied problem in robots is one of obstacle avoidance. Robots must know how far an object is from the robot such that the robot can determine a collision-free path around the object. Robots utilize LIDAR sensors and stereo camera to determine a dense depth-map of obstacles around the robot. Some of the same obstacles determined through this process may be utilized as three-dimensional landmarks in the SLAM implementation.</div>
    </li> <li> <para-num num="[0017]"> </para-num> <div id="p-0018" num="0017" class="description-line">Applicant has developed systems, methods, and devices for improving operations in both SLAM and obstacle avoidance. Applicant presents systems, methods, and devices for generating a dense depth map for obstacle avoidance, determining a robot&#39;s location, and determining pose vector data as a robot traverses its environment. The systems, methods, and devices of the present disclosure utilize a monocular camera and do not necessitate the use of expensive LIDAR sensors or stereo cameras that further require intensive computing resources. The disclosure presents lightweight, inexpensive, and low-computing methods for sensing a robot&#39;s surrounding, localizing a robot within its environment, and enabling the robot to generate obstacle avoidance movement procedures. Such systems, methods, and devices of the present disclosure may be implemented on any suitable robotics system, including for example, an autonomous vehicle, a mobile robot, and/or a drone or smart mobility vehicle.</div>
    </li> <li> <para-num num="[0018]"> </para-num> <div id="p-0019" num="0018" class="description-line">Variational autoencoders (VAEs) are a class of latent variable models that provide compressed latent representations of data. A VAE can serve as an autoencoder while further serving as a generative model from which new data can be generated by sampling from a latent manifold. The VAE consists of an encoder, which maps the input to a compressed latent representation. The VAE further includes a decoder configured to decode the latent vector back to an output. The entire VAE system may be trained end to end as a deep neural network.</div>
    </li> <li> <para-num num="[0019]"> </para-num> <div id="p-0020" num="0019" class="description-line">The VAE may be configured to encode meaningful information about various data attributes in its latent manifold which can then be exploited to carry out pertinent tasks. In an implementation of the disclosure, Applicant presents utilizing a shared latent space assumption of a VAE between an image, pose vector data of the image, and a depth map of the image, to facilitate the use of SLAM in conjunction with the VAE.</div>
    </li> <li> <para-num num="[0020]"> </para-num> <div id="p-0021" num="0020" class="description-line">Generative adversarial networks (GANs) are a class of generative models configured to produce high quality samples from probability distributions of interest. In the image domain, a GAN may generate output samples of stellar artistic quality. The training methodology for a GAN is adversarial, in that the generator (the network that produces samples, often called âfakesâ) learns by fooling another network called the discriminator that decides whether the samples produced are real or fake. The generator network and the discriminator network are trained in tandem, with the generator network eventually learning to produce samples that succeed in fooling the discriminator network. At such a point, the GAN is able to generate samples from the probability distribution underlying the generative process.</div>
    </li> <li> <para-num num="[0021]"> </para-num> <div id="p-0022" num="0021" class="description-line">Applicant recognizes that VAEs confer advantages in providing latent representations of data for further us. However, one drawback of the VAE is the blurriness of the samples produced. GANs, on the other hand, produce excellent samples but do not have a useful latent representation available. The variational autoencoder generative adversarial network (VAE-GAN) utilizes and combines each system such that one obtains a tractable VAE latent representation while also improving upon the quality of the samples by using a GAN as the generator in the decoder of the VAE. This results in crisper images than a VAE alone.</div>
    </li> <li> <para-num num="[0022]"> </para-num> <div id="p-0023" num="0022" class="description-line">The systems, methods, and devices of the present disclosure utilize the VAE-GAN as the central machinery in the SLAM algorithm. Such systems, methods, and devices receiving an input such as a red-green-blue (RGB) image and outputs corresponding depth maps and pose vector data for the camera that captured the RGB image. The system is trained using a regular stereo visual SLAM pipeline, where stereo visual simultaneous localization and mapping (vSLAM) receives a sequence of stereoscopic images, generates the depth maps and corresponding six Degree of Freedom poses as the stereo camera moves through space. Stereo vSLAM trains the VAE-GAN-SLAM algorithm using a sequence of RGB images, the corresponding depth maps for the images, and the corresponding pose vector data for the images. The VAE-GAN is trained to reconstruct the RGB image, the pose vector data for the image, and the depth map for the image while creating a shared latent space representation of the same. The assumption is that the RGB image, depth map of the image, and pose vector data of the image are sampled from places close together in the real world, are close together in the learnt shared latent space as well. After the networks are trained, the VAE-GAN take as its input an RGB image coming from a monocular camera moving through the same environment and produce both a depth map and pose vector data for the monocular camera.</div>
    </li> <li> <para-num num="[0023]"> </para-num> <div id="p-0024" num="0023" class="description-line">In an embodiment, the latent space representation of the VAE-GAN also enables disentanglement and latent space arithmetic. An example of such an embodiment would be to isolate a dimension in the latent vector responsible for a certain attribute of interest, such as a pose dimension, and create previously unseen view of a scene by changing the pose vector.</div>
    </li> <li> <para-num num="[0024]"> </para-num> <div id="p-0025" num="0024" class="description-line">Applicant recognizes that the systems, methods, and devices disclosed herein enable the use of the system as a SLAM box for facilitating fast and easy single-image inference producing the pose of a robot and the positions of obstacles in the environment around the robot.</div>
    </li> <li> <para-num num="[0025]"> </para-num> <div id="p-0026" num="0025" class="description-line">Generative adversarial networks (GANs) have shown that image-to-image transformation, for instance segmentation or labelling tasks, can be achieved with smaller amounts of training data compared to regular convolutional neural networks by training generative networks and discriminative networks in an adversarial manner. Applicant presents systems, methods, and devices for depth estimation of a single image using a GAN. Such systems, methods, and devices improve performance over known depth estimation systems, and further require a smaller number of training images. The use of GAN as opposed to a regular convolutional neural network enables the collection of a small amount of training data in each environment, typically in the hundreds of images as opposed to the hundreds of thousands of images required by convolutional neural networks. Such systems, methods, and devices reduce the burden for data collection by an order of magnitude.</div>
    </li> <li> <para-num num="[0026]"> </para-num> <div id="p-0027" num="0026" class="description-line">Applicant further presents systems, methods, and devices for depth estimation utilizing visual simultaneous localization and mapping (vSLAM) methods for ensuring temporal consistency in the generated depth maps produced by the GAN as the camera moves through an environment. The vSLAM module provides pose information of the camera, e.g. how much the camera has moved between successive images. Such pose information is provided to the GAN as a temporal constraint on training the GAN to promote the GAN to generate consistent depth maps for successive images.</div>
    </li> <li> <para-num num="[0027]"> </para-num> <div id="p-0028" num="0027" class="description-line">Before the methods, systems, and devices for determining simultaneous localization and mapping for a robot are disclosed and described, it is to be understood that this disclosure is not limited to the configurations, process steps, and materials disclosed herein as such configurations, process steps, and materials may vary somewhat. It is also to be understood that the terminology employed herein is used for describing implementations only and is not intended to be limiting since the scope of the disclosure will be limited only by the appended claims and equivalents thereof.</div>
    </li> <li> <para-num num="[0028]"> </para-num> <div id="p-0029" num="0028" class="description-line">In describing and claiming the disclosure, the following terminology will be used in accordance with the definitions set out below.</div>
    </li> <li> <para-num num="[0029]"> </para-num> <div id="p-0030" num="0029" class="description-line">It must be noted that, as used in this specification and the appended claims, the singular forms âa,â âan,â and âtheâ include plural referents unless the context clearly dictates otherwise.</div>
    </li> <li> <para-num num="[0030]"> </para-num> <div id="p-0031" num="0030" class="description-line">As used herein, the terms âcomprising,â âincluding,â âcontaining,â âcharacterized by,â and grammatical equivalents thereof are inclusive or open-ended terms that do not exclude additional, unrecited elements or method steps.</div>
    </li> <li> <para-num num="[0031]"> </para-num> <div id="p-0032" num="0031" class="description-line">In one embodiment, a method for mapping and localizing a robot, such as an autonomous vehicle, in an environment is disclosed. The method includes receiving an image from a camera of a vehicle. The method includes providing the image to a variational autoencoder generative adversarial network (VAE-GAN). The method includes receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image. The method includes calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map. The method is such that the VAE-GAN comprises a single latent space for encoding a plurality of inputs.</div>
    </li> <li> <para-num num="[0032]"> </para-num> <div id="p-0033" num="0032" class="description-line">Further embodiments and examples will be discussed in relation to the figures below.</div>
    </li> <li> <para-num num="[0033]"> </para-num> <div id="p-0034" num="0033" class="description-line">Referring now to the figures, <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates an example <figure-callout id="100" label="vehicle control system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control system</figure-callout> <b>100</b> that may be used for autonomous or assisted driving. The automated driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b> may be used to automate or control operation of a vehicle or to aid a human driver. For example, the automated driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b> may control one or more of braking, steering, acceleration, lights, alerts, driver notifications, radio, or any other auxiliary systems of the vehicle. In another example, the automated driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b> may not be able to provide any control of the driving (e.g., steering, acceleration, or braking), but may provide notifications and alerts to assist a human driver in driving safely. The automated driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b> may use a neural network, or other model or algorithm to detect or localize objects based on perception data gathered by one or more sensors.</div>
    </li> <li> <para-num num="[0034]"> </para-num> <div id="p-0035" num="0034" class="description-line">The <figure-callout id="100" label="vehicle control system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control system</figure-callout> <b>100</b> also includes one or more sensor systems/devices for detecting a presence of objects near or within a sensor range of a parent vehicle (e.g., a vehicle that includes the vehicle control system <b>100</b>). For example, the <figure-callout id="100" label="vehicle control system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control system</figure-callout> <b>100</b> may include one or <figure-callout id="106" label="more radar systems" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">more radar systems</figure-callout> <b>106</b>, one or <figure-callout id="108" label="more LIDAR systems" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">more LIDAR systems</figure-callout> <b>108</b>, one or <figure-callout id="110" label="more camera systems" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">more camera systems</figure-callout> <b>110</b>, a global positioning system (GPS) <b>112</b>, and/or one or <figure-callout id="114" label="more ultrasound systems" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">more ultrasound systems</figure-callout> <b>114</b>. The <figure-callout id="100" label="vehicle control system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control system</figure-callout> <b>100</b> may include a <figure-callout id="116" label="data store" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">data store</figure-callout> <b>116</b> for storing relevant or useful data for navigation and safety such as map data, driving history or other data. The <figure-callout id="100" label="vehicle control system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control system</figure-callout> <b>100</b> may also include a <figure-callout id="118" label="transceiver" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">transceiver</figure-callout> <b>118</b> for wireless communication with a mobile or wireless network, other vehicles, infrastructure, or any other communication system.</div>
    </li> <li> <para-num num="[0035]"> </para-num> <div id="p-0036" num="0035" class="description-line">The <figure-callout id="100" label="vehicle control system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control system</figure-callout> <b>100</b> may include <figure-callout id="120" label="vehicle control actuators" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control actuators</figure-callout> <b>120</b> to control various aspects of the driving of the vehicle such as electric motors, switches or other actuators, to control braking, acceleration, steering or the like. The <figure-callout id="100" label="vehicle control system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control system</figure-callout> <b>100</b> may also include one or <figure-callout id="122" label="more displays" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">more displays</figure-callout> <b>122</b>, <figure-callout id="124" label="speakers" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">speakers</figure-callout> <b>124</b>, or other devices so that notifications to a human driver or passenger may be provided. A <figure-callout id="122" label="display" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">display</figure-callout> <b>122</b> may include a heads-up display, dashboard display or indicator, a display screen, or any other visual indicator which may be seen by a driver or passenger of a vehicle. A heads-up display may be used to provide notifications or indicate locations of detected objects or overlay instructions or driving maneuvers for assisting a driver. The <figure-callout id="124" label="speakers" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">speakers</figure-callout> <b>124</b> may include one or more speakers of a sound system of a vehicle or may include a speaker dedicated to driver notification.</div>
    </li> <li> <para-num num="[0036]"> </para-num> <div id="p-0037" num="0036" class="description-line">It will be appreciated that the embodiment of <figref idrefs="DRAWINGS">FIG. 1</figref> is given by way of example only. Other embodiments may include fewer or additional components without departing from the scope of the disclosure. Additionally, illustrated components may be combined or included within other components without limitation.</div>
    </li> <li> <para-num num="[0037]"> </para-num> <div id="p-0038" num="0037" class="description-line">In one embodiment, the automated driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b> is configured to control driving or navigation of a parent vehicle. For example, the automated driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b> may control the <figure-callout id="120" label="vehicle control actuators" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">vehicle control actuators</figure-callout> <b>120</b> to drive a path on a road, parking lot, driveway or other location. For example, the automated driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b> may determine a path based on information or perception data provided by any of the components <b>106</b>-<b>114</b>. The sensor systems/devices <b>106</b>-<b>114</b> may be used to obtain real-time sensor data so that the automated driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b> can assist a driver or drive a vehicle in real-time.</div>
    </li> <li> <para-num num="[0038]"> </para-num> <div id="p-0039" num="0038" class="description-line"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates a schematic block diagram of a <figure-callout id="200" label="training phase" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training phase</figure-callout> <b>200</b> of a variational autoencoder generative adversarial network (VAE-GAN) <b>201</b>. The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> includes an <figure-callout id="204" label="image encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image encoder</figure-callout> <b>204</b> and a <figure-callout id="206" label="corresponding image decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">corresponding image decoder</figure-callout> <b>206</b>. The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> includes a <figure-callout id="212" label="pose encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose encoder</figure-callout> <b>212</b> and a corresponding pose decoder <b>214</b>. The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> includes a <figure-callout id="222" label="depth encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth encoder</figure-callout> <b>222</b> and a <figure-callout id="224" label="corresponding depth decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">corresponding depth decoder</figure-callout> <b>224</b>. Each of the <figure-callout id="206" label="image decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image decoder</figure-callout> <b>206</b>, the pose decoder <b>214</b>, and the <figure-callout id="224" label="depth decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth decoder</figure-callout> <b>224</b> includes a generative adversarial network (GAN) that comprises a GAN generator (see e.g. <b>404</b>) and a GAN discriminator (see e.g. <b>408</b>). The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> includes a <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> that is shared by each of the <figure-callout id="204" label="image encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image encoder</figure-callout> <b>204</b>, the <figure-callout id="206" label="image decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image decoder</figure-callout> <b>206</b>, the <figure-callout id="212" label="pose encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose encoder</figure-callout> <b>212</b>, the pose decoder <b>214</b>, the <figure-callout id="222" label="depth encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth encoder</figure-callout> <b>222</b>, and the <figure-callout id="224" label="depth decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth decoder</figure-callout> <b>224</b>. The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> receives a <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> at the <figure-callout id="204" label="image encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image encoder</figure-callout> <b>204</b> and generates a <figure-callout id="208" label="reconstructed image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">reconstructed image</figure-callout> <b>208</b> based on the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b>. The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> receives training <figure-callout id="210" label="pose vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose vector data</figure-callout> <b>210</b> that is based on the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> at the <figure-callout id="212" label="pose encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose encoder</figure-callout> <b>212</b> and the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> generates reconstructed <figure-callout id="216" label="pose vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose vector data</figure-callout> <b>216</b> based on the training pose <figure-callout id="210" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>210</b>. The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> receives a <figure-callout id="220" label="training depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training depth map</figure-callout> <b>220</b> that is based on the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> at the <figure-callout id="222" label="depth encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth encoder</figure-callout> <b>222</b> and outputs a reconstructed <figure-callout id="226" label="depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth map</figure-callout> <b>226</b> that is based on the <figure-callout id="220" label="training depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training depth map</figure-callout> <b>220</b>.</div>
    </li> <li> <para-num num="[0039]"> </para-num> <div id="p-0040" num="0039" class="description-line">The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> is the central machinery in the simultaneous localization and mapping (SLAM) algorithm of the present disclosure. In an embodiment the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> is trained utilizing a regular stereo visual SLAM pipeline. In such an embodiment, a stereo visual SLAM takes a sequence of stereoscopic images and generates depth maps and corresponding six Degrees of Freedom poses for the stereo camera as the camera moves through space. Stereo visual SLAM trains the VAE-GAN-SLAM algorithm using a sequence of red-green-blue (RGB) images where only the left image of a stereo pair is used, along with the corresponding depth maps and six Degrees of Freedom pose vector data for the sequence of RGB images. The VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> is trained under the assumption that the RGB image, the depth map of the image, and the pose vector data of the image are sampled from locations close together in the real world that are also close together in the learnt shared <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> as well. After the networks are trained, the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> can take as its input an RGB image coming from a monocular camera moving through the same environment and produce both a depth map and a six Degree of Freedom pose vector data for the camera.</div>
    </li> <li> <para-num num="[0040]"> </para-num> <div id="p-0041" num="0040" class="description-line">The <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> is provided to the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> for training the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> to generate pose vector data and/or depth map data based on an image. In an embodiment the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> is a red-blue-green (RGB) image captured by a monocular camera. In an embodiment the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> is a single image of a stereo image pair captured by a stereo camera. The <figure-callout id="208" label="reconstructed image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">reconstructed image</figure-callout> <b>208</b> is generated by the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> based on the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b>. The <figure-callout id="204" label="image encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image encoder</figure-callout> <b>204</b> and the <figure-callout id="206" label="image decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image decoder</figure-callout> <b>206</b> are adversarial to one another and are configured to generate the <figure-callout id="208" label="reconstructed image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">reconstructed image</figure-callout> <b>208</b>. The <figure-callout id="204" label="image encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image encoder</figure-callout> <b>204</b> is configured to receiving the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> and map the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> to a compress latent representation in the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b>. The <figure-callout id="206" label="image decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image decoder</figure-callout> <b>206</b> comprises a GAN having a GAN generator and a GAN discriminator. The <figure-callout id="206" label="image decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image decoder</figure-callout> <b>206</b> is configured to decode the compressed latent representation of the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> from the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b>. The GAN of the <figure-callout id="206" label="image decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image decoder</figure-callout> <b>206</b> is configured to generate the <figure-callout id="208" label="reconstructed image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">reconstructed image</figure-callout> <b>208</b>.</div>
    </li> <li> <para-num num="[0041]"> </para-num> <div id="p-0042" num="0041" class="description-line">The training pose <figure-callout id="210" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>210</b> is provided to the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> for training the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> to generate pose vector data of an image. In an embodiment, the training pose <figure-callout id="210" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>210</b> includes six Degree of Freedom pose data of a camera that captured the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b>, wherein the six Degree of Freedom pose data indicates a relative pose of the camera when the image was captured as the camera traversed an environment. The reconstructed pose <figure-callout id="216" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>216</b> is generated by the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> based on the training pose <figure-callout id="210" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>210</b>. The <figure-callout id="212" label="pose encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose encoder</figure-callout> <b>212</b> is configured to receive the training pose <figure-callout id="210" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>210</b> and map the training pose <figure-callout id="210" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>210</b> to a compressed latent representation in the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> of the VEA-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b>. The pose decoder <b>214</b> is configured to decode the compressed latent representation of the training pose <figure-callout id="210" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>210</b> from the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b>. The pose decoder <b>214</b> comprises a GAN that comprises a GAN generator and a GAN discriminator. The GAN of the pose decoder <b>214</b> is configured to generate the reconstructed <figure-callout id="216" label="pose vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose vector data</figure-callout> <b>216</b> based on the training pose <figure-callout id="210" label="vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">vector data</figure-callout> <b>210</b>.</div>
    </li> <li> <para-num num="[0042]"> </para-num> <div id="p-0043" num="0042" class="description-line">The <figure-callout id="220" label="training depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training depth map</figure-callout> <b>220</b> is provided to the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> for training the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> to generate a depth map of an image. In an embodiment, the <figure-callout id="220" label="depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth map</figure-callout> <b>220</b> is based on the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b> and includes depth information for the <figure-callout id="202" label="training image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training image</figure-callout> <b>202</b>. The reconstructed <figure-callout id="226" label="depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth map</figure-callout> <b>226</b> is generated by the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> based on the <figure-callout id="220" label="training depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training depth map</figure-callout> <b>220</b>. The <figure-callout id="222" label="depth encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth encoder</figure-callout> <b>222</b> is configured to receive the <figure-callout id="220" label="training depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training depth map</figure-callout> <b>220</b> and map the <figure-callout id="220" label="training depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training depth map</figure-callout> <b>220</b> to a compressed latent representation in the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> of the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b>. The <figure-callout id="224" label="depth decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth decoder</figure-callout> <b>224</b> comprises a GAN that comprises a GAN generator and a GAN discriminator. The <figure-callout id="224" label="depth decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth decoder</figure-callout> <b>224</b> is configured to decode the compressed latent representation of the <figure-callout id="220" label="training depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training depth map</figure-callout> <b>220</b> from the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b>. The GAN of the <figure-callout id="224" label="depth decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth decoder</figure-callout> <b>224</b> is configured to generate the reconstructed <figure-callout id="226" label="depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth map</figure-callout> <b>226</b> based on the <figure-callout id="220" label="training depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">training depth map</figure-callout> <b>220</b>.</div>
    </li> <li> <para-num num="[0043]"> </para-num> <div id="p-0044" num="0043" class="description-line">The <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> of the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> is shared by each of the <figure-callout id="204" label="image encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image encoder</figure-callout> <b>204</b>, the <figure-callout id="206" label="image decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">image decoder</figure-callout> <b>206</b>, the <figure-callout id="212" label="pose encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose encoder</figure-callout> <b>212</b>, the pose decoder <b>214</b>, the <figure-callout id="222" label="depth encoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth encoder</figure-callout> <b>222</b>, and the <figure-callout id="224" label="depth decoder" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth decoder</figure-callout> <b>224</b>. Thus, the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> is trained to generate each of the <figure-callout id="208" label="reconstructed image" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">reconstructed image</figure-callout> <b>208</b>, the reconstructed <figure-callout id="216" label="pose vector data" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">pose vector data</figure-callout> <b>216</b>, and the reconstructed <figure-callout id="226" label="depth map" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">depth map</figure-callout> <b>226</b> in tandem. In an embodiment, the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> includes an encoded latent space vector applicable to each of an image, pose vector data of an image, and a depth map of an image. The <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> representation of the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> enables disentanglement and latent space arithmetic. An example of the disentanglement and latent space arithmetic includes isolating a dimension in the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> responsible for a certain attribute of interest, such as a posed dimension. This may enable the creation of a previously unseen view of a scheme by changing the pose vector. In an embodiment, training the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> simultaneously for all three attributes, namely the image, the pose vector data, and the depth map, forces the <figure-callout id="230" label="latent space" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">latent space</figure-callout> <b>230</b> to be consistent for each of the attributes. This provides an elegant formulation where the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> is not trained separately for each of an image, pose vector data, and a depth map. Thus, because the VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> is trained in tandem, the trained VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b> may receive an input image and generate any outer output such as pose vector data based on the input image or a depth map based on the input image.</div>
    </li> <li> <para-num num="[0044]"> </para-num> <div id="p-0045" num="0044" class="description-line"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates a schematic block diagram of a computing phase <b>300</b> (alternatively may be referred to as a generative or execution phase) of a variational autoencoder generative adversarial network (VAE-GAN) <b>301</b>. The VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> includes an <figure-callout id="304" label="image encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image encoder</figure-callout> <b>304</b> and a <figure-callout id="306" label="corresponding image decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">corresponding image decoder</figure-callout> <b>306</b>, wherein the <figure-callout id="306" label="image decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image decoder</figure-callout> <b>306</b> comprises a GAN configured to generate a reconstructed image based on the <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b>. In an embodiment as illustrated in <figref idrefs="DRAWINGS">FIG. 3</figref>, the <figure-callout id="304" label="image encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image encoder</figure-callout> <b>304</b> and the <figure-callout id="306" label="image decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image decoder</figure-callout> <b>306</b> have been trained (see <figref idrefs="DRAWINGS">FIG. 2</figref>). The VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> includes a <figure-callout id="312" label="pose encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose encoder</figure-callout> <b>312</b> and a <figure-callout id="314" label="corresponding pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">corresponding pose decoder</figure-callout> <b>314</b>, wherein the <figure-callout id="314" label="pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose decoder</figure-callout> <b>314</b> comprises a GAN configured to generate the reconstructed <figure-callout id="316" label="pose vector data" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose vector data</figure-callout> <b>316</b> based on the <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b>. In an embodiment as illustrated in <figref idrefs="DRAWINGS">FIG. 3</figref>, the <figure-callout id="312" label="pose encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose encoder</figure-callout> <b>312</b> and the <figure-callout id="314" label="pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose decoder</figure-callout> <b>314</b> have been trained (see <figref idrefs="DRAWINGS">FIG. 2</figref>). The VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> includes a <figure-callout id="322" label="depth encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth encoder</figure-callout> <b>322</b> and a <figure-callout id="324" label="corresponding depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">corresponding depth decoder</figure-callout> <b>324</b>, wherein the <figure-callout id="324" label="depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth decoder</figure-callout> <b>324</b> comprises a GAN configured to generate the reconstructed <figure-callout id="326" label="depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth map</figure-callout> <b>326</b> based on the <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b>. In an embodiment as illustrated in <figref idrefs="DRAWINGS">FIG. 3</figref>, the <figure-callout id="322" label="depth encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth encoder</figure-callout> <b>322</b> and the <figure-callout id="324" label="depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth decoder</figure-callout> <b>324</b> have been trained (see <figref idrefs="DRAWINGS">FIG. 2</figref>). The VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> includes a <figure-callout id="330" label="latent space" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">latent space</figure-callout> <b>330</b> that is shared by the <figure-callout id="304" label="image encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image encoder</figure-callout> <b>304</b>, the <figure-callout id="306" label="image decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image decoder</figure-callout> <b>306</b>, the <figure-callout id="312" label="pose encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose encoder</figure-callout> <b>312</b>, the <figure-callout id="314" label="pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose decoder</figure-callout> <b>314</b>, the <figure-callout id="322" label="depth encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth encoder</figure-callout> <b>322</b>, and the <figure-callout id="324" label="depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth decoder</figure-callout> <b>324</b>. The VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> receives an <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b> at the <figure-callout id="304" label="image encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image encoder</figure-callout> <b>304</b>. The VAE-GAN outputs reconstructed pose <figure-callout id="316" label="vector data" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">vector data</figure-callout> <b>316</b> at the trained <figure-callout id="314" label="pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose decoder</figure-callout> <b>314</b>. The VAE-GAN outputs a reconstructed <figure-callout id="326" label="depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth map</figure-callout> <b>326</b> at the trained <figure-callout id="324" label="depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth decoder</figure-callout> <b>324</b>.</div>
    </li> <li> <para-num num="[0045]"> </para-num> <div id="p-0046" num="0045" class="description-line">In an embodiment the <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b> is a red-green-blue image captured by a monocular camera and provided to the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> after the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> has been trained. In an embodiment, the <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b> is captured by a monocular camera of a vehicle, is provided to a vehicle controller, and is provided to the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> in real-time. The <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b> may provide a capture of an environment of the vehicle and may be utilized to determine depth perception for the vehicle surroundings. In such an embodiment the vehicle controller may implement the result of the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> into a SLAM algorithm for computing simultaneous localization and mapping of the vehicle in real-time. The vehicle controller may further provide a notification to a driver, determine a driving maneuver, or execute a driving maneuver based on the results of the SLAM algorithm.</div>
    </li> <li> <para-num num="[0046]"> </para-num> <div id="p-0047" num="0046" class="description-line">The reconstructed pose <figure-callout id="316" label="vector data" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">vector data</figure-callout> <b>316</b> is generated by a GAN embedded in the <figure-callout id="314" label="pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose decoder</figure-callout> <b>314</b> of the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b>. The VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> may be trained to generate the reconstructed <figure-callout id="316" label="pose vector data" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose vector data</figure-callout> <b>316</b> based on a monocular image. In an embodiment as illustrated in <figref idrefs="DRAWINGS">FIG. 3</figref>, the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> includes a <figure-callout id="330" label="latent space" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">latent space</figure-callout> <b>330</b> that is shared by each of an image encoder/decoder, a pose encoder/decoder, and a depth encoder/decoder. The shared <figure-callout id="330" label="latent space" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">latent space</figure-callout> <b>330</b> enables the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> to generate any trained output based on an RGB image <b>302</b> (or non-RGB image) as illustrated. The reconstructed pose <figure-callout id="316" label="vector data" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">vector data</figure-callout> <b>316</b> includes six Degree of Freedom pose data for a monocular camera. The reconstructed pose <figure-callout id="316" label="vector data" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">vector data</figure-callout> <b>316</b> may be utilized by a vehicle to determine a location of the vehicle in its environment and further utilized for simultaneous localization and mapping of the vehicle as it moves through space by implementing the data in a SLAM algorithm.</div>
    </li> <li> <para-num num="[0047]"> </para-num> <div id="p-0048" num="0047" class="description-line">The reconstructed <figure-callout id="326" label="depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth map</figure-callout> <b>326</b> is generated by a GAN embedded in the <figure-callout id="324" label="depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth decoder</figure-callout> <b>324</b> of the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b>. The VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> may be trained to generate the reconstructed <figure-callout id="326" label="depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth map</figure-callout> <b>326</b> based only on the <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b>. The reconstructed <figure-callout id="326" label="depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth map</figure-callout> <b>326</b> provides a dense depth map based on the <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b> and may provide a dense depth map of a surrounding of a robot or autonomous vehicle. The reconstructed <figure-callout id="326" label="depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth map</figure-callout> <b>326</b> may be provided to a SLAM algorithm for calculating simultaneous localization and mapping of a robot as the robot moves through its environment. In an embodiment where the robot is an autonomous vehicle, a vehicle controller may then provide a notification to a driver, determine a driving maneuver, and/or execute a driving maneuver such as an obstacle avoidance maneuver based on the reconstructed <figure-callout id="326" label="depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth map</figure-callout> <b>326</b> and the result of the SLAM algorithm.</div>
    </li> <li> <para-num num="[0048]"> </para-num> <div id="p-0049" num="0048" class="description-line">The <figure-callout id="330" label="latent space" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">latent space</figure-callout> <b>330</b> is shared by each of the <figure-callout id="304" label="image encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image encoder</figure-callout> <b>304</b>, the <figure-callout id="306" label="image decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image decoder</figure-callout> <b>306</b>, the <figure-callout id="312" label="pose encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose encoder</figure-callout> <b>312</b>, the <figure-callout id="314" label="pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose decoder</figure-callout> <b>314</b>, the <figure-callout id="322" label="depth encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth encoder</figure-callout> <b>322</b>, and the <figure-callout id="324" label="depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth decoder</figure-callout> <b>324</b>. In an embodiment the <figure-callout id="330" label="latent space" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">latent space</figure-callout> <b>330</b> comprises an encoded latent space vector that is utilized for each of an image, pose vector data of an image, and a depth map of an image. In such an embodiment, the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> is capable of determining any suitable output e.g. reconstructed pose <figure-callout id="316" label="vector data" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">vector data</figure-callout> <b>316</b> and/or a <figure-callout id="326" label="reconstructed depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">reconstructed depth map</figure-callout> <b>326</b> based on an <figure-callout id="302" label="RGB image" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">RGB image</figure-callout> <b>302</b> input. Each of the encoders, including the <figure-callout id="304" label="image encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image encoder</figure-callout> <b>304</b>, the <figure-callout id="312" label="pose encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose encoder</figure-callout> <b>312</b>, and the <figure-callout id="322" label="depth encoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth encoder</figure-callout> <b>322</b> is configured to map an input into a compressed latent representation at the <figure-callout id="330" label="latent space" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">latent space</figure-callout> <b>330</b>. Conversely, each of the decoders, including the <figure-callout id="306" label="image decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">image decoder</figure-callout> <b>306</b>, the <figure-callout id="314" label="pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose decoder</figure-callout> <b>314</b>, and the <figure-callout id="324" label="depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth decoder</figure-callout> <b>324</b> are configured to decode the compressed latent representation of the input from the <figure-callout id="330" label="latent space" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">latent space</figure-callout> <b>330</b>. The decoders of the VAE-<figure-callout id="301" label="GAN" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">GAN</figure-callout> <b>301</b> further include a GAN that is configured to generate an output based on the decoded version of the input.</div>
    </li> <li> <para-num num="[0049]"> </para-num> <div id="p-0050" num="0049" class="description-line"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates a schematic block diagram of a <figure-callout id="400" label="process" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">process</figure-callout> <b>400</b> of determining a depth map of an environment, according to one embodiment. In an embodiment the <figure-callout id="400" label="process" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">process</figure-callout> <b>400</b> is implemented in a <figure-callout id="324" label="depth decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth decoder</figure-callout> <b>324</b> that comprises a GAN configured to generate a reconstructed <figure-callout id="326" label="depth map" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">depth map</figure-callout> <b>326</b>. It should be appreciated that a <figure-callout id="400" label="similar process" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">similar process</figure-callout> <b>400</b> may be implemented in a <figure-callout id="314" label="pose decoder" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose decoder</figure-callout> <b>314</b> that comprises a GAN that is configured to generate reconstructed <figure-callout id="316" label="pose vector data" filenames="US20200041276A1-20200206-D00000.png,US20200041276A1-20200206-D00003.png" state="{{state}}">pose vector data</figure-callout> <b>316</b>. The <figure-callout id="400" label="process" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">process</figure-callout> <b>400</b> includes receiving an <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> and feeding the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> to a generative adversarial network (hereinafter âGANâ) <figure-callout id="404" label="generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">generator</figure-callout> <b>404</b>. The <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> generates a <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> based on the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b>. A generative adversarial network (âGANâ) <figure-callout id="408" label="discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">discriminator</figure-callout> <b>408</b> receives the RGB image <b>402</b> (i.e. the original image) and the <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> generated by the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>. The <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> is configured to distinguish real and fake image pairs <b>410</b>, e.g. genuine images received from a camera versus depth map images generated by the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>.</div>
    </li> <li> <para-num num="[0050]"> </para-num> <div id="p-0051" num="0050" class="description-line">In an embodiment, the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> is received from a monocular camera and may be received from the monocular camera in real-time. In an embodiment, the monocular camera is attached to a moving device, such as a vehicle, and each <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> is captured when the monocular camera is in a unique position or is in a unique pose. In an embodiment, the monocular camera is attached to an exterior of a vehicle and provides the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> to a vehicle controller, and the vehicle controller is in communication with the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>.</div>
    </li> <li> <para-num num="[0051]"> </para-num> <div id="p-0052" num="0051" class="description-line">The GAN (i.e. the combination of the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> and the GAN discriminator <b>408</b>) comprises a deep neural network architecture comprising two adversarial nets in a zero-sum game framework. In an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is configured to generate new data instances and the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> is configured to evaluate the new data instances for authenticity. In such an embodiment, the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> is configured to analyze the new data instances and determine whether each new data instance belongs to the actual training data sets or if it was generated artificially (see <b>410</b>). The <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is configured to create new images that are passed to the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> and the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is trained to generate images that fool the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> into determining that an artificial new data instance belongs to the actual training data.</div>
    </li> <li> <para-num num="[0052]"> </para-num> <div id="p-0053" num="0052" class="description-line">In an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> receives an <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> and returns a <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> based on the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b>. The <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> is fed to the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> alongside a stream of camera images from an actual dataset, and the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> determines a prediction of authenticity for each image, i.e. whether the image is a camera image from the actual dataset or a <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> generated by the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>. Thus, in such an embodiment, the GAN includes a double feedback loop wherein the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> is in a feedback loop with the ground truth of the images and the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is in a feedback loop with the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b>. In an embodiment, the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> is a convolutional neural network configured to categorize images fed to it and the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is an inverse convolutional neural network. In an embodiment, both the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> and the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> are seeking to optimize a different and opposing objective function or loss function. Thus, as the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> changes its behavior, so does the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b>, and vice versa. The losses of the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> and the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> push against each other to improve the outputs of the GAN.</div>
    </li> <li> <para-num num="[0053]"> </para-num> <div id="p-0054" num="0053" class="description-line">In an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is pretrained offline before the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> receives an <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> from a monocular camera. In an embodiment, the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> is pretrained before the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is trained and this may provide a clearer gradient. In an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is trained using a known dataset as the initial training data for the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b>. The <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> may be seeded with a randomized input that is sampled from a predefined latent space, and thereafter, samples synthesized by the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> are evaluated by the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b>.</div>
    </li> <li> <para-num num="[0054]"> </para-num> <div id="p-0055" num="0054" class="description-line">In an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> circumvents the bottleneck for information commonly found in an encoder-decoder network known in the art. In such an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> includes skip connections between each layer of the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>, wherein each skip connection concatenates all channels of the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>. In an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is optimized by alternating between one gradient descent step on the adversarial network then one step on the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>. At interference time, the generator net is run in the same manner as during the training phase. In an embodiment, instance normalization is applied to the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>, wherein dropout is applied at test time and batch normalization is applied using statistics of the test batch rather than aggregated statistics of the training batch.</div>
    </li> <li> <para-num num="[0055]"> </para-num> <div id="p-0056" num="0055" class="description-line">In an embodiment, the GAN comprises an encoder-decoder architecture as illustrated in <figref idrefs="DRAWINGS">FIG. 4</figref>. In such an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> receives the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> and generates the <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b>. The <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> distinguishes between a pair comprising an <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> and a <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b>. The <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> and the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> are trained alternatively until the <figure-callout id="408" label="GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN discriminator</figure-callout> <b>408</b> cannot tell the difference between an <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> and a <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b>. This can encourage the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> to generate depth maps that are as close to ground truth as possible.</div>
    </li> <li> <para-num num="[0056]"> </para-num> <div id="p-0057" num="0056" class="description-line">The <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> constitute image-to-image translation that is carried out by the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> and based on the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b>. In generating the <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b>, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> learns a mapping from a random noise vector z to determine the <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> output image. The <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is trained to produce outputs that cannot be distinguished from real images by an <figure-callout id="408" label="adversarial GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">adversarial GAN discriminator</figure-callout> <b>408</b>. In an embodiment, an <figure-callout id="408" label="adversarial GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">adversarial GAN discriminator</figure-callout> <b>408</b> learns to classify between an <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> and a <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b>, and the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> is trained to fool the <figure-callout id="408" label="adversarial GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">adversarial GAN discriminator</figure-callout> <b>408</b>. In such an embodiment, both the <figure-callout id="408" label="adversarial GAN discriminator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">adversarial GAN discriminator</figure-callout> <b>408</b> and the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> observe the <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> output images.</div>
    </li> <li> <para-num num="[0057]"> </para-num> <div id="p-0058" num="0057" class="description-line">In an embodiment, the input images, i.e. the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> and the output images, i.e. the <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b> differ in surface appearance but both include a rendering of the same underlying structure. Thus, structure in the <figure-callout id="402" label="RGB image" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">RGB image</figure-callout> <b>402</b> is roughly aligned with structure in the <figure-callout id="406" label="depth map" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">depth map</figure-callout> <b>406</b>. In an embodiment, the <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b> architecture is designed around this consideration.</div>
    </li> <li> <para-num num="[0058]"> </para-num> <div id="p-0059" num="0058" class="description-line"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates a schematic flow chart diagram of a <figure-callout id="500" label="method" filenames="US20200041276A1-20200206-D00005.png" state="{{state}}">method</figure-callout> <b>500</b> for localizing a vehicle in an environment and mapping the environment of the vehicle. The <figure-callout id="500" label="method" filenames="US20200041276A1-20200206-D00005.png" state="{{state}}">method</figure-callout> <b>500</b> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b>. The <figure-callout id="500" label="method" filenames="US20200041276A1-20200206-D00005.png" state="{{state}}">method</figure-callout> <b>500</b> begins and the computing device receives an image from a camera of a vehicle at <b>502</b>. The computing device provides the image to a variational autoencoder generative adversarial network (VAE-GAN) at <b>504</b>. The computing device receives from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image at <b>506</b>. The computing device calculates simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map at <b>508</b>. The VAE-GAN is such that the VAE-GAN comprises a latent space for receiving a plurality of inputs (see <b>510</b>).</div>
    </li> <li> <para-num num="[0059]"> </para-num> <div id="p-0060" num="0059" class="description-line"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates a schematic flow chart diagram of a <figure-callout id="600" label="method" filenames="US20200041276A1-20200206-D00006.png" state="{{state}}">method</figure-callout> <b>600</b> for localizing a vehicle in an environment and mapping the environment of the vehicle. The <figure-callout id="100" label="method" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">method</figure-callout> <b>100</b> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b>. The <figure-callout id="600" label="method" filenames="US20200041276A1-20200206-D00006.png" state="{{state}}">method</figure-callout> <b>600</b> begins and the computing device receives an image from a camera of a vehicle at <b>602</b>. The computing devices provides the image to a variational autoencoder generative adversarial network (VAE-GAN) at <b>604</b>. The VAE-GAN is such that the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of an image encoder, an image decoder, a pose encoder, a pose decoder, a depth encoder, and a depth decoder are trained utilizing a single latent space of the VAE-GAN (see <b>606</b>). The VAE-GAN is such that the VEA-GAN comprises a trained image encoder configured to receive the image, a trained pose decoder comprising a GAN configured to generate reconstructed pose vector data based on the image, and a trained depth decoder comprising a GAN configured to generate a reconstructed depth map based on the image (see <b>608</b>). The computing device receives from the VAE-GAN the reconstructed pose vector data based on the image at <b>610</b>. The computing device receives from the VAE-GAN the reconstructed depth map based on the image at <b>612</b>. The computing device calculates simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map at <b>614</b>.</div>
    </li> <li> <para-num num="[0060]"> </para-num> <div id="p-0061" num="0060" class="description-line"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates a schematic flow chart diagram of a <figure-callout id="700" label="method" filenames="US20200041276A1-20200206-D00007.png" state="{{state}}">method</figure-callout> <b>700</b> for training a VAE-GAN. The <figure-callout id="700" label="method" filenames="US20200041276A1-20200206-D00007.png" state="{{state}}">method</figure-callout> <b>700</b> may be performed by any suitable computing device, including for example a vehicle controller such as an autonomous driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b>. The <figure-callout id="700" label="method" filenames="US20200041276A1-20200206-D00007.png" state="{{state}}">method</figure-callout> <b>700</b> begins and the computing device provides a training image to an image encoder of a variational autoencoder generative adversarial network (VAE-GAN) at <b>702</b>. The computing device provides training pose vector data based on the training image to a pose encoder of the VAE-GAN at <b>704</b>. The computing devices provides a training depth map based on the training image to a depth encoder of the VAE-GAN at <b>706</b>. The VAE-GAN is such that the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of the image encoder, the pose encoder, and the depth encoder are trained in tandem utilizing a latent space of the VAE-GAN (see <b>708</b>). The VAE-GAN is such that the VAE-GAN comprises an encoded latent space vector applicable to each of the training image, the training pose vector data, and the training depth map (see <b>710</b>).</div>
    </li> <li> <para-num num="[0061]"> </para-num> <div id="p-0062" num="0061" class="description-line">Referring now to <figref idrefs="DRAWINGS">FIG. 8</figref>, a block diagram of an <figure-callout id="800" label="example computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">example computing device</figure-callout> <b>800</b> is illustrated. <figure-callout id="800" label="Computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">Computing device</figure-callout> <b>800</b> may be used to perform various procedures, such as those discussed herein. In one embodiment, the <figure-callout id="800" label="computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">computing device</figure-callout> <b>800</b> can function as a neural network such as a <figure-callout id="404" label="GAN generator" filenames="US20200041276A1-20200206-D00004.png" state="{{state}}">GAN generator</figure-callout> <b>404</b>, a vehicle controller such as an autonomous driving/<figure-callout id="102" label="assistance system" filenames="US20200041276A1-20200206-D00001.png" state="{{state}}">assistance system</figure-callout> <b>102</b>, a VAE-<figure-callout id="201" label="GAN" filenames="US20200041276A1-20200206-D00002.png" state="{{state}}">GAN</figure-callout> <b>201</b>, a server, and the like. <figure-callout id="800" label="Computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">Computing device</figure-callout> <b>800</b> can perform various monitoring functions as discussed herein, and can execute one or more application programs, such as the application programs or functionality described herein. <figure-callout id="800" label="Computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">Computing device</figure-callout> <b>800</b> can be any of a wide variety of computing devices, such as a desktop computer, in-dash computer, vehicle control system, a notebook computer, a server computer, a handheld computer, tablet computer and the like.</div>
    </li> <li> <para-num num="[0062]"> </para-num> <div id="p-0063" num="0062" class="description-line"> <figure-callout id="800" label="Computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">Computing device</figure-callout> <b>800</b> includes one or more processor(s) <b>802</b>, one or more memory device(s) <b>804</b>, one or more interface(s) <b>806</b>, one or more mass storage device(s) <b>808</b>, one or more Input/output (I/O) device(s) <b>810</b>, and a <figure-callout id="830" label="display device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">display device</figure-callout> <b>830</b> all of which are coupled to a <figure-callout id="812" label="bus" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">bus</figure-callout> <b>812</b>. Processor(s) <b>802</b> include one or more processors or controllers that execute instructions stored in memory device(s) <b>804</b> and/or mass storage device(s) <b>808</b>. Processor(s) <b>802</b> may also include various types of computer-readable media, such as cache memory.</div>
    </li> <li> <para-num num="[0063]"> </para-num> <div id="p-0064" num="0063" class="description-line">Memory device(s) <b>804</b> include various computer-readable media, such as volatile memory (e.g., random access memory (RAM) <b>814</b>) and/or nonvolatile memory (e.g., read-only memory (ROM) <b>816</b>). Memory device(s) <b>804</b> may also include rewritable ROM, such as Flash memory.</div>
    </li> <li> <para-num num="[0064]"> </para-num> <div id="p-0065" num="0064" class="description-line">Mass storage device(s) <b>808</b> include various computer readable media, such as magnetic tapes, magnetic disks, optical disks, solid-state memory (e.g., Flash memory), and so forth. As shown in <figref idrefs="DRAWINGS">FIG. 8</figref>, a particular mass storage device is a <figure-callout id="824" label="hard disk drive" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">hard disk drive</figure-callout> <b>824</b>. Various drives may also be included in mass storage device(s) <b>808</b> to enable reading from and/or writing to the various computer readable media. Mass storage device(s) <b>808</b> include removable media <b>826</b> and/or non-removable media.</div>
    </li> <li> <para-num num="[0065]"> </para-num> <div id="p-0066" num="0065" class="description-line">I/O device(s) <b>810</b> include various devices that allow data and/or other information to be input to or retrieved from <figure-callout id="800" label="computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">computing device</figure-callout> <b>800</b>. Example I/O device(s) <b>810</b> include cursor control devices, keyboards, keypads, microphones, monitors or other display devices, speakers, printers, network interface cards, modems, and the like.</div>
    </li> <li> <para-num num="[0066]"> </para-num> <div id="p-0067" num="0066" class="description-line"> <figure-callout id="830" label="Display device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">Display device</figure-callout> <b>830</b> includes any type of device capable of displaying information to one or more users of <figure-callout id="800" label="computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">computing device</figure-callout> <b>800</b>. Examples of <figure-callout id="830" label="display device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">display device</figure-callout> <b>830</b> include a monitor, display terminal, video projection device, and the like.</div>
    </li> <li> <para-num num="[0067]"> </para-num> <div id="p-0068" num="0067" class="description-line">Interface(s) <b>806</b> include various interfaces that allow <figure-callout id="800" label="computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">computing device</figure-callout> <b>800</b> to interact with other systems, devices, or computing environments. Example interface(s) <b>806</b> may include any number of different network interfaces <b>820</b>, such as interfaces to local area networks (LANs), wide area networks (WANs), wireless networks, and the Internet. Other interface(s) include user interface <b>818</b> and <figure-callout id="822" label="peripheral device interface" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">peripheral device interface</figure-callout> <b>822</b>. The interface(s) <b>806</b> may also include one or more user interface elements <b>818</b>. The interface(s) <b>806</b> may also include one or more peripheral interfaces such as interfaces for printers, pointing devices (mice, track pad, or any suitable user interface now known to those of ordinary skill in the field, or later discovered), keyboards, and the like.</div>
    </li> <li> <para-num num="[0068]"> </para-num> <div id="p-0069" num="0068" class="description-line"> <figure-callout id="812" label="Bus" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">Bus</figure-callout> <b>812</b> allows processor(s) <b>802</b>, memory device(s) <b>804</b>, interface(s) <b>806</b>, mass storage device(s) <b>808</b>, and I/O device(s) <b>810</b> to communicate with one another, as well as other devices or components coupled to <figure-callout id="812" label="bus" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">bus</figure-callout> <b>812</b>. <figure-callout id="812" label="Bus" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">Bus</figure-callout> <b>812</b> represents one or more of several types of bus structures, such as a system bus, PCI bus, IEEE bus, USB bus, and so forth.</div>
    </li> <li> <para-num num="[0069]"> </para-num> <div id="p-0070" num="0069" class="description-line">For purposes of illustration, programs and other executable program components are shown herein as discrete blocks, although it is understood that such programs and components may reside at various times in different storage components of <figure-callout id="800" label="computing device" filenames="US20200041276A1-20200206-D00008.png" state="{{state}}">computing device</figure-callout> <b>800</b> and are executed by processor(s) <b>802</b>. Alternatively, the systems and procedures described herein can be implemented in hardware, or a combination of hardware, software, and/or firmware. For example, one or more application specific integrated circuits (ASICs) can be programmed to carry out one or more of the systems and procedures described herein.</div>
    </li> <heading id="h-0005">EXAMPLES</heading>
    <li> <para-num num="[0070]"> </para-num> <div id="p-0071" num="0070" class="description-line">The following examples pertain to further embodiments.</div>
    </li> <li> <para-num num="[0071]"> </para-num> <div id="p-0072" num="0071" class="description-line">Example 1 is a method for simultaneous localization and mapping of a robot in an environment. The method includes: receiving an image from a camera of a vehicle; providing the image to a variational autoencoder generative adversarial network (VAE-GAN); receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</div>
    </li> <li> <para-num num="[0072]"> </para-num> <div id="p-0073" num="0072" class="description-line">Example 2 is a method as in Example 1, further comprising training the VAE-GAN, wherein training the VAE-GAN comprises: providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation; providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to the compressed latent representation; and providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to the compressed latent representation.</div>
    </li> <li> <para-num num="[0073]"> </para-num> <div id="p-0074" num="0073" class="description-line">Example 3 is a method as in any of Examples 1-2, wherein the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of: the image encoder and the image decoder; the pose encoder and the pose decoder; and the depth encoder and the depth decoder are trained in tandem utilizing the latent space of the VAE-GAN.</div>
    </li> <li> <para-num num="[0074]"> </para-num> <div id="p-0075" num="0074" class="description-line">Example 4 is a method as in any of Examples 1-3, wherein each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</div>
    </li> <li> <para-num num="[0075]"> </para-num> <div id="p-0076" num="0075" class="description-line">Example 5 is a method as in any of Examples 1-4, wherein the VAE-GAN comprises an encoded latent space vector that is applicable to each of the training image, the training pose vector data, and the training depth map.</div>
    </li> <li> <para-num num="[0076]"> </para-num> <div id="p-0077" num="0076" class="description-line">Example 6 is a method as in any of Examples 1-5, further comprising determining the training pose vector data based on the training image, wherein determining the training pose vector data comprises: receiving a plurality of stereo images forming a stereo image sequence; and calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry; wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</div>
    </li> <li> <para-num num="[0077]"> </para-num> <div id="p-0078" num="0077" class="description-line">Example 7 is a method as in any of Examples 1-6, wherein the camera of the vehicle comprises a monocular camera configured to capture a sequence of images of an environment of the vehicle, and wherein the image comprises a red-green-blue (RGB) image.</div>
    </li> <li> <para-num num="[0078]"> </para-num> <div id="p-0079" num="0078" class="description-line">Example 8 is a method as in any of Examples 1-7, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</div>
    </li> <li> <para-num num="[0079]"> </para-num> <div id="p-0080" num="0079" class="description-line">Example 9 is a method as in any of Examples 1-8, wherein the VAE-GAN comprises: a trained image encoder configured to receive the image; a trained pose decoder comprising a GAN configured to generate the reconstructed pose vector data based on the image; and a trained depth decoder comprising a GAN configured to generate the reconstructed depth map based on the image.</div>
    </li> <li> <para-num num="[0080]"> </para-num> <div id="p-0081" num="0080" class="description-line">Example 10 is a method as in any of Examples 1-9, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</div>
    </li> <li> <para-num num="[0081]"> </para-num> <div id="p-0082" num="0081" class="description-line">Example 11 is a method as in any of Examples 1-10, wherein the latent space of the VAE-GAN comprises an encoded latent space vector utilized for each of the image encoder, the pose decoder, and the depth decoder.</div>
    </li> <li> <para-num num="[0082]"> </para-num> <div id="p-0083" num="0082" class="description-line">Example 12 is a method as in any of Examples 1-11, wherein the reconstructed pose vector data comprises six Degree of Freedom pose data pertaining to the camera of the vehicle.</div>
    </li> <li> <para-num num="[0083]"> </para-num> <div id="p-0084" num="0083" class="description-line">Example 13 is non-transitory computer-readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to: receive an image from a camera of a vehicle; provide the image to a variational autoencoder generative adversarial network (VAE-GAN); receive from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and calculate simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</div>
    </li> <li> <para-num num="[0084]"> </para-num> <div id="p-0085" num="0084" class="description-line">Example 14 is non-transitory computer-readable storage media as in Example 13, wherein the instructions further cause the one or more processors to train the VAE-GAN, wherein training the VAE-GAN comprises: providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation; providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to the compressed latent representation; and providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to the compressed latent representation.</div>
    </li> <li> <para-num num="[0085]"> </para-num> <div id="p-0086" num="0085" class="description-line">Example 15 is non-transitory computer-readable storage media as in any of Examples 13-14, wherein the instructions cause the one or more processors to train the VAE-GAN utilizing a plurality of inputs in tandem, such that each of: the image encoder and the image decoder; the pose encoder and the pose decoder; and the depth encoder and the depth decoder are trained in tandem such that each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</div>
    </li> <li> <para-num num="[0086]"> </para-num> <div id="p-0087" num="0086" class="description-line">Example 16 is non-transitory computer-readable storage media as in any of Examples 13-15, the instructions further cause the one or more processors to calculate the training pose vector data based on the training image, wherein calculating the training pose vector data comprises: receiving a plurality of stereo images forming a stereo image sequence; and calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry; wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</div>
    </li> <li> <para-num num="[0087]"> </para-num> <div id="p-0088" num="0087" class="description-line">Example 17 is non-transitory computer-readable storage media as in any of Examples 13-16, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</div>
    </li> <li> <para-num num="[0088]"> </para-num> <div id="p-0089" num="0088" class="description-line">Example 18 is a system for simultaneous localization and mapping of a vehicle in an environment, the system comprising: a monocular camera of a vehicle; a vehicle controller in communication with the monocular camera, wherein the vehicle controller comprises non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to: receive an image from the monocular camera of the vehicle; provide the image to a variational autoencoder generative adversarial network (VAE-GAN); receive from the VAE-GAN reconstructed pose vector data based on the image; receive from the VAE-GAN a reconstructed depth map based on the image; and calculate simultaneous localization and mapping for the vehicle based on one or more of the reconstructed pose vector data and the reconstructed depth map; wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</div>
    </li> <li> <para-num num="[0089]"> </para-num> <div id="p-0090" num="0089" class="description-line">Example 19 is a system as in Example 18, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</div>
    </li> <li> <para-num num="[0090]"> </para-num> <div id="p-0091" num="0090" class="description-line">Example 20 is a system as in any of Examples 18-19, wherein the VAE-GAN comprises: an image encoder configured to map the image to a compressed latent representation; a pose decoder comprising a GAN generator adversarial to a GAN discriminator; a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</div>
    </li> <li> <para-num num="[0091]"> </para-num> <div id="p-0092" num="0091" class="description-line">Example 21 is a system or device that includes means for implementing a method, system, or device as in any of Examples 1-20.</div>
    </li> <li> <para-num num="[0092]"> </para-num> <div id="p-0093" num="0092" class="description-line">In the above disclosure, reference has been made to the accompanying drawings, which form a part hereof, and in which is shown by way of illustration specific implementations in which the disclosure may be practiced. It is understood that other implementations may be utilized, and structural changes may be made without departing from the scope of the present disclosure. References in the specification to âone embodiment,â âan embodiment,â âan example embodiment,â etc., indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to affect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.</div>
    </li> <li> <para-num num="[0093]"> </para-num> <div id="p-0094" num="0093" class="description-line">Implementations of the systems, devices, and methods disclosed herein may comprise or utilize a special purpose or general-purpose computer including computer hardware, such as, for example, one or more processors and system memory, as discussed herein. Implementations within the scope of the present disclosure may also include physical and other computer-readable media for carrying or storing computer-executable instructions and/or data structures. Such computer-readable media can be any available media that can be accessed by a general purpose or special purpose computer system. Computer-readable media that store computer-executable instructions are computer storage media (devices). Computer-readable media that carry computer-executable instructions are transmission media. Thus, by way of example, and not limitation, implementations of the disclosure can comprise at least two distinctly different kinds of computer-readable media: computer storage media (devices) and transmission media.</div>
    </li> <li> <para-num num="[0094]"> </para-num> <div id="p-0095" num="0094" class="description-line">Computer storage media (devices) includes RAM, ROM, EEPROM, CD-ROM, solid state drives (âSSDsâ) (e.g., based on RAM), Flash memory, phase-change memory (âPCMâ), other types of memory, other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium, which can be used to store desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer.</div>
    </li> <li> <para-num num="[0095]"> </para-num> <div id="p-0096" num="0095" class="description-line">An implementation of the devices, systems, and methods disclosed herein may communicate over a computer network. A ânetworkâ is defined as one or more data links that enable the transport of electronic data between computer systems and/or modules and/or other electronic devices. When information is transferred or provided over a network or another communications connection (either hardwired, wireless, or a combination of hardwired or wireless) to a computer, the computer properly views the connection as a transmission medium. Transmissions media can include a network and/or data links, which can be used to carry desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer. Combinations of the above should also be included within the scope of computer-readable media.</div>
    </li> <li> <para-num num="[0096]"> </para-num> <div id="p-0097" num="0096" class="description-line">Computer-executable instructions comprise, for example, instructions and data which, when executed at a processor, cause a general-purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions. The computer executable instructions may be, for example, binaries, intermediate format instructions such as assembly language, or even source code. Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the described features or acts described above. Rather, the described features and acts are disclosed as example forms of implementing the claims.</div>
    </li> <li> <para-num num="[0097]"> </para-num> <div id="p-0098" num="0097" class="description-line">Those skilled in the art will appreciate that the disclosure may be practiced in network computing environments with many types of computer system configurations, including, an in-dash vehicle computer, personal computers, desktop computers, laptop computers, message processors, hand-held devices, multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, mobile telephones, PDAs, tablets, pagers, routers, switches, various storage devices, and the like. The disclosure may also be practiced in distributed system environments where local and remote computer systems, which are linked (either by hardwired data links, wireless data links, or by a combination of hardwired and wireless data links) through a network, both perform tasks. In a distributed system environment, program modules may be located in both local and remote memory storage devices.</div>
    </li> <li> <para-num num="[0098]"> </para-num> <div id="p-0099" num="0098" class="description-line">Further, where appropriate, functions described herein can be performed in one or more of: hardware, software, firmware, digital components, or analog components. For example, one or more application specific integrated circuits (ASICs) can be programmed to carry out one or more of the systems and procedures described herein. Certain terms are used throughout the description and claims to refer to particular system components. The terms âmodulesâ and âcomponentsâ are used in the names of certain components to reflect their implementation independence in software, hardware, circuitry, sensors, or the like. As one skilled in the art will appreciate, components may be referred to by different names. This document does not intend to distinguish between components that differ in name, but not function.</div>
    </li> <li> <para-num num="[0099]"> </para-num> <div id="p-0100" num="0099" class="description-line">It should be noted that the sensor embodiments discussed above may comprise computer hardware, software, firmware, or any combination thereof to perform at least a portion of their functions. For example, a sensor may include computer code configured to be executed in one or more processors and may include hardware logic/electrical circuitry controlled by the computer code. These example devices are provided herein purposes of illustration and are not intended to be limiting. Embodiments of the present disclosure may be implemented in further types of devices, as would be known to persons skilled in the relevant art(s).</div>
    </li> <li> <para-num num="[0100]"> </para-num> <div id="p-0101" num="0100" class="description-line">At least some embodiments of the disclosure have been directed to computer program products comprising such logic (e.g., in the form of software) stored on any computer useable medium. Such software, when executed in one or more data processing devices, causes a device to operate as described herein.</div>
    </li> <li> <para-num num="[0101]"> </para-num> <div id="p-0102" num="0101" class="description-line">While various embodiments of the present disclosure have been described above, it should be understood that they have been presented by way of example only, and not limitation. It will be apparent to persons skilled in the relevant art that various changes in form and detail can be made therein without departing from the spirit and scope of the disclosure. Thus, the breadth and scope of the present disclosure should not be limited by any of the above-described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents. The foregoing description has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the disclosure to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. Further, it should be noted that any or all of the aforementioned alternate implementations may be used in any combination desired to form additional hybrid implementations of the disclosure.</div>
    </li> <li> <para-num num="[0102]"> </para-num> <div id="p-0103" num="0102" class="description-line">Further, although specific implementations of the disclosure have been described and illustrated, the disclosure is not to be limited to the specific forms or arrangements of parts so described and illustrated. The scope of the disclosure is to be defined by the claims appended hereto, any future claims submitted here and in different applications, and their equivalents.</div>
    
  </li> </ul>
  </div>
  </section>

  <section itemprop="claims" itemscope>
    <h2>Claims (<span itemprop="count">20</span>)</h2>
    
    <div itemprop="content" html><div mxw-id="PCLM233901663" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement>
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text"> <b>1</b>. A method comprising:
<div class="claim-text">receiving an image from a camera of a vehicle;</div> <div class="claim-text">providing the image to a variational autoencoder generative adversarial network (VAE-GAN);</div> <div class="claim-text">receiving from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and</div> <div class="claim-text">calculating simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map;</div> <div class="claim-text">wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text"> <b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising training the VAE-GAN, wherein training the VAE-GAN comprises:
<div class="claim-text">providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation of the training image;</div> <div class="claim-text">providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation of the training pose vector data; and</div> <div class="claim-text">providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation of the training depth map.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text"> <b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the VAE-GAN is trained utilizing a plurality of inputs in tandem, such that each of:
<div class="claim-text">the image encoder and a corresponding image decoder;</div> <div class="claim-text">the pose encoder and a corresponding pose decoder; and</div> <div class="claim-text">the depth encoder and a corresponding depth decoder are trained in tandem utilizing the latent space of the VAE-GAN.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text"> <b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text"> <b>5</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the VAE-GAN comprises an encoded latent space vector that is applicable to each of the training image, the training pose vector data, and the training depth map.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text"> <b>6</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising determining the training pose vector data based on the training image, wherein determining the training pose vector data comprises:
<div class="claim-text">receiving a plurality of stereo images forming a stereo image sequence; and</div> <div class="claim-text">calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry;</div> <div class="claim-text">wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text"> <b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the camera of the vehicle comprises a monocular camera configured to capture a sequence of images of an environment of the vehicle, and wherein the image comprises a red-green-blue (RGB) image.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text"> <b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text"> <b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VAE-GAN comprises:
<div class="claim-text">a trained image encoder configured to receive the image;</div> <div class="claim-text">a trained pose decoder comprising a GAN configured to generate the reconstructed pose vector data based on the image; and</div> <div class="claim-text">a trained depth decoder comprising a GAN configured to generate the reconstructed depth map based on the image.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text"> <b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VAE-GAN comprises:
<div class="claim-text">an image encoder configured to map the image to a compressed latent representation;</div> <div class="claim-text">a pose decoder comprising a GAN generator adversarial to a GAN discriminator;</div> <div class="claim-text">a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and</div> <div class="claim-text">a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text"> <b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the latent space of the VAE-GAN comprises an encoded latent space vector utilized for each of the image encoder, the pose decoder, and the depth decoder.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text"> <b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the reconstructed pose vector data comprises six Degree of Freedom pose data pertaining to the camera of the vehicle.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text"> <b>13</b>. Non-transitory computer-readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to:
<div class="claim-text">receive an image from a camera of a vehicle;</div> <div class="claim-text">provide the image to a variational autoencoder generative adversarial network (VAE-GAN);</div> <div class="claim-text">receive from the VAE-GAN reconstructed pose vector data and a reconstructed depth map based on the image; and</div> <div class="claim-text">calculate simultaneous localization and mapping for the vehicle based on the reconstructed pose vector data and the reconstructed depth map;</div> <div class="claim-text">wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text"> <b>14</b>. The non-transitory computer-readable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions further cause the one or more processors to train the VAE-GAN, wherein training the VAE-GAN comprises:
<div class="claim-text">providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation in the latent space;</div> <div class="claim-text">providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation in the latent space; and</div> <div class="claim-text">providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation in the latent space.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text"> <b>15</b>. The non-transitory computer-readable storage media of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the instructions cause the one or more processors to train the VAE-GAN utilizing a plurality of inputs in tandem, such that each of:
<div class="claim-text">the image encoder and a corresponding image decoder;</div> <div class="claim-text">the pose encoder and a corresponding pose decoder; and</div> <div class="claim-text">the depth encoder and a corresponding depth decoder are trained in tandem such that each of the training image, the training pose vector data, and the training depth map share the latent space of the VAE-GAN.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text"> <b>16</b>. The non-transitory computer-readable storage media of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the instructions further cause the one or more processors to calculate the training pose vector data based on the training image, wherein calculating the training pose vector data comprises:
<div class="claim-text">receiving a plurality of stereo images forming a stereo image sequence; and</div> <div class="claim-text">calculating six Degree of Freedom pose vector data for successive images of the stereo image sequence using stereo visual odometry;</div> <div class="claim-text">wherein the training image provided to the VAE-GAN comprises a single image of a stereo image pair of the stereo image sequence.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text"> <b>17</b>. The non-transitory computer-readable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the VAE-GAN comprises an encoder opposite to a decoder, and wherein the decoder comprises a generative adversarial network (GAN) configured to generate an output, wherein the GAN comprises a GAN generator and a GAN discriminator.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text"> <b>18</b>. A system for simultaneous localization and mapping of a vehicle in an environment, the system comprising:
<div class="claim-text">a monocular camera of a vehicle;</div> <div class="claim-text">a vehicle controller in communication with the monocular camera, wherein the vehicle controller comprises non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to:
<div class="claim-text">receive an image from the monocular camera of the vehicle;</div>
<div class="claim-text">provide the image to a variational autoencoder generative adversarial network (VAE-GAN);</div>
<div class="claim-text">receive from the VAE-GAN reconstructed pose vector data based on the image;</div>
<div class="claim-text">receive from the VAE-GAN a reconstructed depth map based on the image; and</div>
<div class="claim-text">calculate simultaneous localization and mapping for the vehicle based on one or more of the reconstructed pose vector data and the reconstructed depth map;</div>
</div> <div class="claim-text">wherein the VAE-GAN comprises a latent space for receiving a plurality of inputs.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text"> <b>19</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the VAE-GAN is trained and training the VAE-GAN comprises:
<div class="claim-text">providing a training image to an image encoder of the VAE-GAN, wherein the image encoder is configured to map the training image to a compressed latent representation of the training image;</div> <div class="claim-text">providing training pose vector data based on the training image to a pose encoder of the VAE-GAN, wherein the pose encoder is configured to map the training pose vector data to a compressed latent representation of the training pose vector data; and</div> <div class="claim-text">providing a training depth map based on the training image to a depth encoder of the VAE-GAN, wherein the depth encoder is configured to map the training depth map to a compressed latent representation of the training depth map.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text"> <b>20</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the VAE-GAN comprises:
<div class="claim-text">an image encoder configured to map the image to a compressed latent representation;</div> <div class="claim-text">a pose decoder comprising a GAN generator adversarial to a GAN discriminator;</div> <div class="claim-text">a depth decoder comprising a GAN generator adversarial to a GAN discriminator; and</div> <div class="claim-text">a latent space, wherein the late space is common to each of the image encoder, the pose decoder, and the depth decoder.</div> </div>
    </div>
  </div> </div>
  </div>
  </section>

  <section itemprop="application" itemscope>

    <section itemprop="metadata" itemscope>
      <span itemprop="applicationNumber">US16/054,694</span>
      <span itemprop="priorityDate">2018-08-03</span>
      <span itemprop="filingDate">2018-08-03</span>
      <span itemprop="title">End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
     </span>
      <span itemprop="ifiStatus">Abandoned</span>
      
      <a href="/patent/US20200041276A1/en">
        <span itemprop="representativePublication">US20200041276A1</span>
        (<span itemprop="primaryLanguage">en</span>)
      </a>
    </section>

    <h2>Priority Applications (3)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="priorityApps" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US16/054,694</span>
            
            <a href="/patent/US20200041276A1/en">
              <span itemprop="representativePublication">US20200041276A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-08-03</td>
          <td itemprop="filingDate">2018-08-03</td>
          <td itemprop="title">End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
     </td>
        </tr>
        <tr itemprop="priorityApps" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">DE102019120880.7A</span>
            
            <a href="/patent/DE102019120880A1/en">
              <span itemprop="representativePublication">DE102019120880A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-08-03</td>
          <td itemprop="filingDate">2019-08-01</td>
          <td itemprop="title">
  END-TO-END-DEEP-GENERATIVE MODEL FOR SIMULTANEOUS LOCALIZATION AND IMAGE
 
     </td>
        </tr>
        <tr itemprop="priorityApps" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">CN201910716773.2A</span>
            
            <a href="/patent/CN110796692A/en">
              <span itemprop="representativePublication">CN110796692A</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-08-03</td>
          <td itemprop="filingDate">2019-08-05</td>
          <td itemprop="title">End-to-end depth generation model for simultaneous localization and mapping 
       </td>
        </tr>
      </tbody>
    </table>

    <h2>Applications Claiming Priority (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US16/054,694</span>
            <a href="/patent/US20200041276A1/en">
              <span itemprop="representativePublication">US20200041276A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-08-03</td>
          <td itemprop="filingDate">2018-08-03</td>
          <td itemprop="title">End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
     </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Publications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Publication Number</th>
          <th>Publication Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="pubs" itemscope repeat>
          <td>
            <span itemprop="publicationNumber">US20200041276A1</span>
            
            <span itemprop="thisPatent">true</span>
            <a href="/patent/US20200041276A1/en">
              US20200041276A1
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-02-06</td>
        </tr>
      </tbody>
    </table>

  </section>

  <section itemprop="family" itemscope>
    <h1>Family</h1>
    <h2>ID=69168517</h2>

    <h2>Family Applications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Title</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="applications" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US16/054,694</span>
            <span itemprop="ifiStatus">Abandoned</span>
            
            <a href="/patent/US20200041276A1/en">
              <span itemprop="representativePublication">US20200041276A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-08-03</td>
          <td itemprop="filingDate">2018-08-03</td>
          <td itemprop="title">End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
     </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Country Status (3)</h2>
    <table>
      <thead>
        <tr>
          <th>Country</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">US</span>
            (<span itemprop="num">1</span>)
            <meta itemprop="thisCountry" content="true">
          </td>
          <td>
            <a href="/patent/US20200041276A1/en">
              <span itemprop="representativePublication">US20200041276A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">CN</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/CN110796692A/en">
              <span itemprop="representativePublication">CN110796692A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">DE</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/DE102019120880A1/en">
              <span itemprop="representativePublication">DE102019120880A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
      </tbody>
    </table>

    <h2>Cited By (27)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20200226475A1/en">
              <span itemprop="publicationNumber">US20200226475A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-14</td>
          <td itemprop="publicationDate">2020-07-16</td>
          <td><span itemprop="assigneeOriginal">Cambia Health Solutions, Inc.</span></td>
          <td itemprop="title">Systems and methods for continual updating of response generation by an artificial intelligence chatbot 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20200240788A1/en">
              <span itemprop="publicationNumber">US20200240788A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-25</td>
          <td itemprop="publicationDate">2020-07-30</td>
          <td><span itemprop="assigneeOriginal">Dell Products, L.P.</span></td>
          <td itemprop="title">BACKCHANNEL ENCODING FOR VIRTUAL, AUGMENTED, OR MIXED REALITY (xR) APPLICATIONS IN CONNECTIVITY-CONSTRAINED ENVIRONMENTS 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/CN111539988A/en">
              <span itemprop="publicationNumber">CN111539988A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-04-15</td>
          <td itemprop="publicationDate">2020-08-14</td>
          <td><span itemprop="assigneeOriginal">äº¬ä¸æ¹ç§æéå¢è¡ä»½æéå¬å¸</span></td>
          <td itemprop="title">Visual odometer implementation method and device and electronic equipment 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US10762650B1/en">
              <span itemprop="publicationNumber">US10762650B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-09-13</td>
          <td itemprop="publicationDate">2020-09-01</td>
          <td><span itemprop="assigneeOriginal">Toyota Motor Engineering &amp; Manufacturing North America, Inc.</span></td>
          <td itemprop="title">System and method for estimating depth using a monocular camera 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US10817050B2/en">
              <span itemprop="publicationNumber">US10817050B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-01-25</td>
          <td itemprop="publicationDate">2020-10-27</td>
          <td><span itemprop="assigneeOriginal">Dell Products, L.P.</span></td>
          <td itemprop="title">Backchannel resilience for virtual, augmented, or mixed reality (xR) applications in connectivity-constrained environments 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20200384981A1/en">
              <span itemprop="publicationNumber">US20200384981A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-06-10</td>
          <td itemprop="publicationDate">2020-12-10</td>
          <td><span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span></td>
          <td itemprop="title">Methods and apparatuses for operating a self-driving vehicle 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/CN112102399A/en">
              <span itemprop="publicationNumber">CN112102399A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-09-11</td>
          <td itemprop="publicationDate">2020-12-18</td>
          <td><span itemprop="assigneeOriginal">æé½çå·¥å¤§å­¦</span></td>
          <td itemprop="title">Visual mileage calculation method based on generative antagonistic network 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/CN112684704A/en">
              <span itemprop="publicationNumber">CN112684704A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-12-18</td>
          <td itemprop="publicationDate">2021-04-20</td>
          <td><span itemprop="assigneeOriginal">ååçå·¥å¤§å­¦</span></td>
          <td itemprop="title">End-to-end motion control method, system, device and medium based on deep learning 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11037328B1/en">
              <span itemprop="publicationNumber">US11037328B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-12-31</td>
          <td itemprop="publicationDate">2021-06-15</td>
          <td><span itemprop="assigneeOriginal">Lyft, Inc.</span></td>
          <td itemprop="title">Overhead view image generation 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11092966B2/en">
              <span itemprop="publicationNumber">US11092966B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-12-14</td>
          <td itemprop="publicationDate">2021-08-17</td>
          <td><span itemprop="assigneeOriginal">The Boeing Company</span></td>
          <td itemprop="title">Building an artificial-intelligence system for an autonomous vehicle 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20210254991A1/en">
              <span itemprop="publicationNumber">US20210254991A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-02-14</td>
          <td itemprop="publicationDate">2021-08-19</td>
          <td><span itemprop="assigneeOriginal">Amadeus S.A.S.</span></td>
          <td itemprop="title">Method and system for camera assisted map and navigation 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/WO2021177500A1/en">
              <span itemprop="publicationNumber">WO2021177500A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-03-05</td>
          <td itemprop="publicationDate">2021-09-10</td>
          <td><span itemprop="assigneeOriginal">íì±ëíêµ ì°ííë ¥ë¨</span></td>
          <td itemprop="title">Method and system for training self-converging generative network 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11144818B2/en">
              <span itemprop="publicationNumber">US11144818B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-07-05</td>
          <td itemprop="publicationDate">2021-10-12</td>
          <td><span itemprop="assigneeOriginal">Toyota Research Institute, Inc.</span></td>
          <td itemprop="title">Network architecture for ego-motion estimation 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11157774B2/en">
              <span itemprop="publicationNumber">US11157774B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-11-14</td>
          <td itemprop="publicationDate">2021-10-26</td>
          <td><span itemprop="assigneeOriginal">Zoox, Inc.</span></td>
          <td itemprop="title">Depth data model training with upsampling, losses, and loss balancing 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11244500B2/en">
              <span itemprop="publicationNumber">US11244500B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-12-31</td>
          <td itemprop="publicationDate">2022-02-08</td>
          <td><span itemprop="assigneeOriginal">Woven Planet North America, Inc.</span></td>
          <td itemprop="title">Map feature extraction using overhead view images 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11290705B2/en">
              <span itemprop="publicationNumber">US11290705B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-05-11</td>
          <td itemprop="publicationDate">2022-03-29</td>
          <td><span itemprop="assigneeOriginal">Mapbox, Inc.</span></td>
          <td itemprop="title">Rendering augmented reality with occlusion 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11288522B2/en">
              <span itemprop="publicationNumber">US11288522B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-12-31</td>
          <td itemprop="publicationDate">2022-03-29</td>
          <td><span itemprop="assigneeOriginal">Woven Planet North America, Inc.</span></td>
          <td itemprop="title">Generating training data from overhead view images 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11295729B2/en">
              <span itemprop="publicationNumber">US11295729B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-10-02</td>
          <td itemprop="publicationDate">2022-04-05</td>
          <td><span itemprop="assigneeOriginal">Samsung Electronics Co., Ltd.</span></td>
          <td itemprop="title">Response inference method and apparatus 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20220164582A1/en">
              <span itemprop="publicationNumber">US20220164582A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-11-24</td>
          <td itemprop="publicationDate">2022-05-26</td>
          <td><span itemprop="assigneeOriginal">Ford Global Technologies, Llc</span></td>
          <td itemprop="title">Vehicle neural network 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/CN114663496A/en">
              <span itemprop="publicationNumber">CN114663496A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2022-03-23</td>
          <td itemprop="publicationDate">2022-06-24</td>
          <td><span itemprop="assigneeOriginal">åäº¬ç§æå¤§å­¦</span></td>
          <td itemprop="title">Monocular vision odometer method based on Kalman pose estimation network 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11430102B1/en">
              <span itemprop="publicationNumber">US11430102B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-12-07</td>
          <td itemprop="publicationDate">2022-08-30</td>
          <td><span itemprop="assigneeOriginal">Meta Platforms, Inc.</span></td>
          <td itemprop="title">Automated detection of tampered images 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11436807B2/en">
              <span itemprop="publicationNumber">US11436807B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2018-08-08</td>
          <td itemprop="publicationDate">2022-09-06</td>
          <td><span itemprop="assigneeOriginal">Abyssal S.A.</span></td>
          <td itemprop="title">System and method of operation for remotely operated vehicles with improved position estimation 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11455740B2/en">
              <span itemprop="publicationNumber">US11455740B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-08-08</td>
          <td itemprop="publicationDate">2022-09-27</td>
          <td><span itemprop="assigneeOriginal">Abyssal S.A.</span></td>
          <td itemprop="title">System and method of operation for remotely operated vehicles for simultaneous localization and mapping 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11475551B2/en">
              <span itemprop="publicationNumber">US11475551B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2018-08-08</td>
          <td itemprop="publicationDate">2022-10-18</td>
          <td><span itemprop="assigneeOriginal">Abyssal S.A.</span></td>
          <td itemprop="title">System and method of operation for remotely operated vehicles for automatic detection of structure integrity threats 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20230025152A1/en">
              <span itemprop="publicationNumber">US20230025152A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-07-20</td>
          <td itemprop="publicationDate">2023-01-26</td>
          <td><span itemprop="assigneeOriginal">Ford Global Technologies, Llc</span></td>
          <td itemprop="title">Object pose estimation 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/EP4141807A1/en">
              <span itemprop="publicationNumber">EP4141807A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-08-23</td>
          <td itemprop="publicationDate">2023-03-01</td>
          <td><span itemprop="assigneeOriginal">Robert Bosch GmbH</span></td>
          <td itemprop="title">Method and device for generating synthetic training data for an ultrasonic sensor model 
       </td>
        </tr>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US11670088B2/en">
              <span itemprop="publicationNumber">US11670088B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2020-12-07</td>
          <td itemprop="publicationDate">2023-06-06</td>
          <td><span itemprop="assigneeOriginal">Ford Global Technologies, Llc</span></td>
          <td itemprop="title">Vehicle neural network localization 
       </td>
        </tr>
      </tbody>
    </table>

    <h2>Families Citing this family (4)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN113542758A/en">
              <span itemprop="publicationNumber">CN113542758A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-04-15</td>
          <td itemprop="publicationDate">2021-10-22</td>
          <td><span itemprop="assigneeOriginal">è¾è¾¾å¬å¸</span></td>
          <td itemprop="title">Generating antagonistic neural network assisted video compression and broadcast 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN111625519B/en">
              <span itemprop="publicationNumber">CN111625519B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-05-28</td>
          <td itemprop="publicationDate">2021-03-23</td>
          <td><span itemprop="assigneeOriginal">æ¨å</span></td>
          <td itemprop="title">Data complexity-based space vector data modeling method 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/DE102021200374A1/en">
              <span itemprop="publicationNumber">DE102021200374A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2021-01-15</td>
          <td itemprop="publicationDate">2022-07-21</td>
          <td><span itemprop="assigneeOriginal">Volkswagen Aktiengesellschaft</span></td>
          <td itemprop="title">
  Digital representation of a material
 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/DE102021123714A1/en">
              <span itemprop="publicationNumber">DE102021123714A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2021-09-14</td>
          <td itemprop="publicationDate">2023-03-16</td>
          <td><span itemprop="assigneeOriginal">Bayerische Motoren Werke Aktiengesellschaft</span></td>
          <td itemprop="title">
  Evaluate a position determination validator
 
       </td>
        </tr>
      </tbody>
    </table>

    

    

    <ul>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2018</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2018-08-03</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/054,694</span>
            <a href="/patent/US20200041276A1/en"><span itemprop="documentId">patent/US20200041276A1/en</span></a>
            <span itemprop="legalStatusCat">not_active</span>
            <span itemprop="legalStatus">Abandoned</span>
            <span itemprop="thisApp" content="true" bool></span>
          </li>
        </ul>
      </li>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2019</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-08-01</span>
            <span itemprop="countryCode">DE</span>
            <span itemprop="applicationNumber">DE102019120880.7A</span>
            <a href="/patent/DE102019120880A1/en"><span itemprop="documentId">patent/DE102019120880A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-08-05</span>
            <span itemprop="countryCode">CN</span>
            <span itemprop="applicationNumber">CN201910716773.2A</span>
            <a href="/patent/CN110796692A/en"><span itemprop="documentId">patent/CN110796692A/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
        </ul>
      </li>
    </ul>

    </section>

  

  

  <h2>Cited By (36)</h2>
  <table>
    <caption>* Cited by examiner, â  Cited by third party</caption>
    <thead>
      <tr>
        <th>Publication number</th>
        <th>Priority date</th>
        <th>Publication date</th>
        <th>Assignee</th>
        <th>Title</th>
      </tr>
    </thead>
    <tbody>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11436807B2/en">
            <span itemprop="publicationNumber">US11436807B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2018-08-08</td>
        <td itemprop="publicationDate">2022-09-06</td>
        <td><span itemprop="assigneeOriginal">Abyssal S.A.</span></td>
        <td itemprop="title">System and method of operation for remotely operated vehicles with improved position estimation 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11455740B2/en">
            <span itemprop="publicationNumber">US11455740B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2018-08-08</td>
        <td itemprop="publicationDate">2022-09-27</td>
        <td><span itemprop="assigneeOriginal">Abyssal S.A.</span></td>
        <td itemprop="title">System and method of operation for remotely operated vehicles for simultaneous localization and mapping 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11475551B2/en">
            <span itemprop="publicationNumber">US11475551B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2018-08-08</td>
        <td itemprop="publicationDate">2022-10-18</td>
        <td><span itemprop="assigneeOriginal">Abyssal S.A.</span></td>
        <td itemprop="title">System and method of operation for remotely operated vehicles for automatic detection of structure integrity threats 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11430102B1/en">
            <span itemprop="publicationNumber">US11430102B1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2018-12-07</td>
        <td itemprop="publicationDate">2022-08-30</td>
        <td><span itemprop="assigneeOriginal">Meta Platforms, Inc.</span></td>
        <td itemprop="title">Automated detection of tampered images 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11092966B2/en">
            <span itemprop="publicationNumber">US11092966B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2018-12-14</td>
        <td itemprop="publicationDate">2021-08-17</td>
        <td><span itemprop="assigneeOriginal">The Boeing Company</span></td>
        <td itemprop="title">Building an artificial-intelligence system for an autonomous vehicle 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US20230085061A1/en">
            <span itemprop="publicationNumber">US20230085061A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-01-14</td>
        <td itemprop="publicationDate">2023-03-16</td>
        <td><span itemprop="assigneeOriginal">Cambia Health Solutions, Inc.</span></td>
        <td itemprop="title">Systems and methods for continual updating of response generation by an artificial intelligence chatbot 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11823061B2/en">
            <span itemprop="publicationNumber">US11823061B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-01-14</td>
        <td itemprop="publicationDate">2023-11-21</td>
        <td><span itemprop="assigneeOriginal">Cambia Health Solutions, Inc.</span></td>
        <td itemprop="title">Systems and methods for continual updating of response generation by an artificial intelligence chatbot 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11514330B2/en">
            <span itemprop="publicationNumber">US11514330B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-01-14</td>
        <td itemprop="publicationDate">2022-11-29</td>
        <td><span itemprop="assigneeOriginal">Cambia Health Solutions, Inc.</span></td>
        <td itemprop="title">Systems and methods for continual updating of response generation by an artificial intelligence chatbot 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US20200226475A1/en">
            <span itemprop="publicationNumber">US20200226475A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-01-14</td>
        <td itemprop="publicationDate">2020-07-16</td>
        <td><span itemprop="assigneeOriginal">Cambia Health Solutions, Inc.</span></td>
        <td itemprop="title">Systems and methods for continual updating of response generation by an artificial intelligence chatbot 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US10816341B2/en">
            <span itemprop="publicationNumber">US10816341B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-01-25</td>
        <td itemprop="publicationDate">2020-10-27</td>
        <td><span itemprop="assigneeOriginal">Dell Products, L.P.</span></td>
        <td itemprop="title">Backchannel encoding for virtual, augmented, or mixed reality (xR) applications in connectivity-constrained environments 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US10817050B2/en">
            <span itemprop="publicationNumber">US10817050B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-01-25</td>
        <td itemprop="publicationDate">2020-10-27</td>
        <td><span itemprop="assigneeOriginal">Dell Products, L.P.</span></td>
        <td itemprop="title">Backchannel resilience for virtual, augmented, or mixed reality (xR) applications in connectivity-constrained environments 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US20200240788A1/en">
            <span itemprop="publicationNumber">US20200240788A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-01-25</td>
        <td itemprop="publicationDate">2020-07-30</td>
        <td><span itemprop="assigneeOriginal">Dell Products, L.P.</span></td>
        <td itemprop="title">BACKCHANNEL ENCODING FOR VIRTUAL, AUGMENTED, OR MIXED REALITY (xR) APPLICATIONS IN CONNECTIVITY-CONSTRAINED ENVIRONMENTS 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US20200384981A1/en">
            <span itemprop="publicationNumber">US20200384981A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-06-10</td>
        <td itemprop="publicationDate">2020-12-10</td>
        <td><span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span></td>
        <td itemprop="title">Methods and apparatuses for operating a self-driving vehicle 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11447127B2/en">
            <span itemprop="publicationNumber">US11447127B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-06-10</td>
        <td itemprop="publicationDate">2022-09-20</td>
        <td><span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span></td>
        <td itemprop="title">Methods and apparatuses for operating a self-driving vehicle 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11144818B2/en">
            <span itemprop="publicationNumber">US11144818B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-07-05</td>
        <td itemprop="publicationDate">2021-10-12</td>
        <td><span itemprop="assigneeOriginal">Toyota Research Institute, Inc.</span></td>
        <td itemprop="title">Network architecture for ego-motion estimation 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US10762650B1/en">
            <span itemprop="publicationNumber">US10762650B1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-09-13</td>
        <td itemprop="publicationDate">2020-09-01</td>
        <td><span itemprop="assigneeOriginal">Toyota Motor Engineering &amp; Manufacturing North America, Inc.</span></td>
        <td itemprop="title">System and method for estimating depth using a monocular camera 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11295729B2/en">
            <span itemprop="publicationNumber">US11295729B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-10-02</td>
        <td itemprop="publicationDate">2022-04-05</td>
        <td><span itemprop="assigneeOriginal">Samsung Electronics Co., Ltd.</span></td>
        <td itemprop="title">Response inference method and apparatus 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11681046B2/en">
            <span itemprop="publicationNumber">US11681046B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2019-11-14</td>
        <td itemprop="publicationDate">2023-06-20</td>
        <td><span itemprop="assigneeOriginal">Zoox, Inc.</span></td>
        <td itemprop="title">Depth data model training with upsampling, losses and loss balancing 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11157774B2/en">
            <span itemprop="publicationNumber">US11157774B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-11-14</td>
        <td itemprop="publicationDate">2021-10-26</td>
        <td><span itemprop="assigneeOriginal">Zoox, Inc.</span></td>
        <td itemprop="title">Depth data model training with upsampling, losses, and loss balancing 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11037328B1/en">
            <span itemprop="publicationNumber">US11037328B1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2019-12-31</td>
        <td itemprop="publicationDate">2021-06-15</td>
        <td><span itemprop="assigneeOriginal">Lyft, Inc.</span></td>
        <td itemprop="title">Overhead view image generation 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11288522B2/en">
            <span itemprop="publicationNumber">US11288522B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2019-12-31</td>
        <td itemprop="publicationDate">2022-03-29</td>
        <td><span itemprop="assigneeOriginal">Woven Planet North America, Inc.</span></td>
        <td itemprop="title">Generating training data from overhead view images 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11244500B2/en">
            <span itemprop="publicationNumber">US11244500B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2019-12-31</td>
        <td itemprop="publicationDate">2022-02-08</td>
        <td><span itemprop="assigneeOriginal">Woven Planet North America, Inc.</span></td>
        <td itemprop="title">Map feature extraction using overhead view images 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11727601B2/en">
            <span itemprop="publicationNumber">US11727601B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2019-12-31</td>
        <td itemprop="publicationDate">2023-08-15</td>
        <td><span itemprop="assigneeOriginal">Woven Planet North America, Inc.</span></td>
        <td itemprop="title">Overhead view image generation 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US20210254991A1/en">
            <span itemprop="publicationNumber">US20210254991A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2020-02-14</td>
        <td itemprop="publicationDate">2021-08-19</td>
        <td><span itemprop="assigneeOriginal">Amadeus S.A.S.</span></td>
        <td itemprop="title">Method and system for camera assisted map and navigation 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/WO2021177500A1/en">
            <span itemprop="publicationNumber">WO2021177500A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2020-03-05</td>
        <td itemprop="publicationDate">2021-09-10</td>
        <td><span itemprop="assigneeOriginal">íì±ëíêµ ì°ííë ¥ë¨</span></td>
        <td itemprop="title">Method and system for training self-converging generative network 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/CN111539988A/en">
            <span itemprop="publicationNumber">CN111539988A</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2020-04-15</td>
        <td itemprop="publicationDate">2020-08-14</td>
        <td><span itemprop="assigneeOriginal">äº¬ä¸æ¹ç§æéå¢è¡ä»½æéå¬å¸</span></td>
        <td itemprop="title">Visual odometer implementation method and device and electronic equipment 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11290705B2/en">
            <span itemprop="publicationNumber">US11290705B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2020-05-11</td>
        <td itemprop="publicationDate">2022-03-29</td>
        <td><span itemprop="assigneeOriginal">Mapbox, Inc.</span></td>
        <td itemprop="title">Rendering augmented reality with occlusion 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/CN112102399A/en">
            <span itemprop="publicationNumber">CN112102399A</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2020-09-11</td>
        <td itemprop="publicationDate">2020-12-18</td>
        <td><span itemprop="assigneeOriginal">æé½çå·¥å¤§å­¦</span></td>
        <td itemprop="title">Visual mileage calculation method based on generative antagonistic network 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US20220164582A1/en">
            <span itemprop="publicationNumber">US20220164582A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2020-11-24</td>
        <td itemprop="publicationDate">2022-05-26</td>
        <td><span itemprop="assigneeOriginal">Ford Global Technologies, Llc</span></td>
        <td itemprop="title">Vehicle neural network 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11562571B2/en">
            <span itemprop="publicationNumber">US11562571B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2020-11-24</td>
        <td itemprop="publicationDate">2023-01-24</td>
        <td><span itemprop="assigneeOriginal">Ford Global Technologies, Llc</span></td>
        <td itemprop="title">Vehicle neural network 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US11670088B2/en">
            <span itemprop="publicationNumber">US11670088B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2020-12-07</td>
        <td itemprop="publicationDate">2023-06-06</td>
        <td><span itemprop="assigneeOriginal">Ford Global Technologies, Llc</span></td>
        <td itemprop="title">Vehicle neural network localization 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/CN112684704A/en">
            <span itemprop="publicationNumber">CN112684704A</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2020-12-18</td>
        <td itemprop="publicationDate">2021-04-20</td>
        <td><span itemprop="assigneeOriginal">ååçå·¥å¤§å­¦</span></td>
        <td itemprop="title">End-to-end motion control method, system, device and medium based on deep learning 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/US20230025152A1/en">
            <span itemprop="publicationNumber">US20230025152A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2021-07-20</td>
        <td itemprop="publicationDate">2023-01-26</td>
        <td><span itemprop="assigneeOriginal">Ford Global Technologies, Llc</span></td>
        <td itemprop="title">Object pose estimation 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/WO2023025451A1/en">
            <span itemprop="publicationNumber">WO2023025451A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2021-08-23</td>
        <td itemprop="publicationDate">2023-03-02</td>
        <td><span itemprop="assigneeOriginal">Robert Bosch Gmbh</span></td>
        <td itemprop="title">Method and device for generating synthetic training data for an ultrasonic sensor model 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/EP4141807A1/en">
            <span itemprop="publicationNumber">EP4141807A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2021-08-23</td>
        <td itemprop="publicationDate">2023-03-01</td>
        <td><span itemprop="assigneeOriginal">Robert Bosch GmbH</span></td>
        <td itemprop="title">Method and device for generating synthetic training data for an ultrasonic sensor model 
       </td>
      </tr>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          <a href="/patent/CN114663496A/en">
            <span itemprop="publicationNumber">CN114663496A</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2022-03-23</td>
        <td itemprop="publicationDate">2022-06-24</td>
        <td><span itemprop="assigneeOriginal">åäº¬ç§æå¤§å­¦</span></td>
        <td itemprop="title">Monocular vision odometer method based on Kalman pose estimation network 
       </td>
      </tr>
    </tbody>
  </table>

  <section>
    <h2>Also Published As</h2>
    <table>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Publication date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/CN110796692A/en">
              <span itemprop="publicationNumber">CN110796692A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-02-14</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/DE102019120880A1/en">
              <span itemprop="publicationNumber">DE102019120880A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-02-06</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Similar Documents</h2>
    <table>
      <thead>
        <tr>
          <th>Publication</th>
          <th>Publication Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20200041276A1/en">
                <span itemprop="publicationNumber">US20200041276A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-02-06">2020-02-06</time>
            
            
          </td>
          <td itemprop="title">End-To-End Deep Generative Model For Simultaneous Localization And Mapping 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10810754B2/en">
                <span itemprop="publicationNumber">US10810754B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-10-20">2020-10-20</time>
            
            
          </td>
          <td itemprop="title">Simultaneous localization and mapping constraints in generative adversarial networks for monocular depth estimation 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230043931A1/en">
                <span itemprop="publicationNumber">US20230043931A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-02-09">2023-02-09</time>
            
            
          </td>
          <td itemprop="title">Multi-Task Multi-Sensor Fusion for Three-Dimensional Object Detection 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11625856B2/en">
                <span itemprop="publicationNumber">US11625856B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-04-11">2023-04-11</time>
            
            
          </td>
          <td itemprop="title">Localization systems and methods 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN110062934B/en">
                <span itemprop="publicationNumber">CN110062934B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-09-01">2023-09-01</time>
            
            
          </td>
          <td itemprop="title">Determining Structure and Motion in Images Using Neural Networks 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11462023B2/en">
                <span itemprop="publicationNumber">US11462023B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-10-04">2022-10-04</time>
            
            
          </td>
          <td itemprop="title">Systems and methods for 3D object detection 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11064178B2/en">
                <span itemprop="publicationNumber">US11064178B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-07-13">2021-07-13</time>
            
            
          </td>
          <td itemprop="title">Deep virtual stereo odometry 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11100646B2/en">
                <span itemprop="publicationNumber">US11100646B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-08-24">2021-08-24</time>
            
            
          </td>
          <td itemprop="title">Future semantic segmentation prediction using 3D structure 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11562490B2/en">
                <span itemprop="publicationNumber">US11562490B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-01-24">2023-01-24</time>
            
            
          </td>
          <td itemprop="title">Systems and methods for video object segmentation 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11836884B2/en">
                <span itemprop="publicationNumber">US11836884B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-12-05">2023-12-05</time>
            
            
          </td>
          <td itemprop="title">Real-time generation of functional road maps 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN114474061B/en">
                <span itemprop="publicationNumber">CN114474061B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-08-04">2023-08-04</time>
            
            
          </td>
          <td itemprop="title">Cloud service-based multi-sensor fusion positioning navigation system and method for robot 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20220051425A1/en">
                <span itemprop="publicationNumber">US20220051425A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-02-17">2022-02-17</time>
            
            
          </td>
          <td itemprop="title">Scale-aware monocular localization and mapping 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20220036579A1/en">
                <span itemprop="publicationNumber">US20220036579A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-02-03">2022-02-03</time>
            
            
          </td>
          <td itemprop="title">Systems and Methods for Simulating Dynamic Objects Based on Real World Data 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/EP4214682A1/en">
                <span itemprop="publicationNumber">EP4214682A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-07-26">2023-07-26</time>
            
            
          </td>
          <td itemprop="title">Multi-modal 3-d pose estimation 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CA3160651A1/en">
                <span itemprop="publicationNumber">CA3160651A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-05-20">2021-05-20</time>
            
            
          </td>
          <td itemprop="title">Spatio-temporal-interactive networks 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20220153310A1/en">
                <span itemprop="publicationNumber">US20220153310A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-05-19">2022-05-19</time>
            
            
          </td>
          <td itemprop="title">Automatic Annotation of Object Trajectories in Multiple Dimensions 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/JP2022164640A/en">
                <span itemprop="publicationNumber">JP2022164640A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-10-27">2022-10-27</time>
            
            
          </td>
          <td itemprop="title">System and method for dataset and model management for multi-modal auto-labeling and active learning 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="9569268566288177288">
              <a href="/scholar/9569268566288177288"><span itemprop="scholarAuthors">Golovnin et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2020">2020</time>
            
          </td>
          <td itemprop="title">Video processing method for high-definition maps generation</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN112800822A/en">
                <span itemprop="publicationNumber">CN112800822A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-05-14">2021-05-14</time>
            
            
          </td>
          <td itemprop="title">3D automatic tagging with structural and physical constraints 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11887248B2/en">
                <span itemprop="publicationNumber">US11887248B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2024-01-30">2024-01-30</time>
            
            
          </td>
          <td itemprop="title">Systems and methods for reconstructing a scene in three dimensions from a two-dimensional image 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="7348132260416761327">
              <a href="/scholar/7348132260416761327"><span itemprop="scholarAuthors">Bhaggiaraj et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2023">2023</time>
            
          </td>
          <td itemprop="title">Deep Learning Based Self Driving Cars Using Computer Vision</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="9529767806545316404">
              <a href="/scholar/9529767806545316404"><span itemprop="scholarAuthors">Sun et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2018">2018</time>
            
          </td>
          <td itemprop="title">Accurate deep direct geo-localization from ground imagery and phone-grade gps</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20240062386A1/en">
                <span itemprop="publicationNumber">US20240062386A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2024-02-22">2024-02-22</time>
            
            
          </td>
          <td itemprop="title">High throughput point cloud processing 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230342944A1/en">
                <span itemprop="publicationNumber">US20230342944A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-10-26">2023-10-26</time>
            
            
          </td>
          <td itemprop="title">System and Method for Motion Prediction in Autonomous Driving 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="16121206036667138903">
              <a href="/scholar/16121206036667138903"><span itemprop="scholarAuthors">Liu</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2023">2023</time>
            
          </td>
          <td itemprop="title">Application of Object Detection in Autonomous Driving</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Legal Events</h2>
    <table>
      <thead>
        <tr>
          <th>Date</th>
          <th>Code</th>
          <th>Title</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2018-08-03">2018-08-03</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">FORD GLOBAL TECHNOLOGIES, LLC, MICHIGAN</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CHAKRAVARTY, PUNARJAY;NARAYANAN, PRAVEEN;REEL/FRAME:046553/0713</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Effective date</strong>:
              <span itemprop="value">20180612</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-04-15">2021-04-15</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NON FINAL ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-07-20">2021-07-20</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-08-30">2021-08-30</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">FINAL REJECTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-11-06">2021-11-06</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-11-10">2021-11-10</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ADVISORY ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-12-08">2021-12-08</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">DOCKETED NEW CASE - READY FOR EXAMINATION</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-01-03">2022-01-03</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NON FINAL ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-04-06">2022-04-06</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-05-25">2022-05-25</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">FINAL REJECTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-08-09">2022-08-09</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-09-21">2022-09-21</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ADVISORY ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2022-10-21">2022-10-21</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">DOCKETED NEW CASE - READY FOR EXAMINATION</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-05-26">2023-05-26</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-07-28">2023-07-28</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">FINAL REJECTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-09-30">2023-09-30</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-10-10">2023-10-10</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ADVISORY ACTION MAILED</span>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </section>

</article>

    </search-app>
    <script type="text/javascript" src="//www.gstatic.com/feedback/api.js"></script>
    <script type="text/javascript" src="//www.gstatic.com/feedback/js/help/prod/service/lazy.min.js"></script>
    <script type="text/javascript">
      if (window.help && window.help.service) {
        helpApi = window.help.service.Lazy.create(0, {apiKey: 'AIzaSyDTEI_0tLX4varJ7bwK8aT-eOI5qr3BmyI', locale: 'en-US'});
        window.requestedSurveys = new Set();
        window.requestSurvey = function(triggerId) {
          if (window.requestedSurveys.has(triggerId)) {
            return;
          }
          window.requestedSurveys.add(triggerId);
          helpApi.requestSurvey({
            triggerId: triggerId,
            enableTestingMode: false,
            callback: (requestSurveyCallbackParam) => {
              if (!requestSurveyCallbackParam.surveyData) {
                return;
              }
              helpApi.presentSurvey({
                productData: {
                  productVersion: window.version,
                  customData: {
                    "experiments": "72459301,72474719",
                  },
                },
                surveyData: requestSurveyCallbackParam.surveyData,
                colorScheme: 1,
                customZIndex: 10000,
              });
            }
          });
        };

        window.requestSurvey('YXTwAsvoW0kedxbuTdH0RArc9VhT');
      }
    </script>
    <script src="/sw/null_loader.js"></script>
  </body>
</html>
