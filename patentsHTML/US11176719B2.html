<!DOCTYPE html>
<html lang="en">
  <head>
    <title>US11176719B2 - Method and apparatus for localization based on images and map data 
        - Google Patents</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <meta name="referrer" content="origin-when-crossorigin">
    <link rel="canonical" href="https://patents.google.com/patent/US11176719B2/en">
    <meta name="description" content="
     Localization apparatuses and methods are disclosed where a localization apparatus extracts a feature of an object from an input image, generates an image in which the object is projected with respect to localization information of a device based on map data, and evaluates the localization information based on feature values corresponding to vertices included in a projection image. 
   
   ">
    <meta name="DC.type" content="patent">
    <meta name="DC.title" content="Method and apparatus for localization based on images and map data 
       ">
    <meta name="DC.date" content="2019-04-01" scheme="dateSubmitted">
    <meta name="DC.description" content="
     Localization apparatuses and methods are disclosed where a localization apparatus extracts a feature of an object from an input image, generates an image in which the object is projected with respect to localization information of a device based on map data, and evaluates the localization information based on feature values corresponding to vertices included in a projection image. 
   
   ">
    <meta name="citation_patent_application_number" content="US:16/371,305">
    <meta name="citation_pdf_url" content="https://patentimages.storage.googleapis.com/ee/11/2d/fbece7157bb909/US11176719.pdf">
    <meta name="citation_patent_number" content="US:11176719">
    <meta name="DC.date" content="2021-11-16" scheme="issue">
    <meta name="DC.contributor" content="Hyun Sung Chang" scheme="inventor">
    <meta name="DC.contributor" content="Minjung SON" scheme="inventor">
    <meta name="DC.contributor" content="Donghoon SAGONG" scheme="inventor">
    <meta name="DC.contributor" content="Wonhee Lee" scheme="inventor">
    <meta name="DC.contributor" content="Kyungboo JUNG" scheme="inventor">
    <meta name="DC.contributor" content="Samsung Electronics Co Ltd" scheme="assignee">
    <meta name="DC.relation" content="JP:4677981:B2" scheme="references">
    <meta name="DC.relation" content="US:20100098297:A1" scheme="references">
    <meta name="DC.relation" content="JP:5116555:B2" scheme="references">
    <meta name="DC.relation" content="JP:5808369:B2" scheme="references">
    <meta name="DC.relation" content="US:8649565" scheme="references">
    <meta name="DC.relation" content="KR:101139389:B1" scheme="references">
    <meta name="DC.relation" content="US:9811731" scheme="references">
    <meta name="DC.relation" content="US:9524434" scheme="references">
    <meta name="DC.relation" content="US:20180075643:A1" scheme="references">
    <meta name="DC.relation" content="KR:20160128077:A" scheme="references">
    <meta name="DC.relation" content="JP:2017009554:A" scheme="references">
    <meta name="DC.relation" content="US:20180253609:A1" scheme="references">
    <meta name="DC.relation" content="KR:20170070945:A" scheme="references">
    <meta name="DC.relation" content="US:20170169300:A1" scheme="references">
    <meta name="DC.relation" content="KR:20180009280:A" scheme="references">
    <meta name="DC.relation" content="KR:20180069501:A" scheme="references">
    <meta name="DC.relation" content="US:20180283892:A1" scheme="references">
    <meta name="DC.relation" content="US:20180336697:A1" scheme="references">
    <meta name="DC.relation" content="US:20190120947:A1" scheme="references">
    <meta name="DC.relation" content="US:20200249032:A1" scheme="references">
    <meta name="DC.relation" content="US:20200105017:A1" scheme="references">
    <meta name="citation_reference" content="Engel, Jakob, et al., âLSD-SLAM: Large-Scale Direct Monocular SLAMâ, European conference on computer vision, 2014 (pp. 834-849)." scheme="references">
    <meta name="citation_reference" content="Extended European Search Report dated Nov. 7, 2019 in counterpart European Patent Application No. 19182124.8 (7 pages in English)." scheme="references">
    <meta name="citation_reference" content="Kunina, I. A., et al. âAerial Image Geolocalization by Matching Its Line Structure with Route Mapâ, Ninth International Conference on Machine Vision (ICMV 2016), vol. 10341, International Society for Optics and Photonics, Mar. 17, 2017 (8 pages in English)." scheme="references">
    <meta name="citation_reference" content="Lepetit, Vincent et al., âEPnP: An Accurate o (n) Solution to the PnP Problemâ, International journal of computer vision, vol. 81, No. 2, 2009 (12 pages in English)." scheme="references">
    <meta name="citation_reference" content="Lu, Yan et al., âMonocular Localization in Urban Environments using Road Markingsâ, 2017 IEEE Intelligent Vehicles Symposium (IV), Jun. 11-14, 2017 (pp. 468-474)." scheme="references">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Product+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700">

    <style>
      body { transition: none; }
    </style>

    <script>
      window.version = 'patent-search.search_20240108_RC01';

      function sendFeedback() {
        userfeedback.api.startFeedback({
          'productId': '713680',
          'bucket': 'patent-search-web',
          'productVersion': window.version,
        });
      }

      window.experiments = {};
      window.experiments.patentCountries = "ae,ag,al,am,ao,ap,ar,at,au,aw,az,ba,bb,bd,be,bf,bg,bh,bj,bn,bo,br,bw,bx,by,bz,ca,cf,cg,ch,ci,cl,cm,cn,co,cr,cs,cu,cy,cz,dd,de,dj,dk,dm,do,dz,ea,ec,ee,eg,em,ep,es,fi,fr,ga,gb,gc,gd,ge,gh,gm,gn,gq,gr,gt,gw,hk,hn,hr,hu,ib,id,ie,il,in,ir,is,it,jo,jp,ke,kg,kh,km,kn,kp,kr,kw,kz,la,lc,li,lk,lr,ls,lt,lu,lv,ly,ma,mc,md,me,mg,mk,ml,mn,mo,mr,mt,mw,mx,my,mz,na,ne,ng,ni,nl,no,nz,oa,om,pa,pe,pg,ph,pl,pt,py,qa,ro,rs,ru,rw,sa,sc,sd,se,sg,si,sk,sl,sm,sn,st,su,sv,sy,sz,td,tg,th,tj,tm,tn,tr,tt,tw,tz,ua,ug,us,uy,uz,vc,ve,vn,wo,yu,za,zm,zw";
      
      
      window.experiments.keywordWizard = true;
      
      
      
      window.experiments.definitions = true;

      window.Polymer = {
        dom: 'shady',
        lazyRegister: true,
      };
    </script>

    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/webcomponentsjs/webcomponents-lite.min.js"></script>
    <link rel="import" href="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.html">
  </head>
  <body unresolved>
    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.js"></script>
    <search-app>
      
      

      <article class="result" itemscope itemtype="http://schema.org/ScholarlyArticle">
  <h1 itemprop="pageTitle">US11176719B2 - Method and apparatus for localization based on images and map data 
        - Google Patents</h1>
  <span itemprop="title">Method and apparatus for localization based on images and map data 
       </span>

  <meta itemprop="type" content="patent">
  <a href="https://patentimages.storage.googleapis.com/ee/11/2d/fbece7157bb909/US11176719.pdf" itemprop="pdfLink">Download PDF</a>
  <h2>Info</h2>

  <dl>
    <dt>Publication number</dt>
    <dd itemprop="publicationNumber">US11176719B2</dd>
    <meta itemprop="numberWithoutCodes" content="11176719">
    <meta itemprop="kindCode" content="B2">
    <meta itemprop="publicationDescription" content="Patent ( having previously published pre-grant publication)">
    <span>US11176719B2</span>
    <span>US16/371,305</span>
    <span>US201916371305A</span>
    <span>US11176719B2</span>
    <span>US 11176719 B2</span>
    <span>US11176719 B2</span>
    <span>US 11176719B2</span>
    <span>  </span>
    <span> </span>
    <span> </span>
    <span>US 201916371305 A</span>
    <span>US201916371305 A</span>
    <span>US 201916371305A</span>
    <span>US 11176719 B2</span>
    <span>US11176719 B2</span>
    <span>US 11176719B2</span>

    <dt>Authority</dt>
    <dd itemprop="countryCode">US</dd>
    <dd itemprop="countryName">United States</dd>

    <dt>Prior art keywords</dt>
    <dd itemprop="priorArtKeywords" repeat>image</dd>
    <dd itemprop="priorArtKeywords" repeat>localization</dd>
    <dd itemprop="priorArtKeywords" repeat>determining</dd>
    <dd itemprop="priorArtKeywords" repeat>distance</dd>
    <dd itemprop="priorArtKeywords" repeat>vertices</dd>

    <dt>Prior art date</dt>
    <dd><time itemprop="priorArtDate" datetime="2018-10-24">2018-10-24</time></dd>

    <dt>Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)</dt>
    <dd itemprop="legalStatusIfi" itemscope>
      <span itemprop="status">Active</span>, expires <time itemprop="expiration" datetime="2039-05-04">2039-05-04</time>
    </dd>
  </dl>

  <dt>Application number</dt>
  <dd itemprop="applicationNumber">US16/371,305</dd>

  

  <dt>Other versions</dt>
  <dd itemprop="directAssociations" itemscope repeat>
    <a href="/patent/US20200134896A1/en">
      <span itemprop="publicationNumber">US20200134896A1</span>
      (<span itemprop="primaryLanguage">en</span>
    </a>
  </dd>

  <dt>Inventor</dt>
  <dd itemprop="inventor" repeat>Hyun Sung Chang</dd>
  <dd itemprop="inventor" repeat>Minjung SON</dd>
  <dd itemprop="inventor" repeat>Donghoon SAGONG</dd>
  <dd itemprop="inventor" repeat>Wonhee Lee</dd>
  <dd itemprop="inventor" repeat>Kyungboo JUNG</dd>

  <dt>Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)</dt>
  <dd itemprop="assigneeCurrent" repeat>
    Samsung Electronics Co Ltd
  </dd>

  <dt>Original Assignee</dt>
  <dd itemprop="assigneeOriginal" repeat>Samsung Electronics Co Ltd</dd>

  <dt>Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)</dt>
  <dd><time itemprop="priorityDate" datetime="2018-10-24">2018-10-24</time></dd>

  <dt>Filing date</dt>
  <dd><time itemprop="filingDate" datetime="2019-04-01">2019-04-01</time></dd>

  <dt>Publication date</dt>
  <dd><time itemprop="publicationDate" datetime="2021-11-16">2021-11-16</time></dd>

  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2019-04-01">2019-04-01</time>
    <span itemprop="title">Application filed by Samsung Electronics Co Ltd</span>
    <span itemprop="type">filed</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    <span itemprop="assigneeSearch">Samsung Electronics Co Ltd</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2019-04-01">2019-04-01</time>
    <span itemprop="title">Assigned to SAMSUNG ELECTRONCIS CO., LTD.</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">SAMSUNG ELECTRONCIS CO., LTD.</span>
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    <span itemprop="description" repeat>Assignors: CHANG, HYUN SUNG, JUNG, KYUNGBOO, LEE, WONHEE, SAGONG, DONGHOON, SON, MINJUNG</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2020-04-30">2020-04-30</time>
    <span itemprop="title">Publication of US20200134896A1</span>
    <span itemprop="type">publication</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US20200134896A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2021-11-16">2021-11-16</time>
    <span itemprop="title">Application granted</span>
    <span itemprop="type">granted</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2021-11-16">2021-11-16</time>
    <span itemprop="title">Publication of US11176719B2</span>
    <span itemprop="type">publication</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US11176719B2/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date">Status</time>
    <span itemprop="title">Active</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    <span itemprop="current" content="true" bool>Current</span>
    
    
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2039-05-04">2039-05-04</time>
    <span itemprop="title">Adjusted expiration</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
  </dd>

  <h2>Links</h2>
  <ul>
    
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoLink">
          <a href="https://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&p=1&u=/netahtml/PTO/srchnum.html&r=1&f=G&l=50&d=PALL&s1=11176719.PN." itemprop="url" target="_blank"><span itemprop="text">USPTO</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoPatentCenterLink">
          <a href="https://patentcenter.uspto.gov/applications/16371305" itemprop="url" target="_blank"><span itemprop="text">USPTO PatentCenter</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoAssignmentLink">
          <a href="https://assignment.uspto.gov/patent/index.html#/patent/search/resultFilter?searchInput=11176719" itemprop="url" target="_blank"><span itemprop="text">USPTO Assignment</span></a>
        </li>

    <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="espacenetLink">
        <a href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=11176719B2&amp;KC=B2&amp;FT=D" itemprop="url" target="_blank"><span itemprop="text">Espacenet</span></a>
      </li>
      

    

    
      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="globalDossierLink">
        <a href="https://globaldossier.uspto.gov/#/result/patent/US/11176719/1" itemprop="url" target="_blank"><span itemprop="text">Global Dossier</span></a>
      </li>
      

      

      

      

      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="stackexchangeLink">
        <a href="https://patents.stackexchange.com/questions/tagged/US11176719" itemprop="url"><span itemprop="text">Discuss</span></a>
      </li>
      
  </ul>

  

  <section>
    <h2>Images</h2>
    <ul>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/16/7c/a9/c8fe4bf18d9231/US11176719-20211116-D00000.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/c3/ca/6e/7a2b12a5ccd5ac/US11176719-20211116-D00000.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="110">
            <meta itemprop="label" content="object">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="310">
              <meta itemprop="top" content="398">
              <meta itemprop="right" content="353">
              <meta itemprop="bottom" content="419">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="115">
            <meta itemprop="label" content="guidance lane">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="759">
              <meta itemprop="top" content="398">
              <meta itemprop="right" content="802">
              <meta itemprop="bottom" content="417">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="120">
            <meta itemprop="label" content="AR image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="739">
              <meta itemprop="top" content="3">
              <meta itemprop="right" content="781">
              <meta itemprop="bottom" content="26">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="130">
            <meta itemprop="label" content="object">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1287">
              <meta itemprop="top" content="120">
              <meta itemprop="right" content="1331">
              <meta itemprop="bottom" content="142">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="135">
            <meta itemprop="label" content="guidance lane">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1712">
              <meta itemprop="top" content="394">
              <meta itemprop="right" content="1755">
              <meta itemprop="bottom" content="415">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="0">
            <meta itemprop="id" content="140">
            <meta itemprop="label" content="AR image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1680">
              <meta itemprop="top" content="9">
              <meta itemprop="right" content="1723">
              <meta itemprop="bottom" content="31">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/33/34/5b/649040f04b8f6d/US11176719-20211116-D00001.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/97/19/73/f9faffece1e44f/US11176719-20211116-D00001.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="110">
            <meta itemprop="label" content="object">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="577">
              <meta itemprop="top" content="872">
              <meta itemprop="right" content="656">
              <meta itemprop="bottom" content="910">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="115">
            <meta itemprop="label" content="guidance lane">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1393">
              <meta itemprop="top" content="871">
              <meta itemprop="right" content="1471">
              <meta itemprop="bottom" content="906">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="120">
            <meta itemprop="label" content="AR image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1355">
              <meta itemprop="top" content="152">
              <meta itemprop="right" content="1436">
              <meta itemprop="bottom" content="193">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/46/12/f3/5bf6d57cead837/US11176719-20211116-D00002.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/e7/8d/90/e2ef6b453c00dc/US11176719-20211116-D00002.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="130">
            <meta itemprop="label" content="object">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="641">
              <meta itemprop="top" content="367">
              <meta itemprop="right" content="721">
              <meta itemprop="bottom" content="407">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="135">
            <meta itemprop="label" content="guidance lane">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1411">
              <meta itemprop="top" content="868">
              <meta itemprop="right" content="1490">
              <meta itemprop="bottom" content="905">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="140">
            <meta itemprop="label" content="AR image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1357">
              <meta itemprop="top" content="168">
              <meta itemprop="right" content="1435">
              <meta itemprop="bottom" content="205">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/b8/6a/7a/b813a90f78c9a5/US11176719-20211116-D00003.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/ac/f7/60/1b6070956998d8/US11176719-20211116-D00003.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/48/6f/35/46df2eebb93820/US11176719-20211116-D00004.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/1d/43/57/af7338e0a06c4e/US11176719-20211116-D00004.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/ba/29/7f/5d63fd4d001f08/US11176719-20211116-D00005.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/49/40/b0/fc4840fee8add3/US11176719-20211116-D00005.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="localization apparatus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="234">
              <meta itemprop="top" content="1000">
              <meta itemprop="right" content="274">
              <meta itemprop="bottom" content="1085">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="210">
            <meta itemprop="label" content="transform device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="336">
              <meta itemprop="top" content="1544">
              <meta itemprop="right" content="371">
              <meta itemprop="bottom" content="1631">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="220">
            <meta itemprop="label" content="transform device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="334">
              <meta itemprop="top" content="989">
              <meta itemprop="right" content="371">
              <meta itemprop="bottom" content="1076">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="230">
            <meta itemprop="label" content="feature extractor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="849">
              <meta itemprop="top" content="1545">
              <meta itemprop="right" content="885">
              <meta itemprop="bottom" content="1631">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="235">
            <meta itemprop="label" content="F">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1266">
              <meta itemprop="top" content="987">
              <meta itemprop="right" content="1304">
              <meta itemprop="bottom" content="1070">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="240">
            <meta itemprop="label" content="pooler">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="849">
              <meta itemprop="top" content="391">
              <meta itemprop="right" content="886">
              <meta itemprop="bottom" content="474">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="5">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="projects">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="464">
              <meta itemprop="top" content="1669">
              <meta itemprop="right" content="502">
              <meta itemprop="bottom" content="1736">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/e0/c4/10/4cc257206bba37/US11176719-20211116-D00006.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/b7/8b/bf/4cee67c478ae3d/US11176719-20211116-D00006.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="410">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="502">
              <meta itemprop="top" content="348">
              <meta itemprop="right" content="587">
              <meta itemprop="bottom" content="385">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="420">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1118">
              <meta itemprop="top" content="348">
              <meta itemprop="right" content="1201">
              <meta itemprop="bottom" content="386">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="430">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="502">
              <meta itemprop="top" content="548">
              <meta itemprop="right" content="587">
              <meta itemprop="bottom" content="585">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="440">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1727">
              <meta itemprop="top" content="518">
              <meta itemprop="right" content="1809">
              <meta itemprop="bottom" content="558">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="450">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1119">
              <meta itemprop="top" content="548">
              <meta itemprop="right" content="1201">
              <meta itemprop="bottom" content="586">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="460">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="502">
              <meta itemprop="top" content="746">
              <meta itemprop="right" content="585">
              <meta itemprop="bottom" content="786">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="470">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="501">
              <meta itemprop="top" content="948">
              <meta itemprop="right" content="586">
              <meta itemprop="bottom" content="985">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="480">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="502">
              <meta itemprop="top" content="1146">
              <meta itemprop="right" content="585">
              <meta itemprop="bottom" content="1186">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/68/69/61/ac8cc0d2604588/US11176719-20211116-D00007.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/4d/8f/8c/d34dfa93f8078d/US11176719-20211116-D00007.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="1">
            <meta itemprop="label" content="Equation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="557">
              <meta itemprop="top" content="706">
              <meta itemprop="right" content="574">
              <meta itemprop="bottom" content="745">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="510">
            <meta itemprop="label" content="image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="518">
              <meta itemprop="top" content="138">
              <meta itemprop="right" content="602">
              <meta itemprop="bottom" content="174">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="7">
            <meta itemprop="id" content="520">
            <meta itemprop="label" content="image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="518">
              <meta itemprop="top" content="813">
              <meta itemprop="right" content="601">
              <meta itemprop="bottom" content="850">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/1d/71/d6/3b88c834f5485d/US11176719-20211116-D00008.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/87/c6/f2/9476dfefed33df/US11176719-20211116-D00008.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="projects">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="270">
              <meta itemprop="top" content="1118">
              <meta itemprop="right" content="336">
              <meta itemprop="bottom" content="1166">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="3D">
            <meta itemprop="label" content="projects">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="760">
              <meta itemprop="top" content="1172">
              <meta itemprop="right" content="824">
              <meta itemprop="bottom" content="1222">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="610">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1146">
              <meta itemprop="top" content="281">
              <meta itemprop="right" content="1228">
              <meta itemprop="bottom" content="318">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="620">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1146">
              <meta itemprop="top" content="476">
              <meta itemprop="right" content="1229">
              <meta itemprop="bottom" content="512">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="630">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1015">
              <meta itemprop="top" content="736">
              <meta itemprop="right" content="1096">
              <meta itemprop="bottom" content="776">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="640">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1146">
              <meta itemprop="top" content="1033">
              <meta itemprop="right" content="1225">
              <meta itemprop="bottom" content="1073">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="650">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1146">
              <meta itemprop="top" content="1338">
              <meta itemprop="right" content="1228">
              <meta itemprop="bottom" content="1375">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="660">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1146">
              <meta itemprop="top" content="1532">
              <meta itemprop="right" content="1228">
              <meta itemprop="bottom" content="1568">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="670">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1147">
              <meta itemprop="top" content="1724">
              <meta itemprop="right" content="1227">
              <meta itemprop="bottom" content="1762">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="8">
            <meta itemprop="id" content="680">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1146">
              <meta itemprop="top" content="2127">
              <meta itemprop="right" content="1225">
              <meta itemprop="bottom" content="2167">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/b5/d1/4a/be5ed6d4ede10a/US11176719-20211116-D00009.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/4c/d5/03/7b698dc861dab0/US11176719-20211116-D00009.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="710">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="500">
              <meta itemprop="top" content="379">
              <meta itemprop="right" content="581">
              <meta itemprop="bottom" content="417">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="720">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1116">
              <meta itemprop="top" content="378">
              <meta itemprop="right" content="1195">
              <meta itemprop="bottom" content="416">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="730">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="499">
              <meta itemprop="top" content="578">
              <meta itemprop="right" content="581">
              <meta itemprop="bottom" content="617">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="740">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1723">
              <meta itemprop="top" content="552">
              <meta itemprop="right" content="1802">
              <meta itemprop="bottom" content="588">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="750">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1115">
              <meta itemprop="top" content="580">
              <meta itemprop="right" content="1199">
              <meta itemprop="bottom" content="617">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="760">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="499">
              <meta itemprop="top" content="778">
              <meta itemprop="right" content="582">
              <meta itemprop="bottom" content="818">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="9">
            <meta itemprop="id" content="770">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="499">
              <meta itemprop="top" content="979">
              <meta itemprop="right" content="580">
              <meta itemprop="bottom" content="1017">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/a1/f7/81/fa1e56eddc593f/US11176719-20211116-D00010.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/cb/81/af/5538e54d235988/US11176719-20211116-D00010.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="810">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1188">
              <meta itemprop="top" content="305">
              <meta itemprop="right" content="1270">
              <meta itemprop="bottom" content="344">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="820">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1188">
              <meta itemprop="top" content="505">
              <meta itemprop="right" content="1269">
              <meta itemprop="bottom" content="543">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="830">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1187">
              <meta itemprop="top" content="704">
              <meta itemprop="right" content="1271">
              <meta itemprop="bottom" content="744">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="840">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1186">
              <meta itemprop="top" content="902">
              <meta itemprop="right" content="1268">
              <meta itemprop="bottom" content="941">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="10">
            <meta itemprop="id" content="850">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1188">
              <meta itemprop="top" content="1104">
              <meta itemprop="right" content="1269">
              <meta itemprop="bottom" content="1142">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/27/2c/e0/2e87e8846bdbd4/US11176719-20211116-D00011.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/20/53/e4/eb3711b1aea158/US11176719-20211116-D00011.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/5e/2c/0e/e4d8bf7baa97c6/US11176719-20211116-D00012.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/d8/68/34/fca01dfd233465/US11176719-20211116-D00012.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="12">
            <meta itemprop="id" content="910">
            <meta itemprop="label" content="feature map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="782">
              <meta itemprop="top" content="169">
              <meta itemprop="right" content="866">
              <meta itemprop="bottom" content="206">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="12">
            <meta itemprop="id" content="911">
            <meta itemprop="label" content="features">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="511">
              <meta itemprop="top" content="550">
              <meta itemprop="right" content="591">
              <meta itemprop="bottom" content="588">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="12">
            <meta itemprop="id" content="920">
            <meta itemprop="label" content="map data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1342">
              <meta itemprop="top" content="170">
              <meta itemprop="right" content="1425">
              <meta itemprop="bottom" content="206">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="12">
            <meta itemprop="id" content="921">
            <meta itemprop="label" content="vertices">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="108">
              <meta itemprop="top" content="551">
              <meta itemprop="right" content="187">
              <meta itemprop="bottom" content="588">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/1b/f4/e6/e7c9847e434cfb/US11176719-20211116-D00013.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/eb/d2/07/450b1eeed7a858/US11176719-20211116-D00013.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="13">
            <meta itemprop="id" content="910">
            <meta itemprop="label" content="feature map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="751">
              <meta itemprop="top" content="151">
              <meta itemprop="right" content="835">
              <meta itemprop="bottom" content="189">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="13">
            <meta itemprop="id" content="920">
            <meta itemprop="label" content="map data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1310">
              <meta itemprop="top" content="150">
              <meta itemprop="right" content="1394">
              <meta itemprop="bottom" content="189">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/67/e5/cd/a14f1b4ec8f423/US11176719-20211116-D00014.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/f1/f8/96/428a0fc9a04d73/US11176719-20211116-D00014.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="910">
            <meta itemprop="label" content="feature map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="751">
              <meta itemprop="top" content="162">
              <meta itemprop="right" content="835">
              <meta itemprop="bottom" content="201">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="920">
            <meta itemprop="label" content="map data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1310">
              <meta itemprop="top" content="161">
              <meta itemprop="right" content="1393">
              <meta itemprop="bottom" content="199">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="14">
            <meta itemprop="id" content="940">
            <meta itemprop="label" content="distance vertices">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="741">
              <meta itemprop="top" content="599">
              <meta itemprop="right" content="825">
              <meta itemprop="bottom" content="635">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/47/5f/5f/a9771b43181530/US11176719-20211116-D00015.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/9a/4a/a3/32718ffcb5cc69/US11176719-20211116-D00015.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="910">
            <meta itemprop="label" content="feature map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="750">
              <meta itemprop="top" content="154">
              <meta itemprop="right" content="834">
              <meta itemprop="bottom" content="189">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="920">
            <meta itemprop="label" content="map data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1310">
              <meta itemprop="top" content="155">
              <meta itemprop="right" content="1392">
              <meta itemprop="bottom" content="192">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="15">
            <meta itemprop="id" content="950">
            <meta itemprop="label" content="lines">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="517">
              <meta itemprop="top" content="1183">
              <meta itemprop="right" content="602">
              <meta itemprop="bottom" content="1223">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/7d/e5/4f/99174bd20ef8f0/US11176719-20211116-D00016.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/f0/e3/df/17f94db90c8791/US11176719-20211116-D00016.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="910">
            <meta itemprop="label" content="feature map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="751">
              <meta itemprop="top" content="158">
              <meta itemprop="right" content="836">
              <meta itemprop="bottom" content="195">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="920">
            <meta itemprop="label" content="map data">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1309">
              <meta itemprop="top" content="158">
              <meta itemprop="right" content="1391">
              <meta itemprop="bottom" content="196">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="16">
            <meta itemprop="id" content="960">
            <meta itemprop="label" content="vertices">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="806">
              <meta itemprop="top" content="593">
              <meta itemprop="right" content="887">
              <meta itemprop="bottom" content="628">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/57/f6/51/46a036a22ff980/US11176719-20211116-D00017.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/64/00/e7/f6186a1a83c563/US11176719-20211116-D00017.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1010">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1089">
              <meta itemprop="top" content="308">
              <meta itemprop="right" content="1196">
              <meta itemprop="bottom" content="344">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1020">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1089">
              <meta itemprop="top" content="508">
              <meta itemprop="right" content="1197">
              <meta itemprop="bottom" content="546">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1030">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1089">
              <meta itemprop="top" content="761">
              <meta itemprop="right" content="1196">
              <meta itemprop="bottom" content="802">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1040">
            <meta itemprop="label" content="Operations">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="981">
              <meta itemprop="top" content="1072">
              <meta itemprop="right" content="1086">
              <meta itemprop="bottom" content="1112">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1050">
            <meta itemprop="label" content="Operations">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1089">
              <meta itemprop="top" content="1329">
              <meta itemprop="right" content="1195">
              <meta itemprop="bottom" content="1367">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="17">
            <meta itemprop="id" content="1060">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1089">
              <meta itemprop="top" content="1676">
              <meta itemprop="right" content="1195">
              <meta itemprop="bottom" content="1715">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/53/d1/ae/9b323cec78796b/US11176719-20211116-D00018.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/65/1d/c9/40b937bfafb297/US11176719-20211116-D00018.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="18">
            <meta itemprop="id" content="1105">
            <meta itemprop="label" content="input image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="33">
              <meta itemprop="top" content="439">
              <meta itemprop="right" content="126">
              <meta itemprop="bottom" content="471">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="18">
            <meta itemprop="id" content="1110">
            <meta itemprop="label" content="first image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="672">
              <meta itemprop="top" content="196">
              <meta itemprop="right" content="765">
              <meta itemprop="bottom" content="230">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="18">
            <meta itemprop="id" content="1120">
            <meta itemprop="label" content="second image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1277">
              <meta itemprop="top" content="197">
              <meta itemprop="right" content="1373">
              <meta itemprop="bottom" content="229">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="18">
            <meta itemprop="id" content="1130">
            <meta itemprop="label" content="image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1277">
              <meta itemprop="top" content="553">
              <meta itemprop="right" content="1372">
              <meta itemprop="bottom" content="587">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="18">
            <meta itemprop="id" content="1140">
            <meta itemprop="label" content="operation">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1552">
              <meta itemprop="top" content="900">
              <meta itemprop="right" content="1645">
              <meta itemprop="bottom" content="935">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="18">
            <meta itemprop="id" content="1150">
            <meta itemprop="label" content="second image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1275">
              <meta itemprop="top" content="1088">
              <meta itemprop="right" content="1372">
              <meta itemprop="bottom" content="1123">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/02/d7/6d/96088c02b0fd3c/US11176719-20211116-D00019.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/7d/2e/8c/33b3a3c51c794c/US11176719-20211116-D00019.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="19">
            <meta itemprop="id" content="1210">
            <meta itemprop="label" content="input image">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="723">
              <meta itemprop="top" content="130">
              <meta itemprop="right" content="832">
              <meta itemprop="bottom" content="167">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="19">
            <meta itemprop="id" content="1230">
            <meta itemprop="label" content="neural network">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="699">
              <meta itemprop="top" content="826">
              <meta itemprop="right" content="807">
              <meta itemprop="bottom" content="863">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="19">
            <meta itemprop="id" content="1250">
            <meta itemprop="label" content="distance field map">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="724">
              <meta itemprop="top" content="1332">
              <meta itemprop="right" content="832">
              <meta itemprop="bottom" content="1370">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/79/76/5d/974c4360899376/US11176719-20211116-D00020.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/8a/41/e3/4706bf1132c67b/US11176719-20211116-D00020.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="20">
            <meta itemprop="id" content="1300">
            <meta itemprop="label" content="localization apparatus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="572">
              <meta itemprop="top" content="134">
              <meta itemprop="right" content="681">
              <meta itemprop="bottom" content="177">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="20">
            <meta itemprop="id" content="1305">
            <meta itemprop="label" content="communication bus">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="636">
              <meta itemprop="top" content="260">
              <meta itemprop="right" content="742">
              <meta itemprop="bottom" content="300">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="20">
            <meta itemprop="id" content="1310">
            <meta itemprop="label" content="sensors">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="318">
              <meta itemprop="top" content="382">
              <meta itemprop="right" content="427">
              <meta itemprop="bottom" content="422">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="20">
            <meta itemprop="id" content="1330">
            <meta itemprop="label" content="processor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1080">
              <meta itemprop="top" content="591">
              <meta itemprop="right" content="1188">
              <meta itemprop="bottom" content="629">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="20">
            <meta itemprop="id" content="1350">
            <meta itemprop="label" content="memory">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="319">
              <meta itemprop="top" content="797">
              <meta itemprop="right" content="425">
              <meta itemprop="bottom" content="838">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="20">
            <meta itemprop="id" content="1370">
            <meta itemprop="label" content="communication interface">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1080">
              <meta itemprop="top" content="1006">
              <meta itemprop="right" content="1187">
              <meta itemprop="bottom" content="1045">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="20">
            <meta itemprop="id" content="1390">
            <meta itemprop="label" content="display device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="318">
              <meta itemprop="top" content="1213">
              <meta itemprop="right" content="424">
              <meta itemprop="bottom" content="1252">
            </span>
          </li>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>Classifications</h2>
    <ul>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/70</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/73</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras using feature-based methods</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/74</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras using feature-based methods involving reference images or patches</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01</span>&mdash;<span itemprop="Description">MEASURING; TESTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C</span>&mdash;<span itemprop="Description">MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/00</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/26</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/28</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network with correlation of data from several navigational instruments</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/30</span>&mdash;<span itemprop="Description">Map- or contour-matching</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/70</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01</span>&mdash;<span itemprop="Description">MEASURING; TESTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C</span>&mdash;<span itemprop="Description">MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/00</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/26</span>&mdash;<span itemprop="Description">Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/34</span>&mdash;<span itemprop="Description">Route searching; Route guidance</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/36</span>&mdash;<span itemprop="Description">Input/output arrangements for on-board computers</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/3626</span>&mdash;<span itemprop="Description">Details of the output of route guidance instructions</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/3635</span>&mdash;<span itemprop="Description">Guidance using 3D or perspective road maps</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G01C21/3638</span>&mdash;<span itemprop="Description">Guidance using 3D or perspective road maps including 3D objects and buildings</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F</span>&mdash;<span itemprop="Description">ELECTRIC DIGITAL DATA PROCESSING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/00</span>&mdash;<span itemprop="Description">Information retrieval; Database structures therefor; File system structures therefor</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/20</span>&mdash;<span itemprop="Description">Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F16/29</span>&mdash;<span itemprop="Description">Geographical information databases</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06K9/6202</span>&mdash;<span itemprop="Description"></span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T11/00</span>&mdash;<span itemprop="Description">2D [Two Dimensional] image generation</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T11/60</span>&mdash;<span itemprop="Description">Editing figures and text; Combining figures or text</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/10</span>&mdash;<span itemprop="Description">Segmentation; Edge detection</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/11</span>&mdash;<span itemprop="Description">Region-based segmentation</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/10</span>&mdash;<span itemprop="Description">Segmentation; Edge detection</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/187</span>&mdash;<span itemprop="Description">Segmentation; Edge detection involving region growing; involving region merging; involving connected component labelling</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/50</span>&mdash;<span itemprop="Description">Depth or shape recovery</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/536</span>&mdash;<span itemprop="Description">Depth or shape recovery from perspective effects, e.g. by using vanishing points</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/00</span>&mdash;<span itemprop="Description">Image analysis</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/70</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T7/73</span>&mdash;<span itemprop="Description">Determining position or orientation of objects or cameras using feature-based methods</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T</span>&mdash;<span itemprop="Description">IMAGE DATA PROCESSING OR GENERATION, IN GENERAL</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/00</span>&mdash;<span itemprop="Description">Indexing scheme for image analysis or image enhancement</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20</span>&mdash;<span itemprop="Description">Special algorithmic details</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06T2207/20084</span>&mdash;<span itemprop="Description">Artificial neural networks [ANN]</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="Additional" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
    </ul>
  </section>

  

  

  <section>
    <h2>Definitions</h2>
    <ul>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the following description</span>
        <span itemprop="definition">relates to a method and apparatus for localization based on images and map data.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">augmented reality (AR) services</span>
        <span itemprop="definition">are provided in fields such as driving assistance for vehicles and other means of transportation, games, or entertainment.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a number of localization methods</span>
        <span itemprop="definition">are used.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a sensor-based localization method</span>
        <span itemprop="definition">uses a combination of sensors such as a global positioning system (GPS) sensor and an inertial measurement unit (IMU) sensor to determine a position and an orientation of an object.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">IMU</span>
        <span itemprop="definition">inertial measurement unit</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a vision-based localization method</span>
        <span itemprop="definition">uses camera information.</span>
        <meta itemprop="num_attr" content="0003">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization method</span>
        <span itemprop="definition">including generating a first image of an object from an input image, generating a second image to project the object with respect to candidate localization information of a device, based on map data including a position of the object, pooling, from the first image, feature values corresponding to vertices in the second image, and determining a score of the candidate localization information based on the pooled feature values.</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the generating of the first image</span>
        <span itemprop="definition">may include generating feature maps corresponding to a plurality of features.</span>
        <meta itemprop="num_attr" content="0006">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the generating of the second image</span>
        <span itemprop="definition">may include extracting a region corresponding to a field of view in the candidate localization information from the map data, and projecting vertices included in the region into a projection point corresponding to the candidate localization information.</span>
        <meta itemprop="num_attr" content="0007">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the pooling</span>
        <span itemprop="definition">may include selecting pixels in the first image based on coordinates of the vertices, and obtaining feature values of the selected pixels.</span>
        <meta itemprop="num_attr" content="0008">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining</span>
        <span itemprop="definition">may include determining a sum of the pooled feature values.</span>
        <meta itemprop="num_attr" content="0009">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining of the sum</span>
        <span itemprop="definition">may include, determining a weighted sum of the feature values based on weights determined for the features, in response to the first image may include feature maps corresponding to features.</span>
        <meta itemprop="num_attr" content="0010">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization method</span>
        <span itemprop="definition">may include determining localization information of the device based on the score of the candidate localization information.</span>
        <meta itemprop="num_attr" content="0011">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining of the localization information of the device</span>
        <span itemprop="definition">may include determining candidate localization information corresponding to a highest score, from among scores of a plurality of candidate localization information, to be the localization information of the device.</span>
        <meta itemprop="num_attr" content="0012">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining of the localization information of the device</span>
        <span itemprop="definition">may include segmenting the second image into regions, and sequentially determining a plurality of degree of freedom (DOF) values included in the candidate localization information using scores calculated in the regions.</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DOE</span>
        <span itemprop="definition">degree of freedom</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the plurality of DOF values</span>
        <span itemprop="definition">may include three translational DOF values, and three rotational DOF values.</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the segmenting</span>
        <span itemprop="definition">may include segmenting the second image into a long-distance region and a short-distance region based on a first criterion associated with a distance, and segmenting the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point.</span>
        <meta itemprop="num_attr" content="0015">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the sequentially determining</span>
        <span itemprop="definition">may include determining rotational DOFs based on the long-distance region, determining a left and right translational DOF based on the vanishing point-oriented short-distance region, and determining a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</span>
        <meta itemprop="num_attr" content="0016">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining</span>
        <span itemprop="definition">may include determining rotational DOFs based on long-distance vertices affected below a first threshold by translational DOFs, from among vertices included in the second image, determining a left and right translational DOF based on vanishing point-oriented short-distance vertices affected below a second threshold by a forward and backward translational DOF, from among short-distance vertices excluding the long-distance vertices from the second image, and determining the forward and backward translational DOF based on non-vanishing point-oriented short-distance vertices excluding the vanishing point-oriented short-distance vertices from the short-distance vertices.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining of the localization information of the device</span>
        <span itemprop="definition">may include determining a direction to improve the score based on a distribution of the pooled feature values, and correcting the candidate localization information based on the direction.</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the first image</span>
        <span itemprop="definition">may include a probability distribution indicating a degree of closeness to the object, wherein the determining of the direction may include determining the direction based on the probability distribution.</span>
        <meta itemprop="num_attr" content="0019">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining of the localization information of the device</span>
        <span itemprop="definition">may include generating a corrected second image in which the object is projected with respect to the corrected candidate localization information, and determining a corrected score of the corrected candidate localization information by pooling, from the first image, feature values corresponding to vertices in the corrected second image, wherein the determining of the direction, the correcting of the candidate localization information, the generating of the corrected second image, and the calculating of the corrected score are iteratively performed until the corrected score satisfies a condition.</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization method</span>
        <span itemprop="definition">may include determining a virtual object on the map data to provide an augmented reality (AR) service, and displaying the virtual object based on the determined localization information.</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AR</span>
        <span itemprop="definition">augmented reality</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the input image</span>
        <span itemprop="definition">may include a driving image of a vehicle, and the virtual object indicates driving route information.</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization method</span>
        <span itemprop="definition">including generating a first image of an object from an input image, generating a second image to project the object with respect to candidate localization information of a device, based on map data including a position of the object, segmenting the second image into regions, and determining degree of freedom (DOF) values included in the candidate localization information through matching between the first image and the regions.</span>
        <meta itemprop="num_attr" content="0023">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DOF</span>
        <span itemprop="definition">degree of freedom</span>
        <meta itemprop="num_attr" content="0023">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining</span>
        <span itemprop="definition">may include determining the DOF values included in the candidate localization information by sequentially using scores calculated through the matching in the regions.</span>
        <meta itemprop="num_attr" content="0024">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining</span>
        <span itemprop="definition">may include calculating, while changing DOF values determined for the regions, scores corresponding to the changed DOF values by pooling, from the first image, feature values corresponding to vertices in the regions, and selecting a DOF value corresponding to a highest score.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the plurality of DOF values</span>
        <span itemprop="definition">may include three translational DOF values, and three rotational DOF values.</span>
        <meta itemprop="num_attr" content="0026">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the segmenting</span>
        <span itemprop="definition">may include segmenting the second image into a long-distance region and a short-distance region based on a first criterion associated with a distance, and segmenting the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point.</span>
        <meta itemprop="num_attr" content="0027">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining</span>
        <span itemprop="definition">may include determining rotational DOFs based on the long-distance region, determining a left and right translational DOF based on the vanishing point-oriented short-distance region, and determining a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</span>
        <meta itemprop="num_attr" content="0028">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the determining</span>
        <span itemprop="definition">may include determining rotational DOFs based on long-distance vertices affected below a first threshold by translational DOFs, from among vertices included in the second image, determining a left and right translational DOF based on vanishing point-oriented short-distance vertices affected below a second threshold by a forward and backward translational DOF, from among short-distance vertices excluding the long-distance vertices from the second image, and determining the forward and backward translational DOF based on non-vanishing point-oriented short-distance vertices excluding the vanishing point-oriented short-distance vertices from the short-distance vertices.</span>
        <meta itemprop="num_attr" content="0029">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization method</span>
        <span itemprop="definition">may include determining a virtual object on the map data to provide an augmented reality (AR) service, and displaying the virtual object based on the determined DOF values.</span>
        <meta itemprop="num_attr" content="0030">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AR</span>
        <span itemprop="definition">augmented reality</span>
        <meta itemprop="num_attr" content="0030">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the input image</span>
        <span itemprop="definition">may include a driving image of a vehicle, and the virtual object indicates driving route information.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus</span>
        <span itemprop="definition">including a processor configured to generate a first image of an object from an input image, generate a second image to project the object with respect to candidate localization information of a device, based on map data including a position of the object, pool, from the first image, feature values corresponding to vertices in the second image, and determine a score of the candidate localization information based on the pooled feature values.</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus</span>
        <span itemprop="definition">including a processor configured to generate a first image of an object from an input image, generate a second image to project the object with respect to candidate localization information of a device, based on map data including a position of the object, segment the second image into regions, and determine degree of freedom (DOF) values included in the candidate localization information through matching between the first image and the regions.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DOE</span>
        <span itemprop="definition">degree of freedom</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus</span>
        <span itemprop="definition">including a sensor disposed on a device, and being configured to sense one or more of an image and candidate localization information of the device, a processor configured to generate a first image of an object from the image, generate a second image to project the object with respect to the candidate localization information, based on map data including a position of the object, determine a score of the candidate localization information based on pooling, from the first image, feature values corresponding to vertices in the second image, and determine localization information of the device based on the score, and a head-up display (HUD) configured to visualize a virtual object on the map data based on the determined localization information.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">HUD</span>
        <span itemprop="definition">head-up display</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor</span>
        <span itemprop="definition">may be configured to segment the second image into a long-distance region and a short-distance region based on a distance, and segment the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a vanishing point.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor</span>
        <span itemprop="definition">may be configured to determine rotational degree of freedom (DOF) values based on the long-distance region, determine a left and right translational DOF based on the vanishing point-oriented short-distance region, and determine a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DOF</span>
        <span itemprop="definition">rotational degree of freedom</span>
        <meta itemprop="num_attr" content="0036">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor</span>
        <span itemprop="definition">may be configured to generate, using a neural network, the first image may include feature maps corresponding to a plurality of features.</span>
        <meta itemprop="num_attr" content="0037">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the second image</span>
        <span itemprop="definition">may include a projection of two-dimensional (2D) vertices corresponding to the object.</span>
        <meta itemprop="num_attr" content="0038">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">may include a memory configured to store the map data, the image, the first image, the second image, the score, and instructions that, when executed, configures the processor to determine any one or any combination of the determined localization information and the virtual object.</span>
        <meta itemprop="num_attr" content="0039">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 1A through 1C</span>
        <span itemprop="definition">illustrate examples of significance of localization accuracy in an augmented reality (AR) application.</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AR</span>
        <span itemprop="definition">augmented reality</span>
        <meta itemprop="num_attr" content="0041">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">illustrates an example of calculating a localization score.</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3</span>
        <span itemprop="definition">illustrates an example of calculating a localization score.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">illustrates an example of determining localization information of a device by utilizing a localization score.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">illustrates an example of scores of pieces of candidate localization information.</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">illustrates an example of a localization method.</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">illustrates an example of determining localization information of a device through an optimization technique.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 8A and 8B</span>
        <span itemprop="definition">illustrate examples of optimization technique.</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 9A through 9E</span>
        <span itemprop="definition">illustrate examples of a result of applying an optimization technique.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 10</span>
        <span itemprop="definition">illustrates an example of a localization method by parameter updating.</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 11</span>
        <span itemprop="definition">illustrates an example of a result of applying a localization method by parameter updating step by step.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 12</span>
        <span itemprop="definition">illustrates an example of a neural network to generate a feature map.</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 13</span>
        <span itemprop="definition">illustrates an example of a localization apparatus.</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">first, second, and the like</span>
        <span itemprop="definition">may be used herein to describe components. Each of these terminologies is not used to define an essence, order or sequence of a corresponding component but used merely to distinguish the corresponding component from other component(s).</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a first component</span>
        <span itemprop="definition">may be referred to as a second component, and similarly the second component may also be referred to as the first component.</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the first component</span>
        <span itemprop="definition">may be directly âconnected,â âcoupled,â or âjoinedâ to the second component, or a third component may be âconnected,â âcoupled,â or âjoinedâ between the first component and the second component.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a third component</span>
        <span itemprop="definition">may not be âconnectedâ or âjoinedâ between the first component and the second component. Similar expressions, for example, âbetweenâ and âimmediately betweenâ and âadjacent toâ and âimmediately adjacent to,â are also to be construed in this manner.</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the first component</span>
        <span itemprop="definition">may be directly âconnected,â âcoupled,â or âjoinedâ to the second component, or a third component may be âconnected,â âcoupled,â or âjoinedâ between the first component and the second component.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a third component</span>
        <span itemprop="definition">may not be âconnectedâ or âjoinedâ between the first component and the second component. Similar expressions, for example, âbetweenâ and âimmediately betweenâ and âadjacent toâ and âimmediately adjacent to,â are also to be construed in this manner.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the examples set forth hereinafter</span>
        <span itemprop="definition">may be implemented on hardware that is applied to technology for localization based on images and map data.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the examples</span>
        <span itemprop="definition">may be used to improve an accuracy of localization in an augmented reality head-up display (AR HUD).</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AR HUD</span>
        <span itemprop="definition">augmented reality head-up display</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization</span>
        <span itemprop="definition">is needed for a number of location-based services in addition to the HUD, and the examples may be used to estimate a position and an orientation in an environment in which high density (HD) map data is provided for high-precision localization.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">HD</span>
        <span itemprop="definition">high density</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 1A through 1C</span>
        <span itemprop="definition">illustrate examples of significance of localization accuracy in an AR application.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AR</span>
        <span itemprop="definition">adds or augments information based on reality and provides the added or augmented information.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AR</span>
        <span itemprop="definition">adds a virtual object corresponding to a virtual image to a background image or an image of a real world and represents the image with the added object.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">AR</span>
        <span itemprop="definition">appropriately combines a virtual world with the real world such that a user experiences an immersive experience when interacting with the virtual world in real time without recognizing a separation between real and virtual environments.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a position and an orientation, i.e., localization information, of a user device or the user which provides AR</span>
        <span itemprop="definition">needs to be determined.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Localization information for providing AR</span>
        <span itemprop="definition">is used to dispose a virtual object at a desired position in an image.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a driving guidance lane corresponding to a virtual object</span>
        <span itemprop="definition">is displayed on a road surface.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">examples</span>
        <span itemprop="definition">are not limited thereto.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1A</span>
        <span itemprop="definition">illustrates an AR image 120 having a relatively small localization error.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1B</span>
        <span itemprop="definition">illustrates an AR image 140 having a relatively great localization error.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a reference route of a vehicle</span>
        <span itemprop="definition">is displayed on a road image based on localization information of an object 110 .</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the object</span>
        <span itemprop="definition">corresponds to a vehicle and/or a user terminal which performs localization.</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization information of the object 110</span>
        <span itemprop="definition">includes error within a small tolerance range</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a driving guidance lane 115 that is a virtual object to be displayed by a device</span>
        <span itemprop="definition">is visually appropriately aligned with a real road image, as shown in the image 120 .</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">localization information of an object 130</span>
        <span itemprop="definition">includes a relatively greater error, i.e., outside the tolerance range</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a driving guidance lane 135</span>
        <span itemprop="definition">that is a virtual object to be displayed by the device is not visually appropriately aligned with a real road image, as shown in the image 140 .</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">localization information</span>
        <span itemprop="definition">includes a position and an orientation of the device.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the position</span>
        <span itemprop="definition">corresponds to three-dimensional (3D) coordinates such as lateral (t x ), vertical (t y ), and longitudinal (t z ), i.e., (x, y, z), as translational degrees of freedom (DOFs).</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the orientation</span>
        <span itemprop="definition">corresponds to pitch (r x ), yaw (r y ), and roll (r z ) as rotational DOFs.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the position</span>
        <span itemprop="definition">is obtained through, for example, a global positioning system (GPS) sensor and a light detection and ranging (LiDAR), and the orientation is obtained through, for example, an inertial measurement unit (IMU) sensor and a gyro sensor.</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">GPS</span>
        <span itemprop="definition">global positioning system</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">LiDAR</span>
        <span itemprop="definition">light detection and ranging</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">IMU</span>
        <span itemprop="definition">inertial measurement unit</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">gyro sensor</span>
        <span itemprop="definition">gyro sensor</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vehicle described herein</span>
        <span itemprop="definition">refers to any mode of transportation, delivery, or communication such as, for example, an automobile, a truck, a tractor, a scooter, a motorcycle, a cycle, an amphibious vehicle, a snowmobile, a boat, a public transit vehicle, a bus, a monorail, a train, a tram, an autonomous or automated driving vehicle, an intelligent vehicle, a self-driving vehicle, an unmanned aerial vehicle, an electric vehicle (EV), a hybrid vehicle, a smart mobility device, an intelligent vehicle with an advanced driver assistance system (ADAS), or a drone.</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the smart mobility device</span>
        <span itemprop="definition">includes mobility devices such as, for example, electric wheels, electric kickboard, and electric bike.</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">vehicles</span>
        <span itemprop="definition">include motorized and non-motorized vehicles, for example, a vehicle with a power engine (for example, a cultivator or a motorcycle), a bicycle or a handcart.</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">methods and apparatuses described herein</span>
        <span itemprop="definition">may be included in various other devices, such as, for example, a smart phone, a walking assistance device, a wearable device, a security device, a robot, a mobile terminal, and various Internet of Things (IoT) devices.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a smart phone</span>
        <span itemprop="definition">such as, for example, a smart phone, a walking assistance device, a wearable device, a security device, a robot, a mobile terminal, and various Internet of Things (IoT) devices.</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">IoT</span>
        <span itemprop="definition">Internet of Things</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">road</span>
        <span itemprop="definition">is a thoroughfare, route, or connection, between two places that has been improved to allow travel by foot or some form of conveyance, such as a vehicle.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a road</span>
        <span itemprop="definition">can include various types of roads refers to a way on which vehicles drive, and includes various types of roads such as, for example, a highway, a national road, a local road, an expressway, farm roads, local roads, high-speed national roads, and a motorway.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the road</span>
        <span itemprop="definition">includes one or more lanes.</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">lane</span>
        <span itemprop="definition">refers to a road space distinguished by lines marked on a surface of the road.</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the lane</span>
        <span itemprop="definition">is distinguished by left and right lines or lane boundary lines thereof.</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the lines</span>
        <span itemprop="definition">are various types of lines, for example, solid lines, broken lines, curved lines, and zigzag lines marked in colors such as white, blue, and yellow on the surface of the road.</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a line</span>
        <span itemprop="definition">corresponds to one line separating a single lane, or corresponds to a pair of lines separating a single lane, that is, left and right lines corresponding to lane boundary lines.</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">lane boundary</span>
        <span itemprop="definition">may be interchangeably used with the term âlane markingâ</span>
        <meta itemprop="num_attr" content="0073">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the methods and apparatuses described herein</span>
        <span itemprop="definition">are used to road guidance information in a navigation device of a vehicle, such as, for example, an augmented reality head-up display (AR 3D HUD), and an autonomous vehicle.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a navigation device of a vehicle</span>
        <span itemprop="definition">such as, for example, an augmented reality head-up display (AR 3D HUD), and an autonomous vehicle.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the examples set forth hereinafter</span>
        <span itemprop="definition">may be utilized to display lines in an AR navigation system of a smart vehicle, generate visual information to assist steering of an autonomous vehicle, or provide a variety of control information related to driving of a vehicle.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the examples</span>
        <span itemprop="definition">are used to assist safe and pleasant driving by providing visual information to a device including an intelligent system such as an HUD installed on a vehicle for driving assistance or fully autonomous driving.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the examples described herein</span>
        <span itemprop="definition">may also be used to interpret visual information for an intelligent system installed for fully autonomous driving or driving assistance in a vehicle, and used to assist safe and comfortable driving.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the examples described herein</span>
        <span itemprop="definition">may be applicable to vehicles and vehicle management systems such as, for example, an autonomous vehicle, an automatic or autonomous driving system, an intelligent vehicle, an advanced driver assistance system (ADAS), a navigation system to assist a vehicle with safely maintaining a lane on which the vehicle is travelling, a smartphone, or a mobile device.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">vehicle management systems</span>
        <span itemprop="definition">such as, for example, an autonomous vehicle, an automatic or autonomous driving system, an intelligent vehicle, an advanced driver assistance system (ADAS), a navigation system to assist a vehicle with safely maintaining a lane on which the vehicle is travelling, a smartphone, or a mobile device.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ADAS</span>
        <span itemprop="definition">advanced driver assistance system</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the examples related to displaying a road guidance information for vehicles</span>
        <span itemprop="definition">is provided as an example only, and other examples such as, for example, training, gaming, applications in healthcare, public safety, tourism, and marketing are considered to be well within the scope of the present disclosure.</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">illustrates an example of calculating a localization score.</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus</span>
        <span itemprop="definition">calculates a score s( â ) of localization parameters  â  based on map data Q and an image I.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">may be implemented by one or more hardware modules.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the map data</span>
        <span itemprop="definition">is a point cloud including a plurality of 3D vertices corresponding to object(s), such as lines.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the 3D vertices of the map data</span>
        <span itemprop="definition">are projected onto two-dimensional (2D) vertices based on localization parameters.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the features of the image</span>
        <span itemprop="definition">include feature values extracted on a basis of pixels included in the image. Thus, for the examples described herein, a correspondence between vertices of the map data and features of the image may not be needed.</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Information related to a correspondence or matching between the vertices</span>
        <span itemprop="definition">may not be needed for the examples described herein. Further, because the features extracted from the image may not be parameterized, a separate analysis on a relation between the features or a search of the map data may not be needed.</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization parameters  â</span>
        <span itemprop="definition">are position/orientation information parameters, and are defined as 6-DOF variables described in FIG. 1C .</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization parameters  â</span>
        <span itemprop="definition">correspond to approximate position/orientation information.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">improves a localization accuracy by correcting the localization parameters  â  using scoring technology based on the image and the map data.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">configures a feature map by extracting features from the image I.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">calculates a matching score with respect to the localization parameters  â .</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">calculates the matching score by projecting vertices from the map data Q based on the localization parameters and pooling feature values of pixels corresponding to 2D coordinates of the projected vertices, among pixels of the feature map.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">updates the localization parameters  â  to increase the matching score.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the device</span>
        <span itemprop="definition">is any device that performs a localization method, and includes devices, such as, for example, a vehicle, a navigation system, or a user device such as a smart phone.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Localization information</span>
        <span itemprop="definition">has 6 DOFs including the position and the orientation of the device, as described above.</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization information</span>
        <span itemprop="definition">is obtained based on outputs of sensors such as, for example, an IMU sensor, a GPS sensor, a lidar sensor, and a radio detection and ranging (radar).</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">sensors</span>
        <span itemprop="definition">such as, for example, an IMU sensor, a GPS sensor, a lidar sensor, and a radio detection and ranging (radar).</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the input image</span>
        <span itemprop="definition">is a background image or other images to be displayed along with a virtual object to provide an AR service.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the input image</span>
        <span itemprop="definition">includes, for example, a driving image of the vehicle.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the driving image</span>
        <span itemprop="definition">is a driving image acquired using a capturing device mounted on the vehicle, and includes one or more frames.</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">acquires the input image based on an output of the capturing device.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the capturing device</span>
        <span itemprop="definition">is fixed to a location on the vehicle such as, for example, a windshield, a dashboard, or a rear-view mirror of the vehicle, to capture driving images of a view in front of the vehicle.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the capturing device</span>
        <span itemprop="definition">includes, for example, a vision sensor, an image sensor, or a device that performs a similar function. Depending on examples, the capturing device captures a single image, or captures images for each frame. In an example, images that are captured by a device other than the capturing device that is fixed to the vehicle are also used as the driving images.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An object</span>
        <span itemprop="definition">includes, for example, a line, a road surface marking, a traffic light, a traffic sign, a curb, a pedestrian, and a structure.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the line</span>
        <span itemprop="definition">includes lines such as, for example, a lane boundary line, a road center line, and a stop line.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the road surface marking</span>
        <span itemprop="definition">includes markings such as, for example, a no parking marking, a crosswalk marking, a towaway zone marking, and a speed limit marking.</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the map data</span>
        <span itemprop="definition">is high density (HD) map data.</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An HD map</span>
        <span itemprop="definition">is a 3D map with a high density, for example, a centimeter-level density, that may be used for autonomous driving.</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the HD map</span>
        <span itemprop="definition">includes, for example, line information related to a road center line and a boundary line, and information related to a traffic light, a traffic sign, a curb, a road surface marking, and various structures in a form of 3D digital data.</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the HD map</span>
        <span itemprop="definition">is established by, for example, a mobile mapping system (MMS).</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">MMS</span>
        <span itemprop="definition">mobile mapping system</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the MMS</span>
        <span itemprop="definition">a 3D space information investigation system equipped with various sensors, obtains minute position information using a moving object equipped with sensors such as a camera, a lidar, and a GPS to measure a position and geographic features.</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3</span>
        <span itemprop="definition">illustrates an example of calculating a localization score.</span>
        <meta itemprop="num_attr" content="0085">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus 200</span>
        <span itemprop="definition">includes transform devices 210 and 220 , a feature extractor 230 , and a pooler 240 .</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the transform device 210</span>
        <span itemprop="definition">receives parameters  â  and map data Q, and applies a 3D position and a 3D orientation of a device corresponding to the parameters  â  to the map data Q through a 3D transform T.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the transform device 210</span>
        <span itemprop="definition">extracts a region corresponding to a range of view at the position and the orientation corresponding to the parameters  â  from the map data.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the transform device 220</span>
        <span itemprop="definition">generates a projection image at a viewpoint of the device through a perspective transform P.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the transform device 220</span>
        <span itemprop="definition">projects 3D vertices included in the region extracted by the transform device 210 onto a 2D projection plane corresponding to the parameters  â .</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D vertices q i k included in the map data Q</span>
        <span itemprop="definition">are transformed to 2D vertices p i k in the projection image through the transform T and the transform P based on the parameters  â .</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">k</span>
        <span itemprop="definition">denotes an index indicating a different feature or class</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">i</span>
        <span itemprop="definition">denotes an index indicating a vertex in the corresponding feature or class.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the feature extractor 230</span>
        <span itemprop="definition">extracts a feature from the image I.</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the feature</span>
        <span itemprop="definition">includes one or more feature maps F 1 and F 2 235 depending on a type or class of an object.</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the feature map F 1</span>
        <span itemprop="definition">includes features related to lines in the image</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the feature map F 2</span>
        <span itemprop="definition">includes features related to traffic signs in the image.</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an example in which two feature maps F 1 and F 2 235 are extracted</span>
        <span itemprop="definition">is described. However, examples are not limited thereto.</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">includes separate feature extractors to extract a plurality of feature maps.</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">includes a single feature extractor, for example, a deep neural network (DNN), to output a plurality of feature maps for each channel.</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DNN</span>
        <span itemprop="definition">deep neural network</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extracted feature maps F 1 and F 2 235</span>
        <span itemprop="definition">may include errors in some examples, and thus may not accurately specify values of corresponding features on a pixel basis.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">each feature map</span>
        <span itemprop="definition">has a value between â0â and â1â for each pixel.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a feature value of a pixel</span>
        <span itemprop="definition">indicates an intensity of the pixel with respect to the feature.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the 2D vertices p i k of the projection image</span>
        <span itemprop="definition">refer to pixels corresponding to the 3D vertices q i k of the map data mapped to the image I.</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">scores of features of the pixels mapped to the image I</span>
        <span itemprop="definition">are summed up.</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">P( )</span>
        <span itemprop="definition">denotes the transform P</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">P(T(q i k ,  â ))</span>
        <span itemprop="definition">denotes a mapping point</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a final score</span>
        <span itemprop="definition">is calculated by calculating a weighted sum of the scores of the features.</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a weight w k</span>
        <span itemprop="definition">is set using an arbitrary scheme.</span>
        <meta itemprop="num_attr" content="0094">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the weight w k</span>
        <span itemprop="definition">is set to be a weight assigned equally in the lump or a value tuned by training data.</span>
        <meta itemprop="num_attr" content="0094">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">illustrates an example of determining localization information of a device by utilizing a localization score.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operations in FIG. 4</span>
        <span itemprop="definition">may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in FIG. 4 may be performed in parallel or concurrently.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more blocks of FIG. 4 , and combinations of the blocks,</span>
        <span itemprop="definition">can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1-3</span>
        <span itemprop="definition">are also applicable to FIG. 4 , and are incorporated herein by reference. Thus, the above description may not be repeated here.</span>
        <meta itemprop="num_attr" content="0095">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an input image</span>
        <span itemprop="definition">is received.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">features</span>
        <span itemprop="definition">are extracted.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">map data</span>
        <span itemprop="definition">is obtained.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">candidate localization information</span>
        <span itemprop="definition">is obtained.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the candidate localization information</span>
        <span itemprop="definition">includes a plurality of pieces of candidate localization information.</span>
        <meta itemprop="num_attr" content="0096">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a projection image of the map data with respect to the candidate localization information</span>
        <span itemprop="definition">is generated.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a plurality of projection images with respect to the plurality of pieces of candidate localization information</span>
        <span itemprop="definition">is generated.</span>
        <meta itemprop="num_attr" content="0097">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">feature values corresponding to 2D vertices in the projection image</span>
        <span itemprop="definition">are pooled from the feature map. Further, in operation 460 , a score of the candidate localization information is calculated based on the pooled feature values. When a plurality of pieces of candidate localization information is provided, scores of the pieces of candidate localization information are calculated.</span>
        <meta itemprop="num_attr" content="0098">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a best score</span>
        <span itemprop="definition">for example, a highest score, is determined.</span>
        <meta itemprop="num_attr" content="0099">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">candidate localization information having the determined best score</span>
        <span itemprop="definition">is determined to be localization information of a device.</span>
        <meta itemprop="num_attr" content="0099">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus 200</span>
        <span itemprop="definition">determines a virtual object on the map data Q to provide an AR service.</span>
        <meta itemprop="num_attr" content="0100">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the virtual object</span>
        <span itemprop="definition">indicates driving route information, and is represented in a form of an arrow or a road marking indicating a direction to travel.</span>
        <meta itemprop="num_attr" content="0100">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">displays the virtual object along with the input image on a display of a user device, a navigation system, or a HUD, based on the localization information determined in operation 480 .</span>
        <meta itemprop="num_attr" content="0100">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">illustrates an example of scores of pieces of candidate localization information.</span>
        <meta itemprop="num_attr" content="0101">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a degree of visual alignment between an image 510 and a projection image with respect to first candidate localization information</span>
        <span itemprop="definition">is lower than a degree of visual alignment between an image 520 and a projection image with respect to second candidate localization information. Accordingly, a score pooled from a feature map of the image 510 based on the first candidate localization information is calculated to be lower than a score pooled from a feature map of the image 520 based on the second candidate localization information.</span>
        <meta itemprop="num_attr" content="0102">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 6</span>
        <span itemprop="definition">illustrates an example of a localization method.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operations in FIG. 6</span>
        <span itemprop="definition">may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in FIG. 6 may be performed in parallel or concurrently.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more blocks of FIG. 6 , and combinations of the blocks,</span>
        <span itemprop="definition">can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1-5</span>
        <span itemprop="definition">are also applicable to FIG. 6 , and are incorporated herein by reference. Thus, the above description may not be repeated here.</span>
        <meta itemprop="num_attr" content="0103">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">At least one feature map</span>
        <span itemprop="definition">is extracted from an input image.</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a candidate group of localization parameters</span>
        <span itemprop="definition">for example, position/orientation parameters, is selected.</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a determination</span>
        <span itemprop="definition">is made whether additional candidate localization information is to be evaluated.</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a projection image corresponding to the candidate localization information</span>
        <span itemprop="definition">is generated.</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">scores of features</span>
        <span itemprop="definition">are calculated.</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a final score</span>
        <span itemprop="definition">is calculated through a weighted sum of the scores of the features.</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a determination</span>
        <span itemprop="definition">is made whether the best candidate localization information is to be updated. In an example, the determination of the best candidate localization information is made by comparing the previous best candidate localization information, from among pieces of evaluated candidate localization information, to the final score.</span>
        <meta itemprop="num_attr" content="0104">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the best candidate localization information</span>
        <span itemprop="definition">is determined to be localization information of a device.</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">parameters of the best candidate localization information</span>
        <span itemprop="definition">are determined to be position/orientation parameters of the device.</span>
        <meta itemprop="num_attr" content="0105">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 7</span>
        <span itemprop="definition">illustrates an example of determining localization information of a device through an optimization technique.</span>
        <meta itemprop="num_attr" content="0106">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operations in FIG. 7</span>
        <span itemprop="definition">may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in FIG. 7 may be performed in parallel or concurrently.</span>
        <meta itemprop="num_attr" content="0106">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more blocks of FIG. 7 , and combinations of the blocks,</span>
        <span itemprop="definition">can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions.</span>
        <meta itemprop="num_attr" content="0106">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1-6</span>
        <span itemprop="definition">are also applicable to FIG. 7 , and are incorporated herein by reference. Thus, the above description may not be repeated here.</span>
        <meta itemprop="num_attr" content="0106">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an input image</span>
        <span itemprop="definition">is received.</span>
        <meta itemprop="num_attr" content="0107">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a feature map</span>
        <span itemprop="definition">is extracted.</span>
        <meta itemprop="num_attr" content="0107">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">map data</span>
        <span itemprop="definition">is received.</span>
        <meta itemprop="num_attr" content="0107">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">initial localization information</span>
        <span itemprop="definition">is received.</span>
        <meta itemprop="num_attr" content="0107">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a projection image with respect to the initial localization information</span>
        <span itemprop="definition">is generated.</span>
        <meta itemprop="num_attr" content="0107">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the initial localization information</span>
        <span itemprop="definition">is updated through an optimization technique.</span>
        <meta itemprop="num_attr" content="0108">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">localization information of a device</span>
        <span itemprop="definition">is determined to be the optimized localization information.</span>
        <meta itemprop="num_attr" content="0108">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 8A and 8B</span>
        <span itemprop="definition">illustrate an example of an optimization technique.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operations in FIG. 8A</span>
        <span itemprop="definition">may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in FIG. 8A may be performed in parallel or concurrently.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more blocks of FIG. 8A , and combinations of the blocks,</span>
        <span itemprop="definition">can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1-7</span>
        <span itemprop="definition">are also applicable to FIG. 8A , and are incorporated herein by reference. Thus, the above description may not be repeated here.</span>
        <meta itemprop="num_attr" content="0110">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus</span>
        <span itemprop="definition">supports a global optimization process.</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">classifies 2D vertices projected from map data by a criterion other than features, for example, a distance, or whether a region is vanishing point-oriented, and uses the classified 2D vertices to estimate different DOFs of localization parameters.</span>
        <meta itemprop="num_attr" content="0111">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">segments a projection image into a plurality of regions, and determines localization information of a device through matching between a feature map and the regions.</span>
        <meta itemprop="num_attr" content="0112">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">sequentially determines a plurality of DOF values included in the localization information by sequentially using scores calculated through the matching in the regions.</span>
        <meta itemprop="num_attr" content="0112">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">calculates pools, from the feature map, feature values corresponding to the 2D vertices included in the regions while changing DOF values determined for the regions.</span>
        <meta itemprop="num_attr" content="0112">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">calculates scores corresponding to the changed DOF values based on the pooled feature values.</span>
        <meta itemprop="num_attr" content="0112">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">determines a DOF to be a value corresponding to a highest score.</span>
        <meta itemprop="num_attr" content="0112">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the distant vertices in the projection image</span>
        <span itemprop="definition">have a characteristic of being practically invariant to a change in position parameter. Based on such a characteristic, the localization apparatus separately performs a process of determining an orientation parameter by calculating a score using long-distance vertices and a process of determining a position parameter by calculating a score using short-distance vertices. This reduces DOFs to be estimated for each process, and thus a search complexity or a local convergence possibility during optimization decreases.</span>
        <meta itemprop="num_attr" content="0113">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">segments the projection image into a long-distance region and a short-distance region based on a first criterion associated with a distance, and segments the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point, which will be described further below.</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the long-distance region</span>
        <span itemprop="definition">includes 2D vertices that are affected below a threshold by translational DOFs.</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vanishing point-oriented short-distance region</span>
        <span itemprop="definition">includes 2D vertices whose influence due to the movement-related DOF in the forward and backward direction, or forward and backward translational DOF, is less than a threshold.</span>
        <meta itemprop="num_attr" content="0114">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">uses a portion of the DOFs of the localization parameters as values determined by advance calibration. Further, in an example, r z and t y of the localization parameters are determined by advance calibration because the height t y and the roll r z at which a camera is installed on a vehicle are fixed.</span>
        <meta itemprop="num_attr" content="0115">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a feature map</span>
        <span itemprop="definition">is extracted from an input image.</span>
        <meta itemprop="num_attr" content="0116">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">long-distance vertices Q 1</span>
        <span itemprop="definition">are selected from a projection image of map data.</span>
        <meta itemprop="num_attr" content="0116">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the long-distance vertices Q 1</span>
        <span itemprop="definition">are not substantially affected by translational DOFs t x and t z from among the DOFs of the localization parameters.</span>
        <meta itemprop="num_attr" content="0116">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">rotational DOFs r x and r y</span>
        <span itemprop="definition">are determined based on the long-distance vertices Q 1 .</span>
        <meta itemprop="num_attr" content="0116">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the rotational DOFs r x and r y</span>
        <span itemprop="definition">are referred to as orientation parameters.</span>
        <meta itemprop="num_attr" content="0116">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">performs a parallel translation of the long-distance vertices Q 1 in a longitudinal direction while changing r x , and performs a parallel translation of the long-distance vertices Q 1 in a transverse direction while changing r y .</span>
        <meta itemprop="num_attr" content="0117">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">searches for values of r x and r y which make a score calculated for the long-distance vertices Q 1 to be greater than or equal to a target value.</span>
        <meta itemprop="num_attr" content="0117">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">short-distance vertices</span>
        <span itemprop="definition">are selected from the map data.</span>
        <meta itemprop="num_attr" content="0118">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the short-distance vertices</span>
        <span itemprop="definition">are selected based on r z and t y being determined by the advance calibration and r x and r y being determined by the long-distance vertices Q 1 .</span>
        <meta itemprop="num_attr" content="0118">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">selects vertices Q 2 corresponding to lines towards the vanishing points from among the short-distance vertices, and selects the other vertices Q 3 .</span>
        <meta itemprop="num_attr" content="0119">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the vertices Q 2</span>
        <span itemprop="definition">are not substantially affected by the forward and backward (movement-related) translational DOF t z from among the DOFs of the localization parameters.</span>
        <meta itemprop="num_attr" content="0119">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the translational DOF t x</span>
        <span itemprop="definition">is determined based on the vertices Q 2 .</span>
        <meta itemprop="num_attr" content="0119">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the translational DOF t z</span>
        <span itemprop="definition">is determined based on the vertices Q 3 .</span>
        <meta itemprop="num_attr" content="0119">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the translational DOFs t x and t z</span>
        <span itemprop="definition">are referred to as position parameters.</span>
        <meta itemprop="num_attr" content="0119">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 9A through 9E</span>
        <span itemprop="definition">illustrate an example of a result of applying an optimization technique.</span>
        <meta itemprop="num_attr" content="0120">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 9A</span>
        <span itemprop="definition">Features 911 extracted from a feature map 910 and vertices 921 projected from map data 920 are illustrated in FIG. 9A .</span>
        <meta itemprop="num_attr" content="0121">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Initial localization information on the map data 920</span>
        <span itemprop="definition">is inaccurate, and thus, the features 911 and the vertices 921 do not match.</span>
        <meta itemprop="num_attr" content="0121">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a process of sequentially determining DOFs of localization parameters to match the features 911 and the vertices 921</span>
        <span itemprop="definition">will be described.</span>
        <meta itemprop="num_attr" content="0121">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">t y and r z</span>
        <span itemprop="definition">are calibrated in advance.</span>
        <meta itemprop="num_attr" content="0122">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus</span>
        <span itemprop="definition">removes a roll effect from the feature map 910 and the map data 920 .</span>
        <meta itemprop="num_attr" content="0123">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">removes the roll effect from the feature map 910 by rotating the feature map 910 based on r z that is calibrated in advance. Further, the localization apparatus detects vertices in the vicinity of the initial localization information from the map data 920 , approximates the detected vertices to a plane, and rotates the map data 920 to remove a roll of the plane.</span>
        <meta itemprop="num_attr" content="0123">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">corrects heights of the vertices of the map data 920 using t y that is calibrated in advance.</span>
        <meta itemprop="num_attr" content="0124">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">deduces r x and r y corresponding to a parallel translation using long-distance vertices 940 in a projection image of map data based on initial localization information.</span>
        <meta itemprop="num_attr" content="0125">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">performs a parallel translation of the long-distance vertices 940 such that a correlation between the long-distance vertices 940 and the feature map 910 is greater than or equal to a target value. For example, by rotating the vertices on the map data 920 through an adjustment of r x as in 945 , the long-distance vertices 940 in the projection image may be matched well with the features of the feature map 910 .</span>
        <meta itemprop="num_attr" content="0125">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">obtains a vanishing point of neighboring lines 950 by analyzing the vertices in the projection image.</span>
        <meta itemprop="num_attr" content="0126">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">aligns the vanishing point of the neighboring lines 950 at a position in the feature map 910 , for example, a center of a feature map.</span>
        <meta itemprop="num_attr" content="0126">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neighboring lines</span>
        <span itemprop="definition">have a characteristic of being invariant to a z-directional translation.</span>
        <meta itemprop="num_attr" content="0126">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">moves vertices corresponding to the neighboring lines 950 in the x direction such that a correlation between the neighboring lines 950 and the feature map 910 is greater than or equal to a target value. For example, by moving the vertices on the map data 920 through an adjustment of t x as in 955 , vertices corresponding to the neighboring lines 950 in the projection image may be matched well with the features of the feature map 910 .</span>
        <meta itemprop="num_attr" content="0126">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">detects a z-directional translation using remaining vertices 960 excluding vanishing point-directional lanes from among short-distance vertices in the projection image.</span>
        <meta itemprop="num_attr" content="0127">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">moves the remaining vertices 960 in the z direction such that a correlation between the remaining vertices 960 and the feature map 910 is greater than or equal to a target value. For example, by moving the vertices on the map data 920 through an adjustment of t z as in 965 , the remaining vertices 960 in the projection image may be matched well with the features of the feature map 910 .</span>
        <meta itemprop="num_attr" content="0127">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 10</span>
        <span itemprop="definition">illustrates an example of a localization method by parameter updating.</span>
        <meta itemprop="num_attr" content="0128">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the operations in FIG. 10</span>
        <span itemprop="definition">may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in FIG. 10 may be performed in parallel or concurrently.</span>
        <meta itemprop="num_attr" content="0128">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more blocks of FIG. 10 , and combinations of the blocks,</span>
        <span itemprop="definition">can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions.</span>
        <meta itemprop="num_attr" content="0128">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1-9E</span>
        <span itemprop="definition">are also applicable to FIG. 10 , and are incorporated herein by reference. Thus, the above description may not be repeated here</span>
        <meta itemprop="num_attr" content="0128">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a feature map</span>
        <span itemprop="definition">is extracted from an input image.</span>
        <meta itemprop="num_attr" content="0129">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">initial localization information</span>
        <span itemprop="definition">for example, initial values of position/orientation parameters.</span>
        <meta itemprop="num_attr" content="0129">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">scores of current parameters</span>
        <span itemprop="definition">are calculated, and a direction to improve the scores of the current parameters is calculated.</span>
        <meta itemprop="num_attr" content="0129">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the feature map</span>
        <span itemprop="definition">includes a probability distribution indicating a degree of closeness to an object.</span>
        <meta itemprop="num_attr" content="0130">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">features included in the feature map</span>
        <span itemprop="definition">include information related to a distance to a closest object, the information expressed using normalized values between â0â and â1â.</span>
        <meta itemprop="num_attr" content="0130">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the feature map</span>
        <span itemprop="definition">provides information related to a direction toward the object.</span>
        <meta itemprop="num_attr" content="0130">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">pools feature values of the feature map corresponding to 2D vertices projected from the map data by the current parameters. The localization apparatus determines the direction to improve the scores of the current parameters based on the pooled feature values.</span>
        <meta itemprop="num_attr" content="0130">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the parameters</span>
        <span itemprop="definition">are updated, in operation 1050 .</span>
        <meta itemprop="num_attr" content="0131">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">updates the parameters based on the direction calculated in operation 1030 .</span>
        <meta itemprop="num_attr" content="0131">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Operations 1050 , 1030 , and 1040</span>
        <span itemprop="definition">are iteratively performed until the iteration termination condition is satisfied.</span>
        <meta itemprop="num_attr" content="0131">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the iteration termination condition</span>
        <span itemprop="definition">includes whether the scores of the parameters are greater than or equal to a target value. In an example, the iteration termination condition further includes whether an iteration count exceeds a threshold for system stability.</span>
        <meta itemprop="num_attr" content="0131">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the current parameters</span>
        <span itemprop="definition">are selected as final localization information, for example, final position/orientation parameters, in operation 1060 .</span>
        <meta itemprop="num_attr" content="0132">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a step by step search for determining better parameters from initial values  â _0</span>
        <span itemprop="definition">is performed.</span>
        <meta itemprop="num_attr" content="0133">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Such a local optimization scheme</span>
        <span itemprop="definition">requires good initial values for better performance.</span>
        <meta itemprop="num_attr" content="0133">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the example of FIG. 10</span>
        <span itemprop="definition">selectively includes an operation of separately estimating the initial values.</span>
        <meta itemprop="num_attr" content="0133">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the example of FIG. 10</span>
        <span itemprop="definition">is performed while the parameters obtained through the example of FIG. 8 are regarded as the initial values.</span>
        <meta itemprop="num_attr" content="0133">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an effect of correcting values</span>
        <span itemprop="definition">such as a camera height and a roll fixed through advance calibration to be suitable for a variation occurring in a real driving environment is achieved.</span>
        <meta itemprop="num_attr" content="0133">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 11</span>
        <span itemprop="definition">illustrates an example of a step by step result of applying a localization method by parameter updating.</span>
        <meta itemprop="num_attr" content="0134">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an input image 1105 , a first image 1110 , and a second image 1120</span>
        <span itemprop="definition">are illustrated.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the first image 1110</span>
        <span itemprop="definition">is generated to correspond to the input image 1105 .</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the second image 1120</span>
        <span itemprop="definition">is an image generated by projecting an object with respect to localization information (x, y, z, r x , r y , and r z ) corresponding to initial localization information based on map data.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the second image 1120</span>
        <span itemprop="definition">is a projection image including a plurality of 2D vertices corresponding to the object.</span>
        <meta itemprop="num_attr" content="0135">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus</span>
        <span itemprop="definition">calculates a score by matching the first image 1110 and the second image 1120 as shown in an image 1130 .</span>
        <meta itemprop="num_attr" content="0136">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">calculates the score by summing up values of pixels corresponding to the object included in the second image 1120 , among a plurality of pixels included in the first image 1110 .</span>
        <meta itemprop="num_attr" content="0136">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the plurality of pixels included in the first image 1110</span>
        <span itemprop="definition">has values between â0â and â1â based on distances to an adjacent object.</span>
        <meta itemprop="num_attr" content="0137">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each pixel</span>
        <span itemprop="definition">has a value close to â1â as being close to the adjacent object and has a value close to â0â as being far from the adjacent object.</span>
        <meta itemprop="num_attr" content="0137">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">extracts pixels matching the second image 1120 from the plurality of pixels included in the first image 1110 , and calculates the score by summing up values of the extracted pixels.</span>
        <meta itemprop="num_attr" content="0137">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">corrects the localization information to increase a degree of visual alignment, i.e., the score , based on a directivity of the first image 1110 .</span>
        <meta itemprop="num_attr" content="0138">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">calculates a localization correction value such that localization information of the object included in the second image 1120 accords with the directivity of the first image 1110 .</span>
        <meta itemprop="num_attr" content="0138">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">applies the localization correction value to the localization information corresponding to the initial localization information, thereby updating the localization information from  â  , in operation 1140 .</span>
        <meta itemprop="num_attr" content="0138">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">determines a direction in which the object of the second image 1120 is to be moved to increase the score, based on the directivity of the first image 1110 .</span>
        <meta itemprop="num_attr" content="0138">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the object of the second image 1120</span>
        <span itemprop="definition">is moved when the localization information is updated, and thus the localization apparatus updates the localization information based on the directivity included in the first image 1110 .</span>
        <meta itemprop="num_attr" content="0138">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">generates an updated second image 1150 based on the updated localization information .</span>
        <meta itemprop="num_attr" content="0139">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">calculates a score by matching the updated second image 1150 and the first image 1110 .</span>
        <meta itemprop="num_attr" content="0139">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus</span>
        <span itemprop="definition">outputs finally optimized localization information by calculating a localization correction value which makes the score to be greater than or equal to a criterion through the process described above.</span>
        <meta itemprop="num_attr" content="0140">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 12</span>
        <span itemprop="definition">illustrates an example of a neural network to generate a feature map.</span>
        <meta itemprop="num_attr" content="0141">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a process of generating a distance field map 1250 corresponding to a first image by applying an input image 1210 to a neural network 1230</span>
        <span itemprop="definition">is illustrated.</span>
        <meta itemprop="num_attr" content="0142">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 1230</span>
        <span itemprop="definition">is trained to generate a first image including a directivity corresponding to an object included in the input image 1210 based on the input image 1210 .</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 1230</span>
        <span itemprop="definition">is implemented on a hardware-based model comprising a framework or a structure of a number of layers or operations to provide for many different machine learning algorithms to work together, process complex data inputs, and recognize patterns.</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 1230</span>
        <span itemprop="definition">is implemented in various structures such as, for example, a convolutional neural network (CNN), a deep neural network (DNN), an n-layer neural network, a recurrent neural network (RNN), or a bidirectional long short term memory (BLSTM).</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">CNN</span>
        <span itemprop="definition">convolutional neural network</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DNN</span>
        <span itemprop="definition">deep neural network</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">RNN</span>
        <span itemprop="definition">n-layer neural network</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">RNN</span>
        <span itemprop="definition">recurrent neural network</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">BLSTM</span>
        <span itemprop="definition">bidirectional long short term memory</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the DNN</span>
        <span itemprop="definition">includes, for example, a fully connected network, a CNN, a deep convolutional network, or a recurrent neural network (RNN), a deep belief network, a bi-directional neural network, a restricted Boltzman machine, or may include different or overlapping neural network portions respectively with full, convolutional, recurrent, and/or bi-directional connections.</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 1230</span>
        <span itemprop="definition">maps, based on deep learning, input data and output data that are in a non-linear relationship, to perform, for example, an object classification, an object recognition, a speech recognition, or an image recognition.</span>
        <meta itemprop="num_attr" content="0143">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network</span>
        <span itemprop="definition">may be implemented as an architecture having a plurality of layers including an input image, feature maps, and an output.</span>
        <meta itemprop="num_attr" content="0144">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a convolution operation between the input image, and a filter referred to as a kernel</span>
        <span itemprop="definition">is performed, and as a result of the convolution operation, the feature maps are output.</span>
        <meta itemprop="num_attr" content="0144">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the feature maps that are output</span>
        <span itemprop="definition">are input feature maps, and a convolution operation between the output feature maps and the kernel is performed again, and as a result, new feature maps are output. Based on such repeatedly performed convolution operations, results of recognition of characteristics of the input image via the neural network may be output.</span>
        <meta itemprop="num_attr" content="0144">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the neural network 1230</span>
        <span itemprop="definition">estimates the object included in the input image 1210 in a form of the distance field map 1250 .</span>
        <meta itemprop="num_attr" content="0145">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a directivity of optimization</span>
        <span itemprop="definition">is determined by utilizing gradient descent.</span>
        <meta itemprop="num_attr" content="0145">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a probability distribution indicating a degree of closeness to the object</span>
        <span itemprop="definition">is present all over the image as in the distance field map 1250 , an amount of data for training increases, and thus the performance of the neural network improves when compared to a case of training with sparse data.</span>
        <meta itemprop="num_attr" content="0145">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 13</span>
        <span itemprop="definition">illustrates an example of a localization apparatus.</span>
        <meta itemprop="num_attr" content="0146">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus 1300</span>
        <span itemprop="definition">includes sensors 1310 and a processor 1330 .</span>
        <meta itemprop="num_attr" content="0147">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus 1300</span>
        <span itemprop="definition">further includes the memory 1350 , the communication interface 1370 , and the display device 1390 .</span>
        <meta itemprop="num_attr" content="0147">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the sensors 1310 , the processor 1330 , the memory 1350 , the communication interface 1370 , and the display device 1390</span>
        <span itemprop="definition">are connected to each other through a communication bus 1305 .</span>
        <meta itemprop="num_attr" content="0147">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the sensor(s) 1310</span>
        <span itemprop="definition">include, for example, an image sensor, a vision sensor, an acceleration sensor, a gyro sensor, a GPS sensor, an IMU sensor, a Radar, and a Lidar.</span>
        <meta itemprop="num_attr" content="0148">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the sensor(s) 1310</span>
        <span itemprop="definition">acquire or capture an input image including a driving image of a vehicle.</span>
        <meta itemprop="num_attr" content="0148">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the sensor(s) 1310</span>
        <span itemprop="definition">senses information such as, for example, a speed, an acceleration, a travelling direction, and a steering angle of the vehicle, in addition to localization information such as, for example, GPS coordinates, a position, and an orientation of the vehicle is sensed by the sensor(s) 1310 .</span>
        <meta itemprop="num_attr" content="0148">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatus 1300</span>
        <span itemprop="definition">obtains sensing information of various sensors including the input image through the communication interface 1370 .</span>
        <meta itemprop="num_attr" content="0149">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the communication interface 1370</span>
        <span itemprop="definition">receives sensing information including a driving image from other sensors existing outside of the localization apparatus 1300 .</span>
        <meta itemprop="num_attr" content="0149">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1330</span>
        <span itemprop="definition">outputs the corrected localization information through the communication interface 1370 and/or the display device 1390 , or displays a virtual object along with the input image on map data based on the corrected localization information, thereby providing an AR service. Further, the processor 1330 performs the at least one method described above through FIGS. 1 through 13 or an algorithm corresponding to the at least one method.</span>
        <meta itemprop="num_attr" content="0150">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1330</span>
        <span itemprop="definition">is a data processing device implemented by hardware including a circuit having a physical structure to perform desired operations.</span>
        <meta itemprop="num_attr" content="0151">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the desired operations</span>
        <span itemprop="definition">include instructions or codes included in a program.</span>
        <meta itemprop="num_attr" content="0151">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hardware-implemented data processing device</span>
        <span itemprop="definition">includes a microprocessor, a central processing unit (CPU), a processor core, a multi-core processor, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA).</span>
        <meta itemprop="num_attr" content="0151">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1330</span>
        <span itemprop="definition">may be a graphics processor unit (GPU), reconfigurable processor, or have any other type of multi- or single-processor configuration.</span>
        <meta itemprop="num_attr" content="0151">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1330</span>
        <span itemprop="definition">executes the program and controls the localization apparatus 1300 .</span>
        <meta itemprop="num_attr" content="0151">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor 1330</span>
        <span itemprop="definition">executes the program and controls the neural network 1230 .</span>
        <meta itemprop="num_attr" content="0151">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the program codes to be executed by the processor 1330</span>
        <span itemprop="definition">are stored in the memory 1350 . Further details regarding the processor 1330 is provided below.</span>
        <meta itemprop="num_attr" content="0151">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1350</span>
        <span itemprop="definition">stores the localization information of the localization apparatus 1300 , the first image, the second image, and/or the corrected localization information.</span>
        <meta itemprop="num_attr" content="0152">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1350</span>
        <span itemprop="definition">stores a variety of information generated during the processing process performed by the processor 1330 .</span>
        <meta itemprop="num_attr" content="0152">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1350</span>
        <span itemprop="definition">stores a variety of data and programs.</span>
        <meta itemprop="num_attr" content="0152">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1350</span>
        <span itemprop="definition">includes a volatile memory or a non-volatile memory.</span>
        <meta itemprop="num_attr" content="0152">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory 1350</span>
        <span itemprop="definition">includes a large capacity storage medium such as a hard disk to store the variety of data. Further details regarding the memory 1120 is provided below.</span>
        <meta itemprop="num_attr" content="0152">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the display device 1390</span>
        <span itemprop="definition">outputs the localization information corrected by the processor 1330 , or displays the virtual object along with the input image on the map data based on the corrected localization information.</span>
        <meta itemprop="num_attr" content="0153">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the display device 1390</span>
        <span itemprop="definition">is a physical structure that includes one or more hardware components that provide the ability to render a user interface, render a display, and/or receive user input.</span>
        <meta itemprop="num_attr" content="0153">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the display device 1390</span>
        <span itemprop="definition">is not limited to the example described above, and any other displays, such as, for example, smart phone and eye glass display (EGD) that are operatively connected to the localization apparatus 1300 may be used without departing from the spirit and scope of the illustrative examples described.</span>
        <meta itemprop="num_attr" content="0153">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a localization apparatus</span>
        <span itemprop="definition">performs localization independently of viewpoints by updating 3D localization information of the localization apparatus using a result of performing the localization method described above based on a capturing device even when viewpoints of the capturing device and the localization apparatus do not match like a HUD or AR glasses. Further, the localization apparatus updates the 3D localization information and is also used to directly correct a 2D position in an image when the viewpoints of the capturing device and the localization apparatus match like a mobile terminal or a smart phone.</span>
        <meta itemprop="num_attr" content="0154">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Examples set forth herein</span>
        <span itemprop="definition">provide technology for localization without establishing correspondence between vertices of an image and vertices of map data. Further, the examples provide technology for localization without parameterizing features of an image, extracting a relation invariant to a three-dimensional (3D) transform and a perspective transform, or easily specifying such an invariant relation during a search of map data.</span>
        <meta itemprop="num_attr" content="0155">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">3D</span>
        <span itemprop="definition">three-dimensional</span>
        <meta itemprop="num_attr" content="0155">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the localization apparatuses 200 and 1300 , transform devices 210 and 220 , feature extractor 230 , pooler 240 , and other apparatuses, units, modules, devices, and other components described herein with respect to FIGS. 1-13</span>
        <span itemprop="definition">are implemented by hardware components.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">hardware components</span>
        <span itemprop="definition">that may be used to perform the operations described in this application where appropriate include controllers, sensors, generators, drivers, memories, comparators, arithmetic logic units, adders, subtractors, multipliers, dividers, integrators, and any other electronic components configured to perform the operations described in this application.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">one or more of the hardware components that perform the operations described in this application</span>
        <span itemprop="definition">are implemented by computing hardware, for example, by one or more processors or computers.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a processor or computer</span>
        <span itemprop="definition">may be implemented by one or more processing elements, such as an array of logic gates, a controller and an arithmetic logic unit, a digital signal processor, a microcomputer, a programmable logic controller, a field-programmable gate array, a programmable logic array, a microprocessor, or any other device or combination of devices that is configured to respond to and execute instructions in a defined manner to achieve a desired result.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a processor or computer</span>
        <span itemprop="definition">includes, or is connected to, one or more memories storing instructions or software that are executed by the processor or computer.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Hardware components implemented by a processor or computer</span>
        <span itemprop="definition">may execute instructions or software, such as an operating system (OS) and one or more software applications that run on the OS, to perform the operations described in this application.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">OS</span>
        <span itemprop="definition">operating system</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the hardware components</span>
        <span itemprop="definition">may also access, manipulate, process, create, and store data in response to execution of the instructions or software.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">processor</span>
        <span itemprop="definition">or âcomputerâ may be used in the description of the examples described in this application, but in other examples multiple processors or computers may be used, or a processor or computer may include multiple processing elements, or multiple types of processing elements, or both.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a single hardware component or two or more hardware components</span>
        <span itemprop="definition">may be implemented by a single processor, or two or more processors, or a processor and a controller.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more hardware components</span>
        <span itemprop="definition">may be implemented by one or more processors, or a processor and a controller, and one or more other hardware components may be implemented by one or more other processors, or another processor and another controller.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more processors</span>
        <span itemprop="definition">may implement a single hardware component, or two or more hardware components.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a hardware component</span>
        <span itemprop="definition">may have any one or more of different processing configurations, examples of which include a single processor, independent processors, parallel processors, single-instruction single-data (SISD) multiprocessing, single-instruction multiple-data (SIMD) multiprocessing, multiple-instruction single-data (MISD) multiprocessing, and multiple-instruction multiple-data (MIMD) multiprocessing.</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SISD</span>
        <span itemprop="definition">single-instruction single-data</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">SIMD</span>
        <span itemprop="definition">single-instruction multiple-data</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">MIMD</span>
        <span itemprop="definition">multiple-instruction multiple-data</span>
        <meta itemprop="num_attr" content="0156">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 1-13</span>
        <span itemprop="definition">that perform the operations described in this application are performed by computing hardware, for example, by one or more processors or computers, implemented as described above executing instructions or software to perform the operations described in this application that are performed by the methods.</span>
        <meta itemprop="num_attr" content="0157">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a single operation or two or more operations</span>
        <span itemprop="definition">may be performed by a single processor, or two or more processors, or a processor and a controller.</span>
        <meta itemprop="num_attr" content="0157">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more operations</span>
        <span itemprop="definition">may be performed by one or more processors, or a processor and a controller, and one or more other operations may be performed by one or more other processors, or another processor and another controller.</span>
        <meta itemprop="num_attr" content="0157">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">One or more processors, or a processor and a controller</span>
        <span itemprop="definition">may perform a single operation, or two or more operations.</span>
        <meta itemprop="num_attr" content="0157">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Instructions or software to control a processor or computer to implement the hardware components and perform the methods as described above</span>
        <span itemprop="definition">are written as computer programs, code segments, instructions or any combination thereof, for individually or collectively instructing or configuring the processor or computer to operate as a machine or special-purpose computer to perform the operations performed by the hardware components and the methods as described above.</span>
        <meta itemprop="num_attr" content="0158">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software</span>
        <span itemprop="definition">includes at least one of an applet, a dynamic link library (DLL), middleware, firmware, a device driver, an application program storing the method of outputting the state information.</span>
        <meta itemprop="num_attr" content="0158">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software</span>
        <span itemprop="definition">include machine code that is directly executed by the processor or computer, such as machine code produced by a compiler.</span>
        <meta itemprop="num_attr" content="0158">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software</span>
        <span itemprop="definition">include higher-level code that is executed by the processor or computer using an interpreter. Programmers of ordinary skill in the art can readily write the instructions or software based on the block diagrams and the flow charts illustrated in the drawings and the corresponding descriptions in the specification, which disclose algorithms for performing the operations performed by the hardware components and the methods as described above.</span>
        <meta itemprop="num_attr" content="0158">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software to control computing hardware</span>
        <span itemprop="definition">for example, one or more processors or computers, to implement the hardware components and perform the methods as described above, and any associated data, data files, and data structures, may be recorded, stored, or fixed in or on one or more non-transitory computer-readable storage media.</span>
        <meta itemprop="num_attr" content="0159">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Examples of a non-transitory computer-readable storage medium</span>
        <span itemprop="definition">include read-only memory (ROM), random-access programmable read only memory (PROM), electrically erasable programmable read-only memory (EEPROM), random-access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), flash memory, non-volatile memory, CD-ROMs, CD-Rs, CD&#43;Rs, CD-RWs, CD&#43;RWs, DVD-ROMs, DVD-Rs, DVD&#43;Rs, DVD-RWs, DVD&#43;RWs, DVD-RAMs, BD-ROMs, BD-Rs, BD-R LTHs, BD-REs, blue-ray or optical disk storage, hard disk drive (HDD), solid state drive (SSD), flash memory, card type memory such as multimedia card, secure digital (SD) card, or extreme digital (XD) card, magnetic tapes, floppy disks, magneto-optical data storage devices, optical data storage devices, hard disks, solid-state disks, and</span>
        <meta itemprop="num_attr" content="0159">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the instructions or software and any associated data, data files, and data structures</span>
        <span itemprop="definition">are distributed over network-coupled computer systems so that the instructions and software and any associated data, data files, and data structures are stored, accessed, and executed in a distributed fashion by the one or more processors or computers.</span>
        <meta itemprop="num_attr" content="0159">
      </li>
    </ul>
  </section>

  


  <section itemprop="abstract" itemscope>
    <h2>Abstract</h2>
    
    <div itemprop="content" html><abstract mxw-id="PA473773165" lang="EN" source="national office" load-source="docdb">
    <div class="abstract">Localization apparatuses and methods are disclosed where a localization apparatus extracts a feature of an object from an input image, generates an image in which the object is projected with respect to localization information of a device based on map data, and evaluates the localization information based on feature values corresponding to vertices included in a projection image.</div>
  </abstract>
  </div>
  </section>

  <section itemprop="description" itemscope>
    <h2>Description</h2>
    
    <div itemprop="content" html><div mxw-id="PDES310886823" lang="EN" load-source="patent-office" class="description">
    
    <heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
    <div id="p-0002" num="0001" class="description-paragraph">This application claims the benefit under 35 USC Â§ 119(a) of Korean Patent Application No. 10-2018-0127589 filed on Oct. 24, 2018 in the Korean Intellectual Property Office, the entire disclosure of which is incorporated herein by reference for all purposes.</div>
    <heading id="h-0002"> <figure-callout id="1" label="BACKGROUND" filenames="US11176719-20211116-D00007.png" state="{{state}}">BACKGROUND</figure-callout> </heading>
    <heading id="h-0003">1. Field</heading>
    <div id="p-0003" num="0002" class="description-paragraph">The following description relates to a method and apparatus for localization based on images and map data.</div>
    <heading id="h-0004">2. Description of Related Art</heading>
    <div id="p-0004" num="0003" class="description-paragraph">Various types of augmented reality (AR) services are provided in fields such as driving assistance for vehicles and other means of transportation, games, or entertainment. To provide more accurate and realistic AR, a number of localization methods are used. For example, a sensor-based localization method uses a combination of sensors such as a global positioning system (GPS) sensor and an inertial measurement unit (IMU) sensor to determine a position and an orientation of an object. Further, a vision-based localization method uses camera information.</div>
    <heading id="h-0005">SUMMARY</heading>
    <div id="p-0005" num="0004" class="description-paragraph">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.</div>
    <div id="p-0006" num="0005" class="description-paragraph">In one general aspect, there is disclosed a localization method, including generating a first image of an object from an input image, generating a second image to project the object with respect to candidate localization information of a device, based on map data including a position of the object, pooling, from the first image, feature values corresponding to vertices in the second image, and determining a score of the candidate localization information based on the pooled feature values.</div>
    <div id="p-0007" num="0006" class="description-paragraph">The generating of the first image may include generating feature maps corresponding to a plurality of features.</div>
    <div id="p-0008" num="0007" class="description-paragraph">The generating of the second image may include extracting a region corresponding to a field of view in the candidate localization information from the map data, and projecting vertices included in the region into a projection point corresponding to the candidate localization information.</div>
    <div id="p-0009" num="0008" class="description-paragraph">The pooling may include selecting pixels in the first image based on coordinates of the vertices, and obtaining feature values of the selected pixels.</div>
    <div id="p-0010" num="0009" class="description-paragraph">The determining may include determining a sum of the pooled feature values.</div>
    <div id="p-0011" num="0010" class="description-paragraph">The determining of the sum may include, determining a weighted sum of the feature values based on weights determined for the features, in response to the first image may include feature maps corresponding to features.</div>
    <div id="p-0012" num="0011" class="description-paragraph">The localization method may include determining localization information of the device based on the score of the candidate localization information.</div>
    <div id="p-0013" num="0012" class="description-paragraph">The determining of the localization information of the device may include determining candidate localization information corresponding to a highest score, from among scores of a plurality of candidate localization information, to be the localization information of the device.</div>
    <div id="p-0014" num="0013" class="description-paragraph">The determining of the localization information of the device may include segmenting the second image into regions, and sequentially determining a plurality of degree of freedom (DOF) values included in the candidate localization information using scores calculated in the regions.</div>
    <div id="p-0015" num="0014" class="description-paragraph">The plurality of DOF values may include three translational DOF values, and three rotational DOF values.</div>
    <div id="p-0016" num="0015" class="description-paragraph">The segmenting may include segmenting the second image into a long-distance region and a short-distance region based on a first criterion associated with a distance, and segmenting the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point.</div>
    <div id="p-0017" num="0016" class="description-paragraph">The sequentially determining may include determining rotational DOFs based on the long-distance region, determining a left and right translational DOF based on the vanishing point-oriented short-distance region, and determining a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</div>
    <div id="p-0018" num="0017" class="description-paragraph">The determining may include determining rotational DOFs based on long-distance vertices affected below a first threshold by translational DOFs, from among vertices included in the second image, determining a left and right translational DOF based on vanishing point-oriented short-distance vertices affected below a second threshold by a forward and backward translational DOF, from among short-distance vertices excluding the long-distance vertices from the second image, and determining the forward and backward translational DOF based on non-vanishing point-oriented short-distance vertices excluding the vanishing point-oriented short-distance vertices from the short-distance vertices.</div>
    <div id="p-0019" num="0018" class="description-paragraph">The determining of the localization information of the device may include determining a direction to improve the score based on a distribution of the pooled feature values, and correcting the candidate localization information based on the direction.</div>
    <div id="p-0020" num="0019" class="description-paragraph">The first image may include a probability distribution indicating a degree of closeness to the object, wherein the determining of the direction may include determining the direction based on the probability distribution.</div>
    <div id="p-0021" num="0020" class="description-paragraph">The determining of the localization information of the device may include generating a corrected second image in which the object is projected with respect to the corrected candidate localization information, and determining a corrected score of the corrected candidate localization information by pooling, from the first image, feature values corresponding to vertices in the corrected second image, wherein the determining of the direction, the correcting of the candidate localization information, the generating of the corrected second image, and the calculating of the corrected score are iteratively performed until the corrected score satisfies a condition.</div>
    <div id="p-0022" num="0021" class="description-paragraph">The localization method may include determining a virtual object on the map data to provide an augmented reality (AR) service, and displaying the virtual object based on the determined localization information.</div>
    <div id="p-0023" num="0022" class="description-paragraph">The input image may include a driving image of a vehicle, and the virtual object indicates driving route information.</div>
    <div id="p-0024" num="0023" class="description-paragraph">In another general aspect, there is disclosed a localization method, including generating a first image of an object from an input image, generating a second image to project the object with respect to candidate localization information of a device, based on map data including a position of the object, segmenting the second image into regions, and determining degree of freedom (DOF) values included in the candidate localization information through matching between the first image and the regions.</div>
    <div id="p-0025" num="0024" class="description-paragraph">The determining may include determining the DOF values included in the candidate localization information by sequentially using scores calculated through the matching in the regions.</div>
    <div id="p-0026" num="0025" class="description-paragraph">The determining may include calculating, while changing DOF values determined for the regions, scores corresponding to the changed DOF values by pooling, from the first image, feature values corresponding to vertices in the regions, and selecting a DOF value corresponding to a highest score.</div>
    <div id="p-0027" num="0026" class="description-paragraph">The plurality of DOF values may include three translational DOF values, and three rotational DOF values.</div>
    <div id="p-0028" num="0027" class="description-paragraph">The segmenting may include segmenting the second image into a long-distance region and a short-distance region based on a first criterion associated with a distance, and segmenting the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point.</div>
    <div id="p-0029" num="0028" class="description-paragraph">The determining may include determining rotational DOFs based on the long-distance region, determining a left and right translational DOF based on the vanishing point-oriented short-distance region, and determining a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</div>
    <div id="p-0030" num="0029" class="description-paragraph">The determining may include determining rotational DOFs based on long-distance vertices affected below a first threshold by translational DOFs, from among vertices included in the second image, determining a left and right translational DOF based on vanishing point-oriented short-distance vertices affected below a second threshold by a forward and backward translational DOF, from among short-distance vertices excluding the long-distance vertices from the second image, and determining the forward and backward translational DOF based on non-vanishing point-oriented short-distance vertices excluding the vanishing point-oriented short-distance vertices from the short-distance vertices.</div>
    <div id="p-0031" num="0030" class="description-paragraph">The localization method may include determining a virtual object on the map data to provide an augmented reality (AR) service, and displaying the virtual object based on the determined DOF values.</div>
    <div id="p-0032" num="0031" class="description-paragraph">The input image may include a driving image of a vehicle, and the virtual object indicates driving route information.</div>
    <div id="p-0033" num="0032" class="description-paragraph">In another general aspect, there is disclosed a localization apparatus, including a processor configured to generate a first image of an object from an input image, generate a second image to project the object with respect to candidate localization information of a device, based on map data including a position of the object, pool, from the first image, feature values corresponding to vertices in the second image, and determine a score of the candidate localization information based on the pooled feature values.</div>
    <div id="p-0034" num="0033" class="description-paragraph">In another general aspect, there is disclosed a localization apparatus, including a processor configured to generate a first image of an object from an input image, generate a second image to project the object with respect to candidate localization information of a device, based on map data including a position of the object, segment the second image into regions, and determine degree of freedom (DOF) values included in the candidate localization information through matching between the first image and the regions.</div>
    <div id="p-0035" num="0034" class="description-paragraph">In another general aspect, there is disclosed a localization apparatus including a sensor disposed on a device, and being configured to sense one or more of an image and candidate localization information of the device, a processor configured to generate a first image of an object from the image, generate a second image to project the object with respect to the candidate localization information, based on map data including a position of the object, determine a score of the candidate localization information based on pooling, from the first image, feature values corresponding to vertices in the second image, and determine localization information of the device based on the score, and a head-up display (HUD) configured to visualize a virtual object on the map data based on the determined localization information.</div>
    <div id="p-0036" num="0035" class="description-paragraph">The processor may be configured to segment the second image into a long-distance region and a short-distance region based on a distance, and segment the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a vanishing point.</div>
    <div id="p-0037" num="0036" class="description-paragraph">The processor may be configured to determine rotational degree of freedom (DOF) values based on the long-distance region, determine a left and right translational DOF based on the vanishing point-oriented short-distance region, and determine a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</div>
    <div id="p-0038" num="0037" class="description-paragraph">The processor may be configured to generate, using a neural network, the first image may include feature maps corresponding to a plurality of features.</div>
    <div id="p-0039" num="0038" class="description-paragraph">The second image may include a projection of two-dimensional (2D) vertices corresponding to the object.</div>
    <div id="p-0040" num="0039" class="description-paragraph">The localization apparatus may include a memory configured to store the map data, the image, the first image, the second image, the score, and instructions that, when executed, configures the processor to determine any one or any combination of the determined localization information and the virtual object.</div>
    <div id="p-0041" num="0040" class="description-paragraph">Other features and aspects will be apparent from the following detailed description, the drawings, and the claims.</div>
    
    
    <description-of-drawings>
      <heading id="h-0006">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <div id="p-0042" num="0041" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. 1A through 1C</figref> illustrate examples of significance of localization accuracy in an augmented reality (AR) application.</div>
      <div id="p-0043" num="0042" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates an example of calculating a localization score.</div>
      <div id="p-0044" num="0043" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates an example of calculating a localization score.</div>
      <div id="p-0045" num="0044" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates an example of determining localization information of a device by utilizing a localization score.</div>
      <div id="p-0046" num="0045" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates an example of scores of pieces of candidate localization information.</div>
      <div id="p-0047" num="0046" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates an example of a localization method.</div>
      <div id="p-0048" num="0047" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates an example of determining localization information of a device through an optimization technique.</div>
      <div id="p-0049" num="0048" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. 8A and 8B</figref> illustrate examples of optimization technique.</div>
      <div id="p-0050" num="0049" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. 9A through 9E</figref> illustrate examples of a result of applying an optimization technique.</div>
      <div id="p-0051" num="0050" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 10</figref> illustrates an example of a localization method by parameter updating.</div>
      <div id="p-0052" num="0051" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 11</figref> illustrates an example of a result of applying a localization method by parameter updating step by step.</div>
      <div id="p-0053" num="0052" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 12</figref> illustrates an example of a neural network to generate a feature map.</div>
      <div id="p-0054" num="0053" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 13</figref> illustrates an example of a localization apparatus.</div>
    </description-of-drawings>
    
    
    <div id="p-0055" num="0054" class="description-paragraph">Throughout the drawings and the detailed description, unless otherwise described or provided, the same drawing reference numerals will be understood to refer to the same elements, features, and structures. The drawings may not be to scale, and the relative size, proportions, and depiction of elements in the drawings may be exaggerated for clarity, illustration, and convenience.</div>
    <heading id="h-0007">DETAILED DESCRIPTION</heading>
    <div id="p-0056" num="0055" class="description-paragraph">The following detailed description is provided to assist the reader in gaining a comprehensive understanding of the methods, apparatuses, and/or systems described herein. However, various changes, modifications, and equivalents of the methods, apparatuses, and/or systems described herein will be apparent after an understanding of the disclosure of this application. For example, the sequences of operations described herein are merely examples, and are not limited to those set forth herein, but may be changed as will be apparent after an understanding of the disclosure of this application, with the exception of operations necessarily occurring in a certain order. Also, descriptions of features that are known in the art may be omitted for increased clarity and conciseness.</div>
    <div id="p-0057" num="0056" class="description-paragraph">The features described herein may be embodied in different forms and are not to be construed as being limited to the examples described herein. Rather, the examples described herein have been provided merely to illustrate some of the many possible ways of implementing the methods, apparatuses, and/or systems described herein that will be apparent after an understanding of the disclosure of this application.</div>
    <div id="p-0058" num="0057" class="description-paragraph">Terms, such as first, second, and the like, may be used herein to describe components. Each of these terminologies is not used to define an essence, order or sequence of a corresponding component but used merely to distinguish the corresponding component from other component(s). For example, a first component may be referred to as a second component, and similarly the second component may also be referred to as the first component.</div>
    <div id="p-0059" num="0058" class="description-paragraph">If the specification states that one component is âconnected,â âcoupled,â or âjoinedâ to a second component, the first component may be directly âconnected,â âcoupled,â or âjoinedâ to the second component, or a third component may be âconnected,â âcoupled,â or âjoinedâ between the first component and the second component. However, if the specification states that a first component is âdirectly connectedâ or âdirectly joinedâ to a second component, a third component may not be âconnectedâ or âjoinedâ between the first component and the second component. Similar expressions, for example, âbetweenâ and âimmediately betweenâ and âadjacent toâ and âimmediately adjacent to,â are also to be construed in this manner.</div>
    <div id="p-0060" num="0059" class="description-paragraph">If the specification states that one component is âconnected,â âcoupled,â or âjoinedâ to a second component, the first component may be directly âconnected,â âcoupled,â or âjoinedâ to the second component, or a third component may be âconnected,â âcoupled,â or âjoinedâ between the first component and the second component. However, if the specification states that a first component is âdirectly connectedâ or âdirectly joinedâ to a second component, a third component may not be âconnectedâ or âjoinedâ between the first component and the second component. Similar expressions, for example, âbetweenâ and âimmediately betweenâ and âadjacent toâ and âimmediately adjacent to,â are also to be construed in this manner.</div>
    <div id="p-0061" num="0060" class="description-paragraph">The terminology used herein is for the purpose of describing particular examples only and is not to be limiting of the examples. As used herein, the singular forms âaâ, âanâ, and âtheâ are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms âcomprises/comprisingâ and/or âincludes/includingâ when used herein, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components and/or groups thereof.</div>
    <div id="p-0062" num="0061" class="description-paragraph">The use of the term âmayâ herein with respect to an example or embodiment, e.g., as to what an example or embodiment may include or implement, means that at least one example or embodiment exists where such a feature is included or implemented while all examples and embodiments are not limited thereto.</div>
    <div id="p-0063" num="0062" class="description-paragraph">The examples set forth hereinafter may be implemented on hardware that is applied to technology for localization based on images and map data. For example, the examples may be used to improve an accuracy of localization in an augmented reality head-up display (AR HUD). Further, the localization is needed for a number of location-based services in addition to the HUD, and the examples may be used to estimate a position and an orientation in an environment in which high density (HD) map data is provided for high-precision localization.</div>
    <div id="p-0064" num="0063" class="description-paragraph">Hereinafter, examples will be described in detail with reference to the accompanying drawings. In the drawings, like reference numerals are used for like elements.</div>
    <div id="p-0065" num="0064" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. 1A through 1C</figref> illustrate examples of significance of localization accuracy in an AR application.</div>
    <div id="p-0066" num="0065" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIGS. 1A through 1C</figref>, in an example, AR adds or augments information based on reality and provides the added or augmented information. For example, AR adds a virtual object corresponding to a virtual image to a background image or an image of a real world and represents the image with the added object. AR appropriately combines a virtual world with the real world such that a user experiences an immersive experience when interacting with the virtual world in real time without recognizing a separation between real and virtual environments. To match a virtual object to a real image, a position and an orientation, i.e., localization information, of a user device or the user which provides AR, needs to be determined.</div>
    <div id="p-0067" num="0066" class="description-paragraph">Localization information for providing AR is used to dispose a virtual object at a desired position in an image. Hereinafter, for ease of description, an example of a driving guidance lane corresponding to a virtual object is displayed on a road surface. However, examples are not limited thereto.</div>
    <div id="p-0068" num="0067" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 1A</figref> illustrates an <figure-callout id="120" label="AR image" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00001.png" state="{{state}}">AR image</figure-callout> <b>120</b> having a relatively small localization error. <figref idrefs="DRAWINGS">FIG. 1B</figref> illustrates an <figure-callout id="140" label="AR image" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00002.png" state="{{state}}">AR image</figure-callout> <b>140</b> having a relatively great localization error.</div>
    <div id="p-0069" num="0068" class="description-paragraph">For example, a reference route of a vehicle is displayed on a road image based on localization information of an <figure-callout id="110" label="object" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00001.png" state="{{state}}">object</figure-callout> <b>110</b>. In an example, the object corresponds to a vehicle and/or a user terminal which performs localization. When the localization information of the <figure-callout id="110" label="object" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00001.png" state="{{state}}">object</figure-callout> <b>110</b> includes error within a small tolerance range, a driving <figure-callout id="115" label="guidance lane" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00001.png" state="{{state}}">guidance lane</figure-callout> <b>115</b> that is a virtual object to be displayed by a device is visually appropriately aligned with a real road image, as shown in the <figure-callout id="120" label="image" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00001.png" state="{{state}}">image</figure-callout> <b>120</b>. When localization information of an <figure-callout id="130" label="object" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00002.png" state="{{state}}">object</figure-callout> <b>130</b> includes a relatively greater error, i.e., outside the tolerance range, a driving <figure-callout id="135" label="guidance lane" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00002.png" state="{{state}}">guidance lane</figure-callout> <b>135</b> that is a virtual object to be displayed by the device is not visually appropriately aligned with a real road image, as shown in the <figure-callout id="140" label="image" filenames="US11176719-20211116-D00000.png,US11176719-20211116-D00002.png" state="{{state}}">image</figure-callout> <b>140</b>.</div>
    <div id="p-0070" num="0069" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 1C</figref>, localization information includes a position and an orientation of the device. The position corresponds to three-dimensional (3D) coordinates such as lateral (t<sub>x</sub>), vertical (t<sub>y</sub>), and longitudinal (t<sub>z</sub>), i.e., (x, y, z), as translational degrees of freedom (DOFs). Further, the orientation corresponds to pitch (r<sub>x</sub>), yaw (r<sub>y</sub>), and roll (r<sub>z</sub>) as rotational DOFs. The position is obtained through, for example, a global positioning system (GPS) sensor and a light detection and ranging (LiDAR), and the orientation is obtained through, for example, an inertial measurement unit (IMU) sensor and a gyro sensor. The localization information is construed as having 6 DOFs including the position and the orientation.</div>
    <div id="p-0071" num="0070" class="description-paragraph">The vehicle described herein refers to any mode of transportation, delivery, or communication such as, for example, an automobile, a truck, a tractor, a scooter, a motorcycle, a cycle, an amphibious vehicle, a snowmobile, a boat, a public transit vehicle, a bus, a monorail, a train, a tram, an autonomous or automated driving vehicle, an intelligent vehicle, a self-driving vehicle, an unmanned aerial vehicle, an electric vehicle (EV), a hybrid vehicle, a smart mobility device, an intelligent vehicle with an advanced driver assistance system (ADAS), or a drone. In an example, the smart mobility device includes mobility devices such as, for example, electric wheels, electric kickboard, and electric bike. In an example, vehicles include motorized and non-motorized vehicles, for example, a vehicle with a power engine (for example, a cultivator or a motorcycle), a bicycle or a handcart.</div>
    <div id="p-0072" num="0071" class="description-paragraph">In addition to the vehicle described herein, methods and apparatuses described herein may be included in various other devices, such as, for example, a smart phone, a walking assistance device, a wearable device, a security device, a robot, a mobile terminal, and various Internet of Things (IoT) devices.</div>
    <div id="p-0073" num="0072" class="description-paragraph">The term âroadâ is a thoroughfare, route, or connection, between two places that has been improved to allow travel by foot or some form of conveyance, such as a vehicle. A road can include various types of roads refers to a way on which vehicles drive, and includes various types of roads such as, for example, a highway, a national road, a local road, an expressway, farm roads, local roads, high-speed national roads, and a motorway. The road includes one or more lanes.</div>
    <div id="p-0074" num="0073" class="description-paragraph">The term âlaneâ refers to a road space distinguished by lines marked on a surface of the road. The lane is distinguished by left and right lines or lane boundary lines thereof. Further, the lines are various types of lines, for example, solid lines, broken lines, curved lines, and zigzag lines marked in colors such as white, blue, and yellow on the surface of the road. A line corresponds to one line separating a single lane, or corresponds to a pair of lines separating a single lane, that is, left and right lines corresponding to lane boundary lines. The term âlane boundaryâ may be interchangeably used with the term âlane markingâ</div>
    <div id="p-0075" num="0074" class="description-paragraph">The methods and apparatuses described herein are used to road guidance information in a navigation device of a vehicle, such as, for example, an augmented reality head-up display (<figure-callout id="3D" label="AR" filenames="US11176719-20211116-D00005.png,US11176719-20211116-D00008.png" state="{{state}}">AR</figure-callout> 3D HUD), and an autonomous vehicle. The examples set forth hereinafter may be utilized to display lines in an AR navigation system of a smart vehicle, generate visual information to assist steering of an autonomous vehicle, or provide a variety of control information related to driving of a vehicle. Further, the examples are used to assist safe and pleasant driving by providing visual information to a device including an intelligent system such as an HUD installed on a vehicle for driving assistance or fully autonomous driving. In an example, the examples described herein may also be used to interpret visual information for an intelligent system installed for fully autonomous driving or driving assistance in a vehicle, and used to assist safe and comfortable driving. The examples described herein may be applicable to vehicles and vehicle management systems such as, for example, an autonomous vehicle, an automatic or autonomous driving system, an intelligent vehicle, an advanced driver assistance system (ADAS), a navigation system to assist a vehicle with safely maintaining a lane on which the vehicle is travelling, a smartphone, or a mobile device. The examples related to displaying a road guidance information for vehicles is provided as an example only, and other examples such as, for example, training, gaming, applications in healthcare, public safety, tourism, and marketing are considered to be well within the scope of the present disclosure.</div>
    <div id="p-0076" num="0075" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates an example of calculating a localization score.</div>
    <div id="p-0077" num="0076" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 2</figref>, a localization apparatus calculates a score s(Î¸) of localization parameters Î¸ based on map data Q and an image I. The localization apparatus may be implemented by one or more hardware modules.</div>
    <div id="p-0078" num="0077" class="description-paragraph">In an example, the map data is a point cloud including a plurality of 3D vertices corresponding to object(s), such as lines. The 3D vertices of the map data are projected onto two-dimensional (2D) vertices based on localization parameters. The features of the image include feature values extracted on a basis of pixels included in the image. Thus, for the examples described herein, a correspondence between vertices of the map data and features of the image may not be needed.</div>
    <div id="p-0079" num="0078" class="description-paragraph">Information related to a correspondence or matching between the vertices, for example, 2D vertices of the map data and the features or pixels of the image, may not be needed for the examples described herein. Further, because the features extracted from the image may not be parameterized, a separate analysis on a relation between the features or a search of the map data may not be needed.</div>
    <div id="p-0080" num="0079" class="description-paragraph">The localization parameters Î¸ are position/orientation information parameters, and are defined as 6-DOF variables described in <figref idrefs="DRAWINGS">FIG. 1C</figref>. The localization parameters Î¸ correspond to approximate position/orientation information. The localization apparatus improves a localization accuracy by correcting the localization parameters Î¸ using scoring technology based on the image and the map data.</div>
    <div id="p-0081" num="0080" class="description-paragraph">In an example, the localization apparatus configures a feature map by extracting features from the image I. The localization apparatus calculates a matching score with respect to the localization parameters Î¸. In detail, the localization apparatus calculates the matching score by projecting vertices from the map data Q based on the localization parameters and pooling feature values of pixels corresponding to 2D coordinates of the projected vertices, among pixels of the feature map. The localization apparatus updates the localization parameters Î¸ to increase the matching score.</div>
    <div id="p-0082" num="0081" class="description-paragraph">In an example, the device is any device that performs a localization method, and includes devices, such as, for example, a vehicle, a navigation system, or a user device such as a smart phone. Localization information has 6 DOFs including the position and the orientation of the device, as described above. The localization information is obtained based on outputs of sensors such as, for example, an IMU sensor, a GPS sensor, a lidar sensor, and a radio detection and ranging (radar).</div>
    <div id="p-0083" num="0082" class="description-paragraph">The input image is a background image or other images to be displayed along with a virtual object to provide an AR service. The input image includes, for example, a driving image of the vehicle. In an example, the driving image is a driving image acquired using a capturing device mounted on the vehicle, and includes one or more frames.</div>
    <div id="p-0084" num="0083" class="description-paragraph">The localization apparatus acquires the input image based on an output of the capturing device. The capturing device is fixed to a location on the vehicle such as, for example, a windshield, a dashboard, or a rear-view mirror of the vehicle, to capture driving images of a view in front of the vehicle. The capturing device includes, for example, a vision sensor, an image sensor, or a device that performs a similar function. Depending on examples, the capturing device captures a single image, or captures images for each frame. In an example, images that are captured by a device other than the capturing device that is fixed to the vehicle are also used as the driving images. An object includes, for example, a line, a road surface marking, a traffic light, a traffic sign, a curb, a pedestrian, and a structure. The line includes lines such as, for example, a lane boundary line, a road center line, and a stop line. The road surface marking includes markings such as, for example, a no parking marking, a crosswalk marking, a towaway zone marking, and a speed limit marking.</div>
    <div id="p-0085" num="0084" class="description-paragraph">In an example, the map data is high density (HD) map data. An HD map is a 3D map with a high density, for example, a centimeter-level density, that may be used for autonomous driving. The HD map includes, for example, line information related to a road center line and a boundary line, and information related to a traffic light, a traffic sign, a curb, a road surface marking, and various structures in a form of 3D digital data. The HD map is established by, for example, a mobile mapping system (MMS). The MMS, a 3D space information investigation system equipped with various sensors, obtains minute position information using a moving object equipped with sensors such as a camera, a lidar, and a GPS to measure a position and geographic features.</div>
    <div id="p-0086" num="0085" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates an example of calculating a localization score.</div>
    <div id="p-0087" num="0086" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 3</figref>, a <figure-callout id="200" label="localization apparatus" filenames="US11176719-20211116-D00005.png" state="{{state}}">localization apparatus</figure-callout> <b>200</b> includes transform <figure-callout id="210" label="devices" filenames="US11176719-20211116-D00005.png" state="{{state}}"> <figure-callout id="220" label="devices" filenames="US11176719-20211116-D00005.png" state="{{state}}">devices</figure-callout> </figure-callout> <b>210</b> and <b>220</b>, a <figure-callout id="230" label="feature extractor" filenames="US11176719-20211116-D00005.png" state="{{state}}">feature extractor</figure-callout> <b>230</b>, and a <figure-callout id="240" label="pooler" filenames="US11176719-20211116-D00005.png" state="{{state}}">pooler</figure-callout> <b>240</b>. In an example, the <figure-callout id="210" label="transform device" filenames="US11176719-20211116-D00005.png" state="{{state}}">transform device</figure-callout> <b>210</b> receives parameters Î¸ and map data Q, and applies a 3D position and a 3D orientation of a device corresponding to the parameters Î¸ to the map data Q through a 3D transform T. For example, the <figure-callout id="210" label="transform device" filenames="US11176719-20211116-D00005.png" state="{{state}}">transform device</figure-callout> <b>210</b> extracts a region corresponding to a range of view at the position and the orientation corresponding to the parameters Î¸ from the map data. In an example, the <figure-callout id="220" label="transform device" filenames="US11176719-20211116-D00005.png" state="{{state}}">transform device</figure-callout> <b>220</b> generates a projection image at a viewpoint of the device through a perspective transform P. For example, the <figure-callout id="220" label="transform device" filenames="US11176719-20211116-D00005.png" state="{{state}}">transform device</figure-callout> <b>220</b> <figure-callout id="3D" label="projects" filenames="US11176719-20211116-D00005.png,US11176719-20211116-D00008.png" state="{{state}}">projects</figure-callout> 3D vertices included in the region extracted by the <figure-callout id="210" label="transform device" filenames="US11176719-20211116-D00005.png" state="{{state}}">transform device</figure-callout> <b>210</b> onto a 2D projection plane corresponding to the parameters Î¸. In this example, 3D vertices q<sub>i</sub> <sup>k </sup>included in the map data Q are transformed to 2D vertices p<sub>i</sub> <sup>k </sup>in the projection image through the transform T and the transform P based on the parameters Î¸. Here, k denotes an index indicating a different feature or class, and i denotes an index indicating a vertex in the corresponding feature or class.</div>
    <div id="p-0088" num="0087" class="description-paragraph">The <figure-callout id="230" label="feature extractor" filenames="US11176719-20211116-D00005.png" state="{{state}}">feature extractor</figure-callout> 230 extracts a feature from the image I. The feature includes one or more feature maps F<sub>1 </sub>and <figure-callout id="235" label="F" filenames="US11176719-20211116-D00005.png" state="{{state}}">F<sub> </sub> </figure-callout> <sub>2 </sub> <b>235</b> depending on a type or class of an object. For example, the feature map F<sub>1 </sub>includes features related to lines in the image, and the feature map F<sub>2 </sub>includes features related to traffic signs in the image. For ease of description, an example in which two feature maps F<sub>1 </sub>and <figure-callout id="235" label="F" filenames="US11176719-20211116-D00005.png" state="{{state}}">F<sub> </sub> </figure-callout> <sub>2 </sub> <b>235</b> are extracted is described. However, examples are not limited thereto.</div>
    <div id="p-0089" num="0088" class="description-paragraph">In an example, The localization apparatus includes separate feature extractors to extract a plurality of feature maps. In another example, the localization apparatus includes a single feature extractor, for example, a deep neural network (DNN), to output a plurality of feature maps for each channel.</div>
    <div id="p-0090" num="0089" class="description-paragraph">The extracted feature maps F<sub>1 </sub>and <figure-callout id="235" label="F" filenames="US11176719-20211116-D00005.png" state="{{state}}">F<sub> </sub> </figure-callout> <sub>2 </sub> <b>235</b> may include errors in some examples, and thus may not accurately specify values of corresponding features on a pixel basis. In this example, each feature map has a value between â0â and â1â for each pixel. A feature value of a pixel indicates an intensity of the pixel with respect to the feature.</div>
    <div id="p-0091" num="0090" class="description-paragraph">The 2D vertices p<sub>i</sub> <sup>k </sup>of the projection image refer to pixels corresponding to the 3D vertices q<sub>i</sub> <sup>k </sup>of the map data mapped to the image I. Referring to <figure-callout id="1" label="Equation" filenames="US11176719-20211116-D00007.png" state="{{state}}">Equation</figure-callout> 1, scores of features of the pixels mapped to the image I are summed up.</div>
    <div id="p-0092" num="0091" class="description-paragraph">
      <maths id="MATH-US-00001" num="00001">
        <math overflow="scroll">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mrow>
                    <msub>
                      <mi>s</mi>
                      <mi>k</mi>
                    </msub>
                    <mo>â¡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>Î¸</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>=</mo>
                  <mrow>
                    <munder>
                      <mo>â</mo>
                      <mi>i</mi>
                    </munder>
                    <mo>â¢</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"> </mspace>
                    </mstyle>
                    <mo>â¢</mo>
                    <mrow>
                      <msub>
                        <mi>F</mi>
                        <mi>k</mi>
                      </msub>
                      <mo>â¡</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>P</mi>
                          <mo>â¡</mo>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <mi>T</mi>
                              <mo>â¡</mo>
                              <mrow>
                                <mo>(</mo>
                                <mrow>
                                  <msubsup>
                                    <mi>q</mi>
                                    <mi>i</mi>
                                    <mi>k</mi>
                                  </msubsup>
                                  <mo>,</mo>
                                  <mi>Î¸</mi>
                                </mrow>
                                <mo>)</mo>
                              </mrow>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>Equation</mi>
                    <mo>â¢</mo>
                    <mstyle>
                      <mspace width="0.8em" height="0.8ex"> </mspace>
                    </mstyle>
                    <mo>â¢</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </maths>
    </div>
    <div id="p-0093" num="0092" class="description-paragraph">In <figure-callout id="1" label="Equation" filenames="US11176719-20211116-D00007.png" state="{{state}}">Equation</figure-callout> 1, T( )denotes the transform T, and P( ) denotes the transform P. In this example, P(T(q<sub>i</sub> <sup>k</sup>, Î¸)) denotes a mapping point, and F<sub>k</sub>( )denotes a feature value or score by a mapping point in a feature map corresponding to a k-th feature or class. In this example, if the mapping point P(T(q<sub>i</sub> <sup>k</sup>, Î¸)) is not an integer, an operation such as rounding off or interpolation is performed. Referring to Equation 2, a final score is calculated by calculating a weighted sum of the scores of the features.</div>
    <div id="p-0094" num="0093" class="description-paragraph">
      <maths id="MATH-US-00002" num="00002">
        <math overflow="scroll">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mrow>
                    <mi>s</mi>
                    <mo>â¡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>Î¸</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>=</mo>
                  <mrow>
                    <munder>
                      <mo>â</mo>
                      <mi>k</mi>
                    </munder>
                    <mo>â¢</mo>
                    <mrow>
                      <msub>
                        <mi>w</mi>
                        <mi>k</mi>
                      </msub>
                      <mo>â¢</mo>
                      <mrow>
                        <msub>
                          <mi>s</mi>
                          <mi>k</mi>
                        </msub>
                        <mo>â¡</mo>
                        <mrow>
                          <mo>(</mo>
                          <mi>Î¸</mi>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>Equation</mi>
                    <mo>â¢</mo>
                    <mstyle>
                      <mspace width="0.8em" height="0.8ex"> </mspace>
                    </mstyle>
                    <mo>â¢</mo>
                    <mn>2</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </maths>
    </div>
    <div id="p-0095" num="0094" class="description-paragraph">In this example, a weight w<sub>k </sub>is set using an arbitrary scheme. For example, the weight w<sub>k </sub>is set to be a weight assigned equally in the lump or a value tuned by training data.</div>
    <div id="p-0096" num="0095" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates an example of determining localization information of a device by utilizing a localization score. The operations in <figref idrefs="DRAWINGS">FIG. 4</figref> may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in <figref idrefs="DRAWINGS">FIG. 4</figref> may be performed in parallel or concurrently. One or more blocks of <figref idrefs="DRAWINGS">FIG. 4</figref>, and combinations of the blocks, can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions. In addition to the description of <figref idrefs="DRAWINGS">FIG. 4</figref> below, the descriptions of <figref idrefs="DRAWINGS">FIG. 1-3</figref> are also applicable to <figref idrefs="DRAWINGS">FIG. 4</figref>, and are incorporated herein by reference. Thus, the above description may not be repeated here.</div>
    <div id="p-0097" num="0096" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 4</figref>, in <figure-callout id="410" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>410</b>, an input image is received. In <figure-callout id="430" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>430</b>, features are extracted. In <figure-callout id="420" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>420</b>, map data is obtained. In <figure-callout id="440" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>440</b>, candidate localization information is obtained. In an example, the candidate localization information includes a plurality of pieces of candidate localization information.</div>
    <div id="p-0098" num="0097" class="description-paragraph">In <figure-callout id="450" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>450</b>, a projection image of the map data with respect to the candidate localization information is generated. In an example, when a plurality of pieces of candidate localization information is provided, a plurality of projection images with respect to the plurality of pieces of candidate localization information is generated.</div>
    <div id="p-0099" num="0098" class="description-paragraph">In <figure-callout id="460" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>460</b>, feature values corresponding to 2D vertices in the projection image are pooled from the feature map. Further, in <figure-callout id="460" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>460</b>, a score of the candidate localization information is calculated based on the pooled feature values. When a plurality of pieces of candidate localization information is provided, scores of the pieces of candidate localization information are calculated.</div>
    <div id="p-0100" num="0099" class="description-paragraph">In <figure-callout id="470" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>470</b>, a best score, for example, a highest score, is determined. In <figure-callout id="480" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>480</b>, candidate localization information having the determined best score is determined to be localization information of a device.</div>
    <div id="p-0101" num="0100" class="description-paragraph">Although not shown in the drawings, the <figure-callout id="200" label="localization apparatus" filenames="US11176719-20211116-D00005.png" state="{{state}}">localization apparatus</figure-callout> <b>200</b> determines a virtual object on the map data Q to provide an AR service. For example, the virtual object indicates driving route information, and is represented in a form of an arrow or a road marking indicating a direction to travel. The localization apparatus displays the virtual object along with the input image on a display of a user device, a navigation system, or a HUD, based on the localization information determined in <figure-callout id="480" label="operation" filenames="US11176719-20211116-D00006.png" state="{{state}}">operation</figure-callout> <b>480</b>.</div>
    <div id="p-0102" num="0101" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates an example of scores of pieces of candidate localization information.</div>
    <div id="p-0103" num="0102" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 5</figref>, a degree of visual alignment between an <figure-callout id="510" label="image" filenames="US11176719-20211116-D00007.png" state="{{state}}">image</figure-callout> <b>510</b> and a projection image with respect to first candidate localization information is lower than a degree of visual alignment between an <figure-callout id="520" label="image" filenames="US11176719-20211116-D00007.png" state="{{state}}">image</figure-callout> <b>520</b> and a projection image with respect to second candidate localization information. Accordingly, a score pooled from a feature map of the <figure-callout id="510" label="image" filenames="US11176719-20211116-D00007.png" state="{{state}}">image</figure-callout> <b>510</b> based on the first candidate localization information is calculated to be lower than a score pooled from a feature map of the <figure-callout id="520" label="image" filenames="US11176719-20211116-D00007.png" state="{{state}}">image</figure-callout> <b>520</b> based on the second candidate localization information.</div>
    <div id="p-0104" num="0103" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates an example of a localization method. The operations in <figref idrefs="DRAWINGS">FIG. 6</figref> may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in <figref idrefs="DRAWINGS">FIG. 6</figref> may be performed in parallel or concurrently. One or more blocks of <figref idrefs="DRAWINGS">FIG. 6</figref>, and combinations of the blocks, can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions. In addition to the description of <figref idrefs="DRAWINGS">FIG. 6</figref> below, the descriptions of <figref idrefs="DRAWINGS">FIG. 1-5</figref> are also applicable to <figref idrefs="DRAWINGS">FIG. 6</figref>, and are incorporated herein by reference. Thus, the above description may not be repeated here.</div>
    <div id="p-0105" num="0104" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 6</figref>, in <figure-callout id="610" label="operation" filenames="US11176719-20211116-D00008.png" state="{{state}}">operation</figure-callout> <b>610</b>, at least one feature map is extracted from an input image. In <figure-callout id="620" label="operation" filenames="US11176719-20211116-D00008.png" state="{{state}}">operation</figure-callout> <b>620</b>, a candidate group of localization parameters, for example, position/orientation parameters, is selected. In <figure-callout id="630" label="operation" filenames="US11176719-20211116-D00008.png" state="{{state}}">operation</figure-callout> <b>630</b>, a determination is made whether additional candidate localization information is to be evaluated. In <figure-callout id="640" label="operation" filenames="US11176719-20211116-D00008.png" state="{{state}}">operation</figure-callout> <b>640</b>, when it is determined that candidate localization information to be evaluated exists, a projection image corresponding to the candidate localization information is generated. In <figure-callout id="650" label="operation" filenames="US11176719-20211116-D00008.png" state="{{state}}">operation</figure-callout> <b>650</b>, scores of features are calculated. In <figure-callout id="660" label="operation" filenames="US11176719-20211116-D00008.png" state="{{state}}">operation</figure-callout> <b>660</b>, a final score is calculated through a weighted sum of the scores of the features. In <figure-callout id="670" label="operation" filenames="US11176719-20211116-D00008.png" state="{{state}}">operation</figure-callout> <b>670</b>, a determination is made whether the best candidate localization information is to be updated. In an example, the determination of the best candidate localization information is made by comparing the previous best candidate localization information, from among pieces of evaluated candidate localization information, to the final score.</div>
    <div id="p-0106" num="0105" class="description-paragraph">When no further candidate localization information to be evaluated exists, in <figure-callout id="680" label="operation" filenames="US11176719-20211116-D00008.png" state="{{state}}">operation</figure-callout> <b>680</b>, the best candidate localization information, from among the pieces of evaluated candidate localization information, is determined to be localization information of a device. In this example, parameters of the best candidate localization information are determined to be position/orientation parameters of the device.</div>
    <div id="p-0107" num="0106" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates an example of determining localization information of a device through an optimization technique. The operations in <figref idrefs="DRAWINGS">FIG. 7</figref> may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in <figref idrefs="DRAWINGS">FIG. 7</figref> may be performed in parallel or concurrently. One or more blocks of <figref idrefs="DRAWINGS">FIG. 7</figref>, and combinations of the blocks, can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions. In addition to the description of <figref idrefs="DRAWINGS">FIG. 7</figref> below, the descriptions of <figref idrefs="DRAWINGS">FIG. 1-6</figref> are also applicable to <figref idrefs="DRAWINGS">FIG. 7</figref>, and are incorporated herein by reference. Thus, the above description may not be repeated here.</div>
    <div id="p-0108" num="0107" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 7</figref>, in <figure-callout id="710" label="operation" filenames="US11176719-20211116-D00009.png" state="{{state}}">operation</figure-callout> <b>710</b>, an input image is received. In <figure-callout id="730" label="operation" filenames="US11176719-20211116-D00009.png" state="{{state}}">operation</figure-callout> <b>730</b>, a feature map is extracted. In <figure-callout id="720" label="operation" filenames="US11176719-20211116-D00009.png" state="{{state}}">operation</figure-callout> <b>720</b>, map data is received. In <figure-callout id="740" label="operation" filenames="US11176719-20211116-D00009.png" state="{{state}}">operation</figure-callout> <b>740</b>, initial localization information is received. In <figure-callout id="750" label="operation" filenames="US11176719-20211116-D00009.png" state="{{state}}">operation</figure-callout> <b>750</b>, a projection image with respect to the initial localization information is generated.</div>
    <div id="p-0109" num="0108" class="description-paragraph">In <figure-callout id="760" label="operation" filenames="US11176719-20211116-D00009.png" state="{{state}}">operation</figure-callout> <b>760</b>, the initial localization information is updated through an optimization technique. In <figure-callout id="770" label="operation" filenames="US11176719-20211116-D00009.png" state="{{state}}">operation</figure-callout> <b>770</b>, localization information of a device is determined to be the optimized localization information.</div>
    <div id="p-0110" num="0109" class="description-paragraph">Hereinafter, the optimization technique of <figure-callout id="760" label="operation" filenames="US11176719-20211116-D00009.png" state="{{state}}">operation</figure-callout> <b>760</b> will be described in detail.</div>
    <div id="p-0111" num="0110" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. 8A and 8B</figref> illustrate an example of an optimization technique. The operations in <figref idrefs="DRAWINGS">FIG. 8A</figref> may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in <figref idrefs="DRAWINGS">FIG. 8A</figref> may be performed in parallel or concurrently. One or more blocks of <figref idrefs="DRAWINGS">FIG. 8A</figref>, and combinations of the blocks, can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions. In addition to the description of <figref idrefs="DRAWINGS">FIG. 8A</figref> below, the descriptions of <figref idrefs="DRAWINGS">FIG. 1-7</figref> are also applicable to <figref idrefs="DRAWINGS">FIG. 8A</figref>, and are incorporated herein by reference. Thus, the above description may not be repeated here.</div>
    <div id="p-0112" num="0111" class="description-paragraph">A localization apparatus supports a global optimization process. The localization apparatus classifies 2D vertices projected from map data by a criterion other than features, for example, a distance, or whether a region is vanishing point-oriented, and uses the classified 2D vertices to estimate different DOFs of localization parameters.</div>
    <div id="p-0113" num="0112" class="description-paragraph">In an example, the localization apparatus segments a projection image into a plurality of regions, and determines localization information of a device through matching between a feature map and the regions. In detail, the localization apparatus sequentially determines a plurality of DOF values included in the localization information by sequentially using scores calculated through the matching in the regions. For example, the localization apparatus calculates pools, from the feature map, feature values corresponding to the 2D vertices included in the regions while changing DOF values determined for the regions. The localization apparatus calculates scores corresponding to the changed DOF values based on the pooled feature values. In an example, the localization apparatus determines a DOF to be a value corresponding to a highest score.</div>
    <div id="p-0114" num="0113" class="description-paragraph">The distant vertices in the projection image have a characteristic of being practically invariant to a change in position parameter. Based on such a characteristic, the localization apparatus separately performs a process of determining an orientation parameter by calculating a score using long-distance vertices and a process of determining a position parameter by calculating a score using short-distance vertices. This reduces DOFs to be estimated for each process, and thus a search complexity or a local convergence possibility during optimization decreases.</div>
    <div id="p-0115" num="0114" class="description-paragraph">In an example, the localization apparatus segments the projection image into a long-distance region and a short-distance region based on a first criterion associated with a distance, and segments the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point, which will be described further below. Here, the long-distance region includes 2D vertices that are affected below a threshold by translational DOFs. The vanishing point-oriented short-distance region includes 2D vertices whose influence due to the movement-related DOF in the forward and backward direction, or forward and backward translational DOF, is less than a threshold.</div>
    <div id="p-0116" num="0115" class="description-paragraph">In an example, the localization apparatus uses a portion of the DOFs of the localization parameters as values determined by advance calibration. Further, in an example, r<sub>z </sub>and t<sub>y </sub>of the localization parameters are determined by advance calibration because the height t<sub>y </sub>and the roll r<sub>z </sub>at which a camera is installed on a vehicle are fixed.</div>
    <div id="p-0117" num="0116" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIGS. 8A and 8B</figref>, in <figure-callout id="810" label="operation" filenames="US11176719-20211116-D00010.png" state="{{state}}">operation</figure-callout> <b>810</b>, a feature map is extracted from an input image. In <figure-callout id="820" label="operation" filenames="US11176719-20211116-D00010.png" state="{{state}}">operation</figure-callout> <b>820</b>, long-distance vertices Q<b>1</b> are selected from a projection image of map data. The long-distance vertices Q<b>1</b> are not substantially affected by translational DOFs t<sub>x </sub>and t<sub>z </sub>from among the DOFs of the localization parameters. Thus, in <figure-callout id="830" label="operation" filenames="US11176719-20211116-D00010.png" state="{{state}}">operation</figure-callout> <b>830</b>, rotational DOFs r<sub>x </sub>and r<sub>y </sub>are determined based on the long-distance vertices Q<b>1</b>. The rotational DOFs r<sub>x </sub>and r<sub>y </sub>are referred to as orientation parameters.</div>
    <div id="p-0118" num="0117" class="description-paragraph">In an example, the localization apparatus performs a parallel translation of the long-distance vertices Q<b>1</b> in a longitudinal direction while changing r<sub>x</sub>, and performs a parallel translation of the long-distance vertices Q<b>1</b> in a transverse direction while changing r<sub>y</sub>. The localization apparatus searches for values of r<sub>x </sub>and r<sub>y </sub>which make a score calculated for the long-distance vertices Q<b>1</b> to be greater than or equal to a target value.</div>
    <div id="p-0119" num="0118" class="description-paragraph">In <figure-callout id="840" label="operation" filenames="US11176719-20211116-D00010.png" state="{{state}}">operation</figure-callout> <b>840</b>, short-distance vertices are selected from the map data. The short-distance vertices are selected based on r<sub>z </sub>and t<sub>y </sub>being determined by the advance calibration and r<sub>x </sub>and r<sub>y </sub>being determined by the long-distance vertices Q<b>1</b>.</div>
    <div id="p-0120" num="0119" class="description-paragraph">The localization apparatus selects vertices Q<b>2</b> corresponding to lines towards the vanishing points from among the short-distance vertices, and selects the other vertices Q<b>3</b>. The vertices Q<b>2</b> are not substantially affected by the forward and backward (movement-related) translational DOF t<sub>z </sub>from among the DOFs of the localization parameters. Thus, in <figure-callout id="850" label="operation" filenames="US11176719-20211116-D00010.png" state="{{state}}">operation</figure-callout> <b>850</b>, the translational DOF t<sub>x </sub>is determined based on the vertices Q<b>2</b>. Further, the translational DOF t<sub>z </sub>is determined based on the vertices Q<b>3</b>. The translational DOFs t<sub>x </sub>and t<sub>z </sub>are referred to as position parameters.</div>
    <div id="p-0121" num="0120" class="description-paragraph"> <figref idrefs="DRAWINGS">FIGS. 9A through 9E</figref> illustrate an example of a result of applying an optimization technique.</div>
    <div id="p-0122" num="0121" class="description-paragraph"> <figure-callout id="911" label="Features" filenames="US11176719-20211116-D00012.png" state="{{state}}">Features</figure-callout> <b>911</b> extracted from a <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b> and <figure-callout id="921" label="vertices" filenames="US11176719-20211116-D00012.png" state="{{state}}">vertices</figure-callout> <b>921</b> projected from <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b> are illustrated in <figref idrefs="DRAWINGS">FIG. 9A</figref>. Initial localization information on the <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b> is inaccurate, and thus, the <figure-callout id="911" label="features" filenames="US11176719-20211116-D00012.png" state="{{state}}">features</figure-callout> <b>911</b> and the <figure-callout id="921" label="vertices" filenames="US11176719-20211116-D00012.png" state="{{state}}">vertices</figure-callout> <b>921</b> do not match. Hereinafter, a process of sequentially determining DOFs of localization parameters to match the <figure-callout id="911" label="features" filenames="US11176719-20211116-D00012.png" state="{{state}}">features</figure-callout> <b>911</b> and the <figure-callout id="921" label="vertices" filenames="US11176719-20211116-D00012.png" state="{{state}}">vertices</figure-callout> <b>921</b> will be described.</div>
    <div id="p-0123" num="0122" class="description-paragraph">Considering that a camera installed on a vehicle has a relatively constant height and a relatively constant roll relative to a road surface, t<sub>y </sub>and r<sub>z </sub>are calibrated in advance.</div>
    <div id="p-0124" num="0123" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 9B</figref>, a localization apparatus removes a roll effect from the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b> and the <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b>. In <b>930</b>, the localization apparatus removes the roll effect from the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b> by rotating the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b> based on r<sub>z </sub>that is calibrated in advance. Further, the localization apparatus detects vertices in the vicinity of the initial localization information from the <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b>, approximates the detected vertices to a plane, and rotates the <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b> to remove a roll of the plane.</div>
    <div id="p-0125" num="0124" class="description-paragraph">Further, the localization apparatus corrects heights of the vertices of the <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b> using t<sub>y </sub>that is calibrated in advance.</div>
    <div id="p-0126" num="0125" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 9C</figref>, the localization apparatus deduces r<sub>x </sub>and r<sub>y </sub>corresponding to a parallel translation using long-<figure-callout id="940" label="distance vertices" filenames="US11176719-20211116-D00014.png" state="{{state}}">distance vertices</figure-callout> <b>940</b> in a projection image of map data based on initial localization information. In an example, the localization apparatus performs a parallel translation of the long-<figure-callout id="940" label="distance vertices" filenames="US11176719-20211116-D00014.png" state="{{state}}">distance vertices</figure-callout> <b>940</b> such that a correlation between the long-<figure-callout id="940" label="distance vertices" filenames="US11176719-20211116-D00014.png" state="{{state}}">distance vertices</figure-callout> <b>940</b> and the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b> is greater than or equal to a target value. For example, by rotating the vertices on the <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b> through an adjustment of r<sub>x </sub>as in <b>945</b>, the long-<figure-callout id="940" label="distance vertices" filenames="US11176719-20211116-D00014.png" state="{{state}}">distance vertices</figure-callout> <b>940</b> in the projection image may be matched well with the features of the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b>.</div>
    <div id="p-0127" num="0126" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 9D</figref>, the localization apparatus obtains a vanishing point of neighboring <figure-callout id="950" label="lines" filenames="US11176719-20211116-D00015.png" state="{{state}}">lines</figure-callout> <b>950</b> by analyzing the vertices in the projection image. The localization apparatus aligns the vanishing point of the neighboring <figure-callout id="950" label="lines" filenames="US11176719-20211116-D00015.png" state="{{state}}">lines</figure-callout> <b>950</b> at a position in the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b>, for example, a center of a feature map. In this example, the neighboring lines have a characteristic of being invariant to a z-directional translation. The localization apparatus moves vertices corresponding to the neighboring <figure-callout id="950" label="lines" filenames="US11176719-20211116-D00015.png" state="{{state}}">lines</figure-callout> <b>950</b> in the x direction such that a correlation between the neighboring <figure-callout id="950" label="lines" filenames="US11176719-20211116-D00015.png" state="{{state}}">lines</figure-callout> <b>950</b> and the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b> is greater than or equal to a target value. For example, by moving the vertices on the <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b> through an adjustment of t<sub>x </sub>as in <b>955</b>, vertices corresponding to the neighboring <figure-callout id="950" label="lines" filenames="US11176719-20211116-D00015.png" state="{{state}}">lines</figure-callout> <b>950</b> in the projection image may be matched well with the features of the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b>.</div>
    <div id="p-0128" num="0127" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 9E</figref>, the localization apparatus detects a z-directional translation using remaining <figure-callout id="960" label="vertices" filenames="US11176719-20211116-D00016.png" state="{{state}}">vertices</figure-callout> <b>960</b> excluding vanishing point-directional lanes from among short-distance vertices in the projection image. The localization apparatus moves the remaining <figure-callout id="960" label="vertices" filenames="US11176719-20211116-D00016.png" state="{{state}}">vertices</figure-callout> <b>960</b> in the z direction such that a correlation between the remaining <figure-callout id="960" label="vertices" filenames="US11176719-20211116-D00016.png" state="{{state}}">vertices</figure-callout> <b>960</b> and the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b> is greater than or equal to a target value. For example, by moving the vertices on the <figure-callout id="920" label="map data" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">map data</figure-callout> <b>920</b> through an adjustment of t<sub>z </sub>as in <b>965</b>, the remaining <figure-callout id="960" label="vertices" filenames="US11176719-20211116-D00016.png" state="{{state}}">vertices</figure-callout> <b>960</b> in the projection image may be matched well with the features of the <figure-callout id="910" label="feature map" filenames="US11176719-20211116-D00012.png,US11176719-20211116-D00013.png" state="{{state}}">feature map</figure-callout> <b>910</b>.</div>
    <div id="p-0129" num="0128" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 10</figref> illustrates an example of a localization method by parameter updating. The operations in <figref idrefs="DRAWINGS">FIG. 10</figref> may be performed in the sequence and manner as shown, although the order of some operations may be changed or some of the operations omitted without departing from the spirit and scope of the illustrative examples described. Many of the operations shown in <figref idrefs="DRAWINGS">FIG. 10</figref> may be performed in parallel or concurrently. One or more blocks of <figref idrefs="DRAWINGS">FIG. 10</figref>, and combinations of the blocks, can be implemented by special purpose hardware-based computer, such as a processor, that perform the specified functions, or combinations of special purpose hardware and computer instructions. In addition to the description of <figref idrefs="DRAWINGS">FIG. 10</figref> below, the descriptions of <figref idrefs="DRAWINGS">FIG. 1-9E</figref> are also applicable to <figref idrefs="DRAWINGS">FIG. 10</figref>, and are incorporated herein by reference. Thus, the above description may not be repeated here</div>
    <div id="p-0130" num="0129" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 10</figref>, in <figure-callout id="1010" label="operation" filenames="US11176719-20211116-D00017.png" state="{{state}}">operation</figure-callout> <b>1010</b>, a feature map is extracted from an input image. In <figure-callout id="1020" label="operation" filenames="US11176719-20211116-D00017.png" state="{{state}}">operation</figure-callout> <b>1020</b>, initial localization information, for example, initial values of position/orientation parameters, is selected. In <figure-callout id="1030" label="operation" filenames="US11176719-20211116-D00017.png" state="{{state}}">operation</figure-callout> <b>1030</b>, scores of current parameters are calculated, and a direction to improve the scores of the current parameters is calculated.</div>
    <div id="p-0131" num="0130" class="description-paragraph">In an example, the feature map includes a probability distribution indicating a degree of closeness to an object. For example, features included in the feature map include information related to a distance to a closest object, the information expressed using normalized values between â0â and â1â. In this example, the feature map provides information related to a direction toward the object. The localization apparatus pools feature values of the feature map corresponding to 2D vertices projected from the map data by the current parameters. The localization apparatus determines the direction to improve the scores of the current parameters based on the pooled feature values.</div>
    <div id="p-0132" num="0131" class="description-paragraph">In <figure-callout id="1040" label="operation" filenames="US11176719-20211116-D00017.png" state="{{state}}">operation</figure-callout> 1040, a determination is made whether an iteration termination condition is satisfied. When it is determined that the iteration termination condition is not satisfied, the parameters are updated, in <figure-callout id="1050" label="operation" filenames="US11176719-20211116-D00017.png" state="{{state}}">operation</figure-callout> <b>1050</b>. The localization apparatus updates the parameters based on the direction calculated in <figure-callout id="1030" label="operation" filenames="US11176719-20211116-D00017.png" state="{{state}}">operation</figure-callout> <b>1030</b>. <figure-callout id="1050" label="Operations" filenames="US11176719-20211116-D00017.png" state="{{state}}"> <figure-callout id="1030" label="Operations" filenames="US11176719-20211116-D00017.png" state="{{state}}"> <figure-callout id="1040" label="Operations" filenames="US11176719-20211116-D00017.png" state="{{state}}">Operations</figure-callout> </figure-callout> </figure-callout> <b>1050</b>, <b>1030</b>, and <b>1040</b> are iteratively performed until the iteration termination condition is satisfied. The iteration termination condition includes whether the scores of the parameters are greater than or equal to a target value. In an example, the iteration termination condition further includes whether an iteration count exceeds a threshold for system stability.</div>
    <div id="p-0133" num="0132" class="description-paragraph">When it is determined that the iteration termination condition is satisfied, the current parameters are selected as final localization information, for example, final position/orientation parameters, in <figure-callout id="1060" label="operation" filenames="US11176719-20211116-D00017.png" state="{{state}}">operation</figure-callout> <b>1060</b>.</div>
    <div id="p-0134" num="0133" class="description-paragraph">In the example of <figref idrefs="DRAWINGS">FIG. 10</figref>, a step by step search for determining better parameters from initial values Î¸_0 is performed. Such a local optimization scheme requires good initial values for better performance. Thus, the example of <figref idrefs="DRAWINGS">FIG. 10</figref> selectively includes an operation of separately estimating the initial values. For example, the example of <figref idrefs="DRAWINGS">FIG. 10</figref> is performed while the parameters obtained through the example of <figref idrefs="DRAWINGS">FIG. 8</figref> are regarded as the initial values. In this example, an effect of correcting values such as a camera height and a roll fixed through advance calibration to be suitable for a variation occurring in a real driving environment is achieved.</div>
    <div id="p-0135" num="0134" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 11</figref> illustrates an example of a step by step result of applying a localization method by parameter updating.</div>
    <div id="p-0136" num="0135" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 11</figref>, an <figure-callout id="1105" label="input image" filenames="US11176719-20211116-D00018.png" state="{{state}}">input image</figure-callout> <b>1105</b>, a <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b>, and a <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b> are illustrated. The <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b> is generated to correspond to the <figure-callout id="1105" label="input image" filenames="US11176719-20211116-D00018.png" state="{{state}}">input image</figure-callout> <b>1105</b>. Further, the <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b> is an image generated by projecting an object with respect to localization information <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/87/eb/7e/168f32dc8cd87c/US11176719-20211116-P00001.png"><img id="CUSTOM-CHARACTER-00001" he="3.22mm" wi="1.44mm" file="US11176719-20211116-P00001.TIF" alt="Figure US11176719-20211116-P00001" img-content="character" img-format="tif" orientation="portrait" inline="no" width="6" height="13" alt="Figure US11176719-20211116-P00001" class="patent-full-image" src="https://patentimages.storage.googleapis.com/87/eb/7e/168f32dc8cd87c/US11176719-20211116-P00001.png"/></a></div> (x, y, z, r<sub>x</sub>, r<sub>y</sub>, and r<sub>z</sub>) corresponding to initial localization information based on map data. The <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b> is a projection image including a plurality of 2D vertices corresponding to the object.</div>
    <div id="p-0137" num="0136" class="description-paragraph">A localization apparatus calculates a score by matching the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b> and the <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b> as shown in an <figure-callout id="1130" label="image" filenames="US11176719-20211116-D00018.png" state="{{state}}">image</figure-callout> <b>1130</b>. The localization apparatus calculates the score by summing up values of pixels corresponding to the object included in the <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b>, among a plurality of pixels included in the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b>.</div>
    <div id="p-0138" num="0137" class="description-paragraph">For example, the plurality of pixels included in the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b> has values between â0â and â1â based on distances to an adjacent object. Each pixel has a value close to â1â as being close to the adjacent object and has a value close to â0â as being far from the adjacent object. The localization apparatus extracts pixels matching the <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b> from the plurality of pixels included in the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b>, and calculates the score by summing up values of the extracted pixels.</div>
    <div id="p-0139" num="0138" class="description-paragraph">The localization apparatus corrects the localization information to increase a degree of visual alignment, i.e., the score , based on a directivity of the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b>. The localization apparatus calculates a localization correction value such that localization information of the object included in the <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b> accords with the directivity of the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b>. The localization apparatus applies the localization correction value to the localization information corresponding to the initial localization information, thereby updating the localization information from <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/87/eb/7e/168f32dc8cd87c/US11176719-20211116-P00001.png"><img id="CUSTOM-CHARACTER-00002" he="3.22mm" wi="1.44mm" file="US11176719-20211116-P00001.TIF" alt="Figure US11176719-20211116-P00001" img-content="character" img-format="tif" orientation="portrait" inline="no" width="6" height="13" alt="Figure US11176719-20211116-P00001" class="patent-full-image" src="https://patentimages.storage.googleapis.com/87/eb/7e/168f32dc8cd87c/US11176719-20211116-P00001.png"/></a></div>â<div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/8f/c1/23/a450a622b0ddc0/US11176719-20211116-P00002.png"><img id="CUSTOM-CHARACTER-00003" he="3.22mm" wi="1.78mm" file="US11176719-20211116-P00002.TIF" alt="Figure US11176719-20211116-P00002" img-content="character" img-format="tif" orientation="portrait" inline="no" width="7" height="13" alt="Figure US11176719-20211116-P00002" class="patent-full-image" src="https://patentimages.storage.googleapis.com/8f/c1/23/a450a622b0ddc0/US11176719-20211116-P00002.png"/></a></div>, in <figure-callout id="1140" label="operation" filenames="US11176719-20211116-D00018.png" state="{{state}}">operation</figure-callout> <b>1140</b>. For example, the localization apparatus determines a direction in which the object of the <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b> is to be moved to increase the score, based on the directivity of the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b>. The object of the <figure-callout id="1120" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1120</b> is moved when the localization information is updated, and thus the localization apparatus updates the localization information based on the directivity included in the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b>.</div>
    <div id="p-0140" num="0139" class="description-paragraph">The localization apparatus generates an updated <figure-callout id="1150" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1150</b> based on the updated localization information <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/8f/c1/23/a450a622b0ddc0/US11176719-20211116-P00002.png"><img id="CUSTOM-CHARACTER-00004" he="3.22mm" wi="1.78mm" file="US11176719-20211116-P00002.TIF" alt="Figure US11176719-20211116-P00002" img-content="character" img-format="tif" orientation="portrait" inline="no" width="7" height="13" alt="Figure US11176719-20211116-P00002" class="patent-full-image" src="https://patentimages.storage.googleapis.com/8f/c1/23/a450a622b0ddc0/US11176719-20211116-P00002.png"/></a></div>. The localization apparatus calculates a score by matching the updated <figure-callout id="1150" label="second image" filenames="US11176719-20211116-D00018.png" state="{{state}}">second image</figure-callout> <b>1150</b> and the <figure-callout id="1110" label="first image" filenames="US11176719-20211116-D00018.png" state="{{state}}">first image</figure-callout> <b>1110</b>.</div>
    <div id="p-0141" num="0140" class="description-paragraph">The localization apparatus outputs finally optimized localization information <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/42/43/f3/f01df7e6f7dec1/US11176719-20211116-P00003.png"><img id="CUSTOM-CHARACTER-00005" he="3.22mm" wi="2.12mm" file="US11176719-20211116-P00003.TIF" alt="Figure US11176719-20211116-P00003" img-content="character" img-format="tif" orientation="portrait" inline="no" width="8" height="13" alt="Figure US11176719-20211116-P00003" class="patent-full-image" src="https://patentimages.storage.googleapis.com/42/43/f3/f01df7e6f7dec1/US11176719-20211116-P00003.png"/></a></div> by calculating a localization correction value which makes the score to be greater than or equal to a criterion through the process described above.</div>
    <div id="p-0142" num="0141" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 12</figref> illustrates an example of a neural network to generate a feature map.</div>
    <div id="p-0143" num="0142" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 12</figref>, a process of generating a <figure-callout id="1250" label="distance field map" filenames="US11176719-20211116-D00019.png" state="{{state}}">distance field map</figure-callout> <b>1250</b> corresponding to a first image by applying an <figure-callout id="1210" label="input image" filenames="US11176719-20211116-D00019.png" state="{{state}}">input image</figure-callout> <b>1210</b> to a <figure-callout id="1230" label="neural network" filenames="US11176719-20211116-D00019.png" state="{{state}}">neural network</figure-callout> <b>1230</b> is illustrated.</div>
    <div id="p-0144" num="0143" class="description-paragraph">In an example, the <figure-callout id="1230" label="neural network" filenames="US11176719-20211116-D00019.png" state="{{state}}">neural network</figure-callout> <b>1230</b> is trained to generate a first image including a directivity corresponding to an object included in the <figure-callout id="1210" label="input image" filenames="US11176719-20211116-D00019.png" state="{{state}}">input image</figure-callout> <b>1210</b> based on the <figure-callout id="1210" label="input image" filenames="US11176719-20211116-D00019.png" state="{{state}}">input image</figure-callout> <b>1210</b>. The <figure-callout id="1230" label="neural network" filenames="US11176719-20211116-D00019.png" state="{{state}}">neural network</figure-callout> <b>1230</b> is implemented on a hardware-based model comprising a framework or a structure of a number of layers or operations to provide for many different machine learning algorithms to work together, process complex data inputs, and recognize patterns. The <figure-callout id="1230" label="neural network" filenames="US11176719-20211116-D00019.png" state="{{state}}">neural network</figure-callout> <b>1230</b> is implemented in various structures such as, for example, a convolutional neural network (CNN), a deep neural network (DNN), an n-layer neural network, a recurrent neural network (RNN), or a bidirectional long short term memory (BLSTM). The DNN includes, for example, a fully connected network, a CNN, a deep convolutional network, or a recurrent neural network (RNN), a deep belief network, a bi-directional neural network, a restricted Boltzman machine, or may include different or overlapping neural network portions respectively with full, convolutional, recurrent, and/or bi-directional connections. The <figure-callout id="1230" label="neural network" filenames="US11176719-20211116-D00019.png" state="{{state}}">neural network</figure-callout> <b>1230</b> maps, based on deep learning, input data and output data that are in a non-linear relationship, to perform, for example, an object classification, an object recognition, a speech recognition, or an image recognition.</div>
    <div id="p-0145" num="0144" class="description-paragraph">The neural network may be implemented as an architecture having a plurality of layers including an input image, feature maps, and an output. In the neural network, a convolution operation between the input image, and a filter referred to as a kernel, is performed, and as a result of the convolution operation, the feature maps are output. Here, the feature maps that are output are input feature maps, and a convolution operation between the output feature maps and the kernel is performed again, and as a result, new feature maps are output. Based on such repeatedly performed convolution operations, results of recognition of characteristics of the input image via the neural network may be output.</div>
    <div id="p-0146" num="0145" class="description-paragraph">In an example, the <figure-callout id="1230" label="neural network" filenames="US11176719-20211116-D00019.png" state="{{state}}">neural network</figure-callout> <b>1230</b> estimates the object included in the <figure-callout id="1210" label="input image" filenames="US11176719-20211116-D00019.png" state="{{state}}">input image</figure-callout> <b>1210</b> in a form of the <figure-callout id="1250" label="distance field map" filenames="US11176719-20211116-D00019.png" state="{{state}}">distance field map</figure-callout> <b>1250</b>. For example, when the first image includes directivity information toward a close object as in the <figure-callout id="1250" label="distance field map" filenames="US11176719-20211116-D00019.png" state="{{state}}">distance field map</figure-callout> <b>1250</b>, a directivity of optimization is determined by utilizing gradient descent. Further, when a probability distribution indicating a degree of closeness to the object is present all over the image as in the <figure-callout id="1250" label="distance field map" filenames="US11176719-20211116-D00019.png" state="{{state}}">distance field map</figure-callout> <b>1250</b>, an amount of data for training increases, and thus the performance of the neural network improves when compared to a case of training with sparse data.</div>
    <div id="p-0147" num="0146" class="description-paragraph"> <figref idrefs="DRAWINGS">FIG. 13</figref> illustrates an example of a localization apparatus.</div>
    <div id="p-0148" num="0147" class="description-paragraph">Referring to <figref idrefs="DRAWINGS">FIG. 13</figref>, a <figure-callout id="1300" label="localization apparatus" filenames="US11176719-20211116-D00020.png" state="{{state}}">localization apparatus</figure-callout> <b>1300</b> includes <figure-callout id="1310" label="sensors" filenames="US11176719-20211116-D00020.png" state="{{state}}">sensors</figure-callout> <b>1310</b> and a <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b>. The <figure-callout id="1300" label="localization apparatus" filenames="US11176719-20211116-D00020.png" state="{{state}}">localization apparatus</figure-callout> <b>1300</b> further includes the <figure-callout id="1350" label="memory" filenames="US11176719-20211116-D00020.png" state="{{state}}">memory</figure-callout> <b>1350</b>, the <figure-callout id="1370" label="communication interface" filenames="US11176719-20211116-D00020.png" state="{{state}}">communication interface</figure-callout> <b>1370</b>, and the <figure-callout id="1390" label="display device" filenames="US11176719-20211116-D00020.png" state="{{state}}">display device</figure-callout> <b>1390</b>. The <figure-callout id="1310" label="sensors" filenames="US11176719-20211116-D00020.png" state="{{state}}">sensors</figure-callout> <b>1310</b>, the <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b>, the <figure-callout id="1350" label="memory" filenames="US11176719-20211116-D00020.png" state="{{state}}">memory</figure-callout> <b>1350</b>, the <figure-callout id="1370" label="communication interface" filenames="US11176719-20211116-D00020.png" state="{{state}}">communication interface</figure-callout> <b>1370</b>, and the <figure-callout id="1390" label="display device" filenames="US11176719-20211116-D00020.png" state="{{state}}">display device</figure-callout> <b>1390</b> are connected to each other through a <figure-callout id="1305" label="communication bus" filenames="US11176719-20211116-D00020.png" state="{{state}}">communication bus</figure-callout> <b>1305</b>.</div>
    <div id="p-0149" num="0148" class="description-paragraph">The sensor(s) <b>1310</b> include, for example, an image sensor, a vision sensor, an acceleration sensor, a gyro sensor, a GPS sensor, an IMU sensor, a Radar, and a Lidar. The sensor(s) <b>1310</b> acquire or capture an input image including a driving image of a vehicle. The sensor(s) <b>1310</b> senses information such as, for example, a speed, an acceleration, a travelling direction, and a steering angle of the vehicle, in addition to localization information such as, for example, GPS coordinates, a position, and an orientation of the vehicle is sensed by the sensor(s) <b>1310</b>.</div>
    <div id="p-0150" num="0149" class="description-paragraph">In an example, the <figure-callout id="1300" label="localization apparatus" filenames="US11176719-20211116-D00020.png" state="{{state}}">localization apparatus</figure-callout> <b>1300</b> obtains sensing information of various sensors including the input image through the <figure-callout id="1370" label="communication interface" filenames="US11176719-20211116-D00020.png" state="{{state}}">communication interface</figure-callout> <b>1370</b>. The <figure-callout id="1370" label="communication interface" filenames="US11176719-20211116-D00020.png" state="{{state}}">communication interface</figure-callout> <b>1370</b> receives sensing information including a driving image from other sensors existing outside of the <figure-callout id="1300" label="localization apparatus" filenames="US11176719-20211116-D00020.png" state="{{state}}">localization apparatus</figure-callout> <b>1300</b>.</div>
    <div id="p-0151" num="0150" class="description-paragraph">The <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b> outputs the corrected localization information through the <figure-callout id="1370" label="communication interface" filenames="US11176719-20211116-D00020.png" state="{{state}}">communication interface</figure-callout> <b>1370</b> and/or the <figure-callout id="1390" label="display device" filenames="US11176719-20211116-D00020.png" state="{{state}}">display device</figure-callout> <b>1390</b>, or displays a virtual object along with the input image on map data based on the corrected localization information, thereby providing an AR service. Further, the <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b> performs the at least one method described above through <figref idrefs="DRAWINGS">FIGS. 1 through 13</figref> or an algorithm corresponding to the at least one method.</div>
    <div id="p-0152" num="0151" class="description-paragraph">The <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b> is a data processing device implemented by hardware including a circuit having a physical structure to perform desired operations. For example, the desired operations include instructions or codes included in a program. For example, the hardware-implemented data processing device includes a microprocessor, a central processing unit (CPU), a processor core, a multi-core processor, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA). In an example, the <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b> may be a graphics processor unit (GPU), reconfigurable processor, or have any other type of multi- or single-processor configuration. The <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b> executes the program and controls the <figure-callout id="1300" label="localization apparatus" filenames="US11176719-20211116-D00020.png" state="{{state}}">localization apparatus</figure-callout> <b>1300</b>. In an example, the <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b> executes the program and controls the <figure-callout id="1230" label="neural network" filenames="US11176719-20211116-D00019.png" state="{{state}}">neural network</figure-callout> <b>1230</b>. The program codes to be executed by the <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b> are stored in the <figure-callout id="1350" label="memory" filenames="US11176719-20211116-D00020.png" state="{{state}}">memory</figure-callout> <b>1350</b>. Further details regarding the <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b> is provided below.</div>
    <div id="p-0153" num="0152" class="description-paragraph">The <figure-callout id="1350" label="memory" filenames="US11176719-20211116-D00020.png" state="{{state}}">memory</figure-callout> <b>1350</b> stores the localization information of the <figure-callout id="1300" label="localization apparatus" filenames="US11176719-20211116-D00020.png" state="{{state}}">localization apparatus</figure-callout> <b>1300</b>, the first image, the second image, and/or the corrected localization information. The <figure-callout id="1350" label="memory" filenames="US11176719-20211116-D00020.png" state="{{state}}">memory</figure-callout> <b>1350</b> stores a variety of information generated during the processing process performed by the <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b>. In addition, the <figure-callout id="1350" label="memory" filenames="US11176719-20211116-D00020.png" state="{{state}}">memory</figure-callout> <b>1350</b> stores a variety of data and programs. The <figure-callout id="1350" label="memory" filenames="US11176719-20211116-D00020.png" state="{{state}}">memory</figure-callout> <b>1350</b> includes a volatile memory or a non-volatile memory. The <figure-callout id="1350" label="memory" filenames="US11176719-20211116-D00020.png" state="{{state}}">memory</figure-callout> <b>1350</b> includes a large capacity storage medium such as a hard disk to store the variety of data. Further details regarding the <figure-callout id="1120" label="memory" filenames="US11176719-20211116-D00018.png" state="{{state}}">memory</figure-callout> <b>1120</b> is provided below.</div>
    <div id="p-0154" num="0153" class="description-paragraph">The <figure-callout id="1390" label="display device" filenames="US11176719-20211116-D00020.png" state="{{state}}">display device</figure-callout> <b>1390</b> outputs the localization information corrected by the <figure-callout id="1330" label="processor" filenames="US11176719-20211116-D00020.png" state="{{state}}">processor</figure-callout> <b>1330</b>, or displays the virtual object along with the input image on the map data based on the corrected localization information. The <figure-callout id="1390" label="display device" filenames="US11176719-20211116-D00020.png" state="{{state}}">display device</figure-callout> <b>1390</b> is a physical structure that includes one or more hardware components that provide the ability to render a user interface, render a display, and/or receive user input. However, the <figure-callout id="1390" label="display device" filenames="US11176719-20211116-D00020.png" state="{{state}}">display device</figure-callout> <b>1390</b> is not limited to the example described above, and any other displays, such as, for example, smart phone and eye glass display (EGD) that are operatively connected to the <figure-callout id="1300" label="localization apparatus" filenames="US11176719-20211116-D00020.png" state="{{state}}">localization apparatus</figure-callout> <b>1300</b> may be used without departing from the spirit and scope of the illustrative examples described.</div>
    <div id="p-0155" num="0154" class="description-paragraph">According to examples, a localization apparatus performs localization independently of viewpoints by updating 3D localization information of the localization apparatus using a result of performing the localization method described above based on a capturing device even when viewpoints of the capturing device and the localization apparatus do not match like a HUD or AR glasses. Further, the localization apparatus updates the 3D localization information and is also used to directly correct a 2D position in an image when the viewpoints of the capturing device and the localization apparatus match like a mobile terminal or a smart phone.</div>
    <div id="p-0156" num="0155" class="description-paragraph">Examples set forth herein provide technology for localization without establishing correspondence between vertices of an image and vertices of map data. Further, the examples provide technology for localization without parameterizing features of an image, extracting a relation invariant to a three-dimensional (3D) transform and a perspective transform, or easily specifying such an invariant relation during a search of map data.</div>
    <div id="p-0157" num="0156" class="description-paragraph">The <figure-callout id="200" label="localization apparatuses" filenames="US11176719-20211116-D00005.png" state="{{state}}"> <figure-callout id="1300" label="localization apparatuses" filenames="US11176719-20211116-D00020.png" state="{{state}}">localization apparatuses</figure-callout> </figure-callout> <b>200</b> and <b>1300</b>, transform <figure-callout id="210" label="devices" filenames="US11176719-20211116-D00005.png" state="{{state}}"> <figure-callout id="220" label="devices" filenames="US11176719-20211116-D00005.png" state="{{state}}">devices</figure-callout> </figure-callout> <b>210</b> and <b>220</b>, <figure-callout id="230" label="feature extractor" filenames="US11176719-20211116-D00005.png" state="{{state}}">feature extractor</figure-callout> <b>230</b>, <figure-callout id="240" label="pooler" filenames="US11176719-20211116-D00005.png" state="{{state}}">pooler</figure-callout> <b>240</b>, and other apparatuses, units, modules, devices, and other components described herein with respect to <figref idrefs="DRAWINGS">FIGS. 1-13</figref> are implemented by hardware components. Examples of hardware components that may be used to perform the operations described in this application where appropriate include controllers, sensors, generators, drivers, memories, comparators, arithmetic logic units, adders, subtractors, multipliers, dividers, integrators, and any other electronic components configured to perform the operations described in this application. In other examples, one or more of the hardware components that perform the operations described in this application are implemented by computing hardware, for example, by one or more processors or computers. A processor or computer may be implemented by one or more processing elements, such as an array of logic gates, a controller and an arithmetic logic unit, a digital signal processor, a microcomputer, a programmable logic controller, a field-programmable gate array, a programmable logic array, a microprocessor, or any other device or combination of devices that is configured to respond to and execute instructions in a defined manner to achieve a desired result. In one example, a processor or computer includes, or is connected to, one or more memories storing instructions or software that are executed by the processor or computer. Hardware components implemented by a processor or computer may execute instructions or software, such as an operating system (OS) and one or more software applications that run on the OS, to perform the operations described in this application. The hardware components may also access, manipulate, process, create, and store data in response to execution of the instructions or software. For simplicity, the singular term âprocessorâ or âcomputerâ may be used in the description of the examples described in this application, but in other examples multiple processors or computers may be used, or a processor or computer may include multiple processing elements, or multiple types of processing elements, or both. For example, a single hardware component or two or more hardware components may be implemented by a single processor, or two or more processors, or a processor and a controller. One or more hardware components may be implemented by one or more processors, or a processor and a controller, and one or more other hardware components may be implemented by one or more other processors, or another processor and another controller. One or more processors, or a processor and a controller, may implement a single hardware component, or two or more hardware components. A hardware component may have any one or more of different processing configurations, examples of which include a single processor, independent processors, parallel processors, single-instruction single-data (SISD) multiprocessing, single-instruction multiple-data (SIMD) multiprocessing, multiple-instruction single-data (MISD) multiprocessing, and multiple-instruction multiple-data (MIMD) multiprocessing.</div>
    <div id="p-0158" num="0157" class="description-paragraph">The methods illustrated in <figref idrefs="DRAWINGS">FIGS. 1-13</figref> that perform the operations described in this application are performed by computing hardware, for example, by one or more processors or computers, implemented as described above executing instructions or software to perform the operations described in this application that are performed by the methods. For example, a single operation or two or more operations may be performed by a single processor, or two or more processors, or a processor and a controller. One or more operations may be performed by one or more processors, or a processor and a controller, and one or more other operations may be performed by one or more other processors, or another processor and another controller. One or more processors, or a processor and a controller, may perform a single operation, or two or more operations.</div>
    <div id="p-0159" num="0158" class="description-paragraph">Instructions or software to control a processor or computer to implement the hardware components and perform the methods as described above are written as computer programs, code segments, instructions or any combination thereof, for individually or collectively instructing or configuring the processor or computer to operate as a machine or special-purpose computer to perform the operations performed by the hardware components and the methods as described above. In an example, the instructions or software includes at least one of an applet, a dynamic link library (DLL), middleware, firmware, a device driver, an application program storing the method of outputting the state information. In one example, the instructions or software include machine code that is directly executed by the processor or computer, such as machine code produced by a compiler. In another example, the instructions or software include higher-level code that is executed by the processor or computer using an interpreter. Programmers of ordinary skill in the art can readily write the instructions or software based on the block diagrams and the flow charts illustrated in the drawings and the corresponding descriptions in the specification, which disclose algorithms for performing the operations performed by the hardware components and the methods as described above.</div>
    <div id="p-0160" num="0159" class="description-paragraph">The instructions or software to control computing hardware, for example, one or more processors or computers, to implement the hardware components and perform the methods as described above, and any associated data, data files, and data structures, may be recorded, stored, or fixed in or on one or more non-transitory computer-readable storage media. Examples of a non-transitory computer-readable storage medium include read-only memory (ROM), random-access programmable read only memory (PROM), electrically erasable programmable read-only memory (EEPROM), random-access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), flash memory, non-volatile memory, CD-ROMs, CD-Rs, CD+Rs, CD-RWs, CD+RWs, DVD-ROMs, DVD-Rs, DVD+Rs, DVD-RWs, DVD+RWs, DVD-RAMs, BD-ROMs, BD-Rs, BD-R LTHs, BD-REs, blue-ray or optical disk storage, hard disk drive (HDD), solid state drive (SSD), flash memory, card type memory such as multimedia card, secure digital (SD) card, or extreme digital (XD) card, magnetic tapes, floppy disks, magneto-optical data storage devices, optical data storage devices, hard disks, solid-state disks, and any other device that is configured to store the instructions or software and any associated data, data files, and data structures in a non-transitory manner and providing the instructions or software and any associated data, data files, and data structures to a processor or computer so that the processor or computer can execute the instructions. In one example, the instructions or software and any associated data, data files, and data structures are distributed over network-coupled computer systems so that the instructions and software and any associated data, data files, and data structures are stored, accessed, and executed in a distributed fashion by the one or more processors or computers.</div>
    <div id="p-0161" num="0160" class="description-paragraph">While this disclosure includes specific examples, it will be apparent after an understanding of the disclosure of this application that various changes in form and details may be made in these examples without departing from the spirit and scope of the claims and their equivalents. The examples described herein are to be considered in a descriptive sense only, and not for purposes of limitation. Descriptions of features or aspects in each example are to be considered as being applicable to similar features or aspects in other examples. Suitable results may be achieved if the described techniques are performed in a different order, and/or if components in a described system, architecture, device, or circuit are combined in a different manner, and/or replaced or supplemented by other components or their equivalents. Therefore, the scope of the disclosure is defined not by the detailed description, but by the claims and their equivalents, and all variations within the scope of the claims and their equivalents are to be construed as being included in the disclosure.</div>
    
  </div>
  </div>
  </section>

  <section itemprop="claims" itemscope>
    <h2>Claims (<span itemprop="count">30</span>)</h2>
    
    <div itemprop="content" html><div mxw-id="PCLM306527364" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement>
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A localization method, comprising:
<div class="claim-text">generating a first image of an object from an input image;</div>
<div class="claim-text">generating a second image to project the object with respect to a plurality of candidate localization information of a device, based on map data comprising a position of the object;</div>
<div class="claim-text">pooling, from the first image, feature values corresponding to vertices in the second image;</div>
<div class="claim-text">determining scores of the plurality of candidate localization information based on the pooled feature values; and</div>
<div class="claim-text">determining a candidate localization information corresponding to a highest score to be localization information of the device</div>
<div class="claim-text">wherein the determining of the scores comprises:
<div class="claim-text">determining rotational degree of freedom (DOF)s based on long-distance vertices affected below a first threshold by translational DOFs, from among vertices included in the second image;</div>
<div class="claim-text">after the determination of the rotational DOFs, determining a left and right translational DOF based on vanishing point-oriented short-distance vertices affected below a second threshold by a forward and backward translational DOF, from among short-distance vertices excluding the long-distance vertices from the second image; and</div>
<div class="claim-text">after the determination of the left and right translational DOF, determining the forward and backward translational DOF based on non-vanishing point-oriented short-distance vertices excluding the vanishing point-oriented short-distance vertices from the short-distance vertices.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generating of the first image comprises generating feature maps corresponding to a plurality of features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generating of the second image comprises:
<div class="claim-text">extracting a region corresponding to a field of view in the candidate localization information from the map data; and</div>
<div class="claim-text">projecting vertices included in the region into a projection point corresponding to the candidate localization information.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pooling comprises:
<div class="claim-text">selecting pixels in the first image based on coordinates of the vertices; and</div>
<div class="claim-text">obtaining feature values of the selected pixels.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the scores comprise determining a sum of the pooled feature values.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The localization method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the determining of the sum comprises, determining a weighted sum of the feature values based on weights determined for the features, in response to the first image comprising feature maps corresponding to features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the localization information of the device comprises:
<div class="claim-text">segmenting the second image into regions; and</div>
<div class="claim-text">sequentially determining a plurality of degree of freedom (DOF) values included in the candidate localization information using scores calculated in the regions.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The localization method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the plurality of DOF values comprises:
<div class="claim-text">three translational DOF values; and</div>
<div class="claim-text">three rotational DOF values.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The localization method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the segmenting comprises:
<div class="claim-text">segmenting the second image into a long-distance region and a short-distance region based on a first criterion associated with a distance; and</div>
<div class="claim-text">segmenting the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The localization method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the sequentially determining comprises:
<div class="claim-text">determining rotational DOFs based on the long-distance region;</div>
<div class="claim-text">determining a left and right translational DOF based on the vanishing point-oriented short-distance region; and</div>
<div class="claim-text">determining a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the localization information of the device comprises:
<div class="claim-text">determining a direction to improve the score based on a distribution of the pooled feature values; and</div>
<div class="claim-text">correcting the candidate localization information based on the direction.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. The localization method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first image comprises a probability distribution indicating a degree of closeness to the object,
<div class="claim-text">wherein the determining of the direction comprises determining the direction based on the probability distribution.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The localization method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the determining of the localization information of the device comprises:
<div class="claim-text">generating a corrected second image in which the object is projected with respect to the corrected candidate localization information; and</div>
<div class="claim-text">determining a corrected score of the corrected candidate localization information by pooling, from the first image, feature values corresponding to vertices in the corrected second image,</div>
<div class="claim-text">wherein the determining of the direction, the correcting of the candidate localization information, the generating of the corrected second image, and the calculating of the corrected score are iteratively performed until the corrected score satisfies a condition.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. The localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">determining a virtual object on the map data to provide an augmented reality (AR) service; and</div>
<div class="claim-text">displaying the virtual object based on the determined localization information.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. The localization method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the input image comprises a driving image of a vehicle, and
<div class="claim-text">the virtual object indicates driving route information.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text">16. A non-transitory computer-readable storage medium storing instructions that, when executed by a processor, cause the processor to perform the localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text">17. The localization method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image comprises a probability distribution indicating a degree of closeness to the object; and
<div class="claim-text">the determining of the localization information comprises determining a direction to improve the score based on a distribution of the pooled feature values, the information related to a direction towards the object being included in the first image, and correcting the candidate localization information based on the direction.</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text">18. A localization method, comprising:
<div class="claim-text">generating a first image of an object from an input image;</div>
<div class="claim-text">generating a second image to project the object with respect to candidate localization information of a device, based on map data comprising a position of the object;</div>
<div class="claim-text">segmenting the second image into a long-distance region and a short-distance region based on a first threshold associated with a distance;</div>
<div class="claim-text">segmenting the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point; and</div>
<div class="claim-text">determining degree of freedom (DOF) values included in the candidate localization information through matching between the first image and the regions,</div>
<div class="claim-text">wherein the determining of the DOF comprises:
<div class="claim-text">determining rotational DOFs based on long-distance vertices affected below a first threshold by translational DOFs, from among vertices included in the second image,</div>
<div class="claim-text">after the determination of the rotational DOFs, determining a left and right translational DOF based on vanishing point-oriented short-distance vertices affected below a second threshold by a forward and backward translational DOF, from among short-distance vertices excluding the long-distance vertices from the second image, and</div>
<div class="claim-text">after the determination of the left and right translational DOF, determining the forward and backward translational DOF based on non-vanishing point-oriented short-distance vertices excluding the vanishing point-oriented short-distance vertices from the short-distance vertices.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text">19. The localization method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the determining comprises determining the DOF values included in the candidate localization information by sequentially using scores calculated through the matching in the regions.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text">20. The localization method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the determining comprises:
<div class="claim-text">calculating, while changing DOF values determined for the regions, scores corresponding to the changed DOF values by pooling, from the first image, feature values corresponding to vertices in the regions; and</div>
<div class="claim-text">selecting a DOF value corresponding to a highest score.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
      <div class="claim-text">21. The localization method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the plurality of DOF values comprises:
<div class="claim-text">three translational DOF values; and</div>
<div class="claim-text">three rotational DOF values.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
      <div class="claim-text">22. The localization method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:
<div class="claim-text">determining a virtual object on the map data to provide an augmented reality (AR) service; and</div>
<div class="claim-text">displaying the virtual object based on the determined DOF values.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
      <div class="claim-text">23. The localization method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the input image comprises a driving image of a vehicle, and
<div class="claim-text">the virtual object indicates driving route information.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
      <div class="claim-text">24. A non-transitory computer-readable storage medium storing instructions that, when executed by a processor, cause the processor to perform the localization method of <claim-ref idref="CLM-00018">claim 18</claim-ref>.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00025" num="00025" class="claim">
      <div class="claim-text">25. A localization apparatus, comprising:
<div class="claim-text">a processor configured to:</div>
<div class="claim-text">generate a first image of an object from an input image,</div>
<div class="claim-text">generate a second image to project the object with respect to a plurality of candidate localization information of a device, based on map data comprising a position of the object,</div>
<div class="claim-text">pool, from the first image, feature values corresponding to vertices in the second image,</div>
<div class="claim-text">determine scores of the plurality of candidate localization information based on the pooled feature values; and</div>
<div class="claim-text">determine a candidate localization information corresponding to a highest score to be localization information of the device</div>
<div class="claim-text">wherein the determining of the scores comprises:</div>
<div class="claim-text">determining rotational degree of freedom (DOF)s based on long-distance vertices affected below a first threshold by translational DOFs, from among vertices included in the second image;</div>
<div class="claim-text">after the determination of the rotational DOFs, determining a left and right translational DOF based on vanishing point-oriented short-distance vertices affected below a second threshold by a forward and backward translational DOF, from among short-distance vertices excluding the long-distance vertices from the second image; and</div>
<div class="claim-text">after the determination of the left and right translational DOF, determining the forward and backward translational DOF based on non-vanishing point-oriented short-distance vertices excluding the vanishing point-oriented short-distance vertices from the short-distance vertices.</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00026" num="00026" class="claim">
      <div class="claim-text">26. A localization apparatus, comprising:
<div class="claim-text">a processor configured to:</div>
<div class="claim-text">generate a first image of an object from an input image,</div>
<div class="claim-text">generate a second image to project the object with respect to candidate localization information of a device, based on map data comprising a position of the object,</div>
<div class="claim-text">segment the second image into a long-distance region and a short-distance region based on a first threshold associated with a distance,</div>
<div class="claim-text">segment the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a second criterion associated with a vanishing point; and</div>
<div class="claim-text">determine degree of freedom (DOF) values included in the candidate localization information through matching between the first image and the regions,</div>
<div class="claim-text">wherein the processor is further configured to:
<div class="claim-text">determine rotational degree of freedom (DOF) values based on the long-distance region,</div>
<div class="claim-text">after the determination of the rotational DOF, determine a left and right translational DOF based on the vanishing point-oriented short-distance region, and</div>
<div class="claim-text">after the determination of the left and right translational DOF, determine a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</div>
</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00027" num="00027" class="claim">
      <div class="claim-text">27. A localization apparatus comprising:
<div class="claim-text">a sensor disposed on a device, and being configured to sense one or more of an image and candidate localization information of the device;</div>
<div class="claim-text">a processor configured to
<div class="claim-text">generate a first image of an object from the image,</div>
<div class="claim-text">generate a second image to project the object with respect to the candidate localization information, based on map data comprising a position of the object,</div>
<div class="claim-text">segment the second image into a long-distance region and a short-distance region based on a distance,</div>
<div class="claim-text">segment the short-distance region into a vanishing point-oriented short-distance region and a non-vanishing point-oriented short-distance region based on a vanishing point,</div>
<div class="claim-text">determine a score of the candidate localization information based on pooling, from the first image, feature values corresponding to vertices in the second image,</div>
<div class="claim-text">determine localization information of the device based on the score, and</div>
</div>
<div class="claim-text">a head-up display (HUD) configured to visualize a virtual object on the map data based on the determined localization information,</div>
<div class="claim-text">wherein the processor to determine is configured to:
<div class="claim-text">determine rotational degree of freedom (DOF) values based on the long-distance region;</div>
<div class="claim-text">after the determination of the rotational DOF, determine a left and right translational DOF based on the vanishing point-oriented short-distance region; and</div>
<div class="claim-text">after the determination of the left and right translational DOF, determine a forward and backward translational DOF based on the non-vanishing point-oriented short-distance region.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00028" num="00028" class="claim">
      <div class="claim-text">28. The localization apparatus of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the processor is further configured to generate, using a neural network, the first image comprising feature maps corresponding to a plurality of features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00029" num="00029" class="claim">
      <div class="claim-text">29. The localization apparatus of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the second image comprise a projection of two-dimensional (2D) vertices corresponding to the object.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00030" num="00030" class="claim">
      <div class="claim-text">30. The localization apparatus of <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising a memory configured to store the map data, the image, the first image, the second image, the score, and instructions that, when executed, configures the processor to determine any one or any combination of the determined localization information and the virtual object.</div>
    </div>
  </div> </div>
  </div>
  </section>

  <section itemprop="application" itemscope>

    <section itemprop="metadata" itemscope>
      <span itemprop="applicationNumber">US16/371,305</span>
      <span itemprop="priorityDate">2018-10-24</span>
      <span itemprop="filingDate">2019-04-01</span>
      <span itemprop="title">Method and apparatus for localization based on images and map data 
       </span>
      <span itemprop="ifiStatus">Active</span>
      <span itemprop="ifiExpiration">2039-05-04</span>
      <a href="/patent/US11176719B2/en">
        <span itemprop="representativePublication">US11176719B2</span>
        (<span itemprop="primaryLanguage">en</span>)
      </a>
    </section>

    

    <h2>Applications Claiming Priority (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">KR1020180127589A</span>
            <a href="/patent/KR20200046437A/en">
              <span itemprop="representativePublication">KR20200046437A</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-10-24</td>
          <td itemprop="filingDate">2018-10-24</td>
          <td itemprop="title">Localization method based on images and map data and apparatus thereof 
       </td>
        </tr>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">KR10-2018-0127589</span>
            
          </td>
          <td itemprop="priorityDate"></td>
          <td itemprop="filingDate">2018-10-24</td>
          <td itemprop="title"></td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Publications (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Publication Number</th>
          <th>Publication Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="pubs" itemscope repeat>
          <td>
            <span itemprop="publicationNumber">US20200134896A1</span>
            
            <a href="/patent/US20200134896A1/en">
              US20200134896A1
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-30</td>
        </tr>
        <tr itemprop="pubs" itemscope repeat>
          <td>
            <span itemprop="publicationNumber">US11176719B2</span>
            
            <span itemprop="thisPatent">true</span>
            <a href="/patent/US11176719B2/en">
              US11176719B2
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2021-11-16</td>
        </tr>
      </tbody>
    </table>

  </section>

  <section itemprop="family" itemscope>
    <h1>Family</h1>
    <h2>ID=67060291</h2>

    <h2>Family Applications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Title</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="applications" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US16/371,305</span>
            <span itemprop="ifiStatus">Active</span>
            <span itemprop="ifiExpiration">2039-05-04</span>
            <a href="/patent/US11176719B2/en">
              <span itemprop="representativePublication">US11176719B2</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2018-10-24</td>
          <td itemprop="filingDate">2019-04-01</td>
          <td itemprop="title">Method and apparatus for localization based on images and map data 
       </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Country Status (5)</h2>
    <table>
      <thead>
        <tr>
          <th>Country</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">US</span>
            (<span itemprop="num">1</span>)
            <meta itemprop="thisCountry" content="true">
          </td>
          <td>
            <a href="/patent/US11176719B2/en">
              <span itemprop="representativePublication">US11176719B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">EP</span>
            (<span itemprop="num">2</span>)
            
          </td>
          <td>
            <a href="/patent/EP3644278A1/en">
              <span itemprop="representativePublication">EP3644278A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">JP</span>
            (<span itemprop="num">2</span>)
            
          </td>
          <td>
            <a href="/patent/JP2020076745A/en">
              <span itemprop="representativePublication">JP2020076745A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">KR</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/KR20200046437A/en">
              <span itemprop="representativePublication">KR20200046437A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">CN</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/CN111089597A/en">
              <span itemprop="representativePublication">CN111089597A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
      </tbody>
    </table>

    

    <h2>Families Citing this family (7)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/KR102519666B1/en">
              <span itemprop="publicationNumber">KR102519666B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-10-15</td>
          <td itemprop="publicationDate">2023-04-07</td>
          <td><span itemprop="assigneeOriginal">ì¼ì±ì ìì£¼ìíì¬</span></td>
          <td itemprop="title">Device and method to convert image 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/US10789851B1/en">
              <span itemprop="publicationNumber">US10789851B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2019-09-04</td>
          <td itemprop="publicationDate">2020-09-29</td>
          <td><span itemprop="assigneeOriginal">GM Global Technology Operations LLC</span></td>
          <td itemprop="title">System and method for vision sensor detection 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/US11829128B2/en">
              <span itemprop="publicationNumber">US11829128B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-10-23</td>
          <td itemprop="publicationDate">2023-11-28</td>
          <td><span itemprop="assigneeOriginal">GM Global Technology Operations LLC</span></td>
          <td itemprop="title">Perception system diagnosis using predicted sensor data and perception results 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/KR102495005B1/en">
              <span itemprop="publicationNumber">KR102495005B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-07-13</td>
          <td itemprop="publicationDate">2023-02-06</td>
          <td><span itemprop="assigneeOriginal">ì£¼ìíì¬ ì°ë¦¬ìì´</span></td>
          <td itemprop="title">Method for indoor localization using deep learning 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/KR102449031B1/en">
              <span itemprop="publicationNumber">KR102449031B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-10-26</td>
          <td itemprop="publicationDate">2022-09-29</td>
          <td><span itemprop="assigneeOriginal">ì£¼ìíì¬ ì°ë¦¬ìì´</span></td>
          <td itemprop="title">Method for indoor localization using deep learning 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/WO2022114311A1/en">
              <span itemprop="publicationNumber">WO2022114311A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2020-11-30</td>
          <td itemprop="publicationDate">2022-06-02</td>
          <td><span itemprop="assigneeOriginal">íêµ­ì ìê¸°ì ì°êµ¬ì</span></td>
          <td itemprop="title">Method and apparatus for lidar sensor information correction and up-sampling using multiple images 
       </td>
        </tr>
        <tr itemprop="forwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/CN113269832B/en">
              <span itemprop="publicationNumber">CN113269832B</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2021-05-31</td>
          <td itemprop="publicationDate">2022-03-29</td>
          <td><span itemprop="assigneeOriginal">é¿æ¥å·¥ç¨å­¦é¢</span></td>
          <td itemprop="title">Electric power operation augmented reality navigation system and method for extreme weather environment 
       </td>
        </tr>
      </tbody>
    </table>

    <h2>Citations (21)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20100098297A1/en">
              <span itemprop="publicationNumber">US20100098297A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2008-04-24</td>
          <td itemprop="publicationDate">2010-04-22</td>
          <td>
            <span itemprop="assigneeOriginal">Gm Global Technology Operations, Inc.</span>
          </td>
          <td itemprop="title">Clear path detection using segmentation-based method 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/JP4677981B2/en">
              <span itemprop="publicationNumber">JP4677981B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2006-12-20</td>
          <td itemprop="publicationDate">2011-04-27</td>
          <td>
            <span itemprop="assigneeOriginal">ã¢ã¤ã·ã³ã»ã¨ã£ã»ãããªã¥æ ªå¼ä¼ç¤¾</span>
          </td>
          <td itemprop="title">
  Own vehicle position identification method and own vehicle position identification device
 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/KR101139389B1/en">
              <span itemprop="publicationNumber">KR101139389B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2010-04-28</td>
          <td itemprop="publicationDate">2012-04-27</td>
          <td>
            <span itemprop="assigneeOriginal">ì£¼ìíì¬ ìì´í°ìì¤ìíë¦¬í°</span>
          </td>
          <td itemprop="title">Video Analysing Apparatus and Method Using Stereo Cameras 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/JP5116555B2/en">
              <span itemprop="publicationNumber">JP5116555B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2008-04-25</td>
          <td itemprop="publicationDate">2013-01-09</td>
          <td>
            <span itemprop="assigneeOriginal">ä¸è±é»æ©æ ªå¼ä¼ç¤¾</span>
          </td>
          <td itemprop="title">
  LOCATION DEVICE, LOCATION SYSTEM, LOCATION SERVER DEVICE, AND LOCATION METHOD
 
     </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US8649565B1/en">
              <span itemprop="publicationNumber">US8649565B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2009-06-18</td>
          <td itemprop="publicationDate">2014-02-11</td>
          <td>
            <span itemprop="assigneeOriginal">Hrl Laboratories, Llc</span>
          </td>
          <td itemprop="title">System for automatic object localization based on visual simultaneous localization and mapping (SLAM) and cognitive swarm recognition 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/JP5808369B2/en">
              <span itemprop="publicationNumber">JP5808369B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2008-08-29</td>
          <td itemprop="publicationDate">2015-11-10</td>
          <td>
            <span itemprop="assigneeOriginal">ä¸è±é»æ©æ ªå¼ä¼ç¤¾</span>
          </td>
          <td itemprop="title">
  Overhead image generation device, overhead image generation method, and overhead image generation program
 
     </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/KR20160128077A/en">
              <span itemprop="publicationNumber">KR20160128077A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-04-28</td>
          <td itemprop="publicationDate">2016-11-07</td>
          <td>
            <span itemprop="assigneeOriginal">íêµ­ì ìíµì ì°êµ¬ì</span>
          </td>
          <td itemprop="title">Auto calibration apparatus for vihicle sensor and method thereof 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US9524434B2/en">
              <span itemprop="publicationNumber">US9524434B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2013-10-04</td>
          <td itemprop="publicationDate">2016-12-20</td>
          <td>
            <span itemprop="assigneeOriginal">Qualcomm Incorporated</span>
          </td>
          <td itemprop="title">Object tracking based on dynamically built environment map data 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/JP2017009554A/en">
              <span itemprop="publicationNumber">JP2017009554A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-06-26</td>
          <td itemprop="publicationDate">2017-01-12</td>
          <td>
            <span itemprop="assigneeOriginal">æ¥ç£èªåè»æ ªå¼ä¼ç¤¾</span>
          </td>
          <td itemprop="title">Vehicle location determination device and vehicle location determination method 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20170169300A1/en">
              <span itemprop="publicationNumber">US20170169300A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-12-15</td>
          <td itemprop="publicationDate">2017-06-15</td>
          <td>
            <span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span>
          </td>
          <td itemprop="title">System and method for image based vehicle localization 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/KR20170070945A/en">
              <span itemprop="publicationNumber">KR20170070945A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-12-14</td>
          <td itemprop="publicationDate">2017-06-23</td>
          <td>
            <span itemprop="assigneeOriginal">íëì¤í¸ë¡  ì£¼ìíì¬</span>
          </td>
          <td itemprop="title">Apparatus for pitch angle detecting of vehicle using previous vehicle recognition and method therof 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US9811731B2/en">
              <span itemprop="publicationNumber">US9811731B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2013-10-04</td>
          <td itemprop="publicationDate">2017-11-07</td>
          <td>
            <span itemprop="assigneeOriginal">Qualcomm Incorporated</span>
          </td>
          <td itemprop="title">Dynamic extension of map data for object detection and tracking 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/KR20180009280A/en">
              <span itemprop="publicationNumber">KR20180009280A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2016-07-18</td>
          <td itemprop="publicationDate">2018-01-26</td>
          <td>
            <span itemprop="assigneeOriginal">ì£¼ìíì¬ í¨ì ìíí¸</span>
          </td>
          <td itemprop="title">System for route providing and detecting land position using 3d map data 
     </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20180075643A1/en">
              <span itemprop="publicationNumber">US20180075643A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-04-10</td>
          <td itemprop="publicationDate">2018-03-15</td>
          <td>
            <span itemprop="assigneeOriginal">The European Atomic Energy Community (Euratom), Represented By The European Commission</span>
          </td>
          <td itemprop="title">Method and device for real-time mapping and localization 
     </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/KR20180069501A/en">
              <span itemprop="publicationNumber">KR20180069501A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2016-12-15</td>
          <td itemprop="publicationDate">2018-06-25</td>
          <td>
            <span itemprop="assigneeOriginal">íëìëì°¨ì£¼ìíì¬</span>
          </td>
          <td itemprop="title">Apparatus for estimating location of vehicle, method for thereof, apparatus for constructing map thereof, and method for constructing map 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20180253609A1/en">
              <span itemprop="publicationNumber">US20180253609A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-07-28</td>
          <td itemprop="publicationDate">2018-09-06</td>
          <td>
            <span itemprop="assigneeOriginal">Apple Inc.</span>
          </td>
          <td itemprop="title">System and method for light and image projection 
     </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20180283892A1/en">
              <span itemprop="publicationNumber">US20180283892A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-04-03</td>
          <td itemprop="publicationDate">2018-10-04</td>
          <td>
            <span itemprop="assigneeOriginal">Robert Bosch Gmbh</span>
          </td>
          <td itemprop="title">Automated image labeling for vehicles based on maps 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20180336697A1/en">
              <span itemprop="publicationNumber">US20180336697A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-05-22</td>
          <td itemprop="publicationDate">2018-11-22</td>
          <td>
            <span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span>
          </td>
          <td itemprop="title">Monocular localization in urban environments using road markings 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20190120947A1/en">
              <span itemprop="publicationNumber">US20190120947A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-10-19</td>
          <td itemprop="publicationDate">2019-04-25</td>
          <td>
            <span itemprop="assigneeOriginal">DeepMap Inc.</span>
          </td>
          <td itemprop="title">Lidar to camera calibration based on edge detection 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20200105017A1/en">
              <span itemprop="publicationNumber">US20200105017A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-09-30</td>
          <td itemprop="publicationDate">2020-04-02</td>
          <td>
            <span itemprop="assigneeOriginal">Boe Technology Group Co., Ltd.</span>
          </td>
          <td itemprop="title">Calibration method and calibration device of vehicle-mounted camera, vehicle and storage medium 
       </td>
        </tr>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            <a href="/patent/US20200249032A1/en">
              <span itemprop="publicationNumber">US20200249032A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-01-15</td>
          <td itemprop="publicationDate">2020-08-06</td>
          <td>
            <span itemprop="assigneeOriginal">Sk Telecom Co., Ltd.</span>
          </td>
          <td itemprop="title">Apparatus and method for updating high definition map for autonomous driving 
     </td>
        </tr>
      </tbody>
    </table>

    <h2>Family Cites Families (3)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/US20110153198A1/en">
              <span itemprop="publicationNumber">US20110153198A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2009-12-21</td>
          <td itemprop="publicationDate">2011-06-23</td>
          <td><span itemprop="assigneeOriginal">Navisus LLC</span></td>
          <td itemprop="title">Method for the display of navigation instructions using an augmented-reality concept 
       </td>
        </tr>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/RU2628553C1/en">
              <span itemprop="publicationNumber">RU2628553C1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2014-02-24</td>
          <td itemprop="publicationDate">2017-08-18</td>
          <td><span itemprop="assigneeOriginal">ÐÐ¸ÑÑÐ°Ð½ ÐÐ¾ÑÐ¾Ñ ÐÐ¾., ÐÑÐ´.</span></td>
          <td itemprop="title">Own position calculating device and own position calculating method 
       </td>
        </tr>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/US10309777B2/en">
              <span itemprop="publicationNumber">US10309777B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2016-12-30</td>
          <td itemprop="publicationDate">2019-06-04</td>
          <td><span itemprop="assigneeOriginal">DeepMap Inc.</span></td>
          <td itemprop="title">Visual odometry and pairwise alignment for high definition map creation 
       </td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2018</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2018-10-24</span>
            <span itemprop="countryCode">KR</span>
            <span itemprop="applicationNumber">KR1020180127589A</span>
            <a href="/patent/KR20200046437A/en"><span itemprop="documentId">patent/KR20200046437A/en</span></a>
            <span itemprop="legalStatusCat">not_active</span>
            <span itemprop="legalStatus">Application Discontinuation</span>
            
          </li>
        </ul>
      </li>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2019</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-04-01</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US16/371,305</span>
            <a href="/patent/US11176719B2/en"><span itemprop="documentId">patent/US11176719B2/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            <span itemprop="thisApp" content="true" bool></span>
          </li>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-06-25</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP19182124.8A</span>
            <a href="/patent/EP3644278A1/en"><span itemprop="documentId">patent/EP3644278A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-06-25</span>
            <span itemprop="countryCode">EP</span>
            <span itemprop="applicationNumber">EP23190121.6A</span>
            <a href="/patent/EP4246440A3/en"><span itemprop="documentId">patent/EP4246440A3/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-07-17</span>
            <span itemprop="countryCode">CN</span>
            <span itemprop="applicationNumber">CN201910648410.XA</span>
            <a href="/patent/CN111089597A/en"><span itemprop="documentId">patent/CN111089597A/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2019-09-20</span>
            <span itemprop="countryCode">JP</span>
            <span itemprop="applicationNumber">JP2019171729A</span>
            <a href="/patent/JP2020076745A/en"><span itemprop="documentId">patent/JP2020076745A/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
        </ul>
      </li>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2024</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2024-02-01</span>
            <span itemprop="countryCode">JP</span>
            <span itemprop="applicationNumber">JP2024013948A</span>
            <a href="/patent/JP2024032933A/en"><span itemprop="documentId">patent/JP2024032933A/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            
          </li>
        </ul>
      </li>
    </ul>

    </section>

  <section>
    <h2>Patent Citations (21)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/JP4677981B2/en">
              <span itemprop="publicationNumber">JP4677981B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2006-12-20</td>
          <td itemprop="publicationDate">2011-04-27</td>
          <td><span itemprop="assigneeOriginal">ã¢ã¤ã·ã³ã»ã¨ã£ã»ãããªã¥æ ªå¼ä¼ç¤¾</span></td>
          <td itemprop="title">
  Own vehicle position identification method and own vehicle position identification device
 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20100098297A1/en">
              <span itemprop="publicationNumber">US20100098297A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2008-04-24</td>
          <td itemprop="publicationDate">2010-04-22</td>
          <td><span itemprop="assigneeOriginal">Gm Global Technology Operations, Inc.</span></td>
          <td itemprop="title">Clear path detection using segmentation-based method 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/JP5116555B2/en">
              <span itemprop="publicationNumber">JP5116555B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2008-04-25</td>
          <td itemprop="publicationDate">2013-01-09</td>
          <td><span itemprop="assigneeOriginal">ä¸è±é»æ©æ ªå¼ä¼ç¤¾</span></td>
          <td itemprop="title">
  LOCATION DEVICE, LOCATION SYSTEM, LOCATION SERVER DEVICE, AND LOCATION METHOD
 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/JP5808369B2/en">
              <span itemprop="publicationNumber">JP5808369B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2008-08-29</td>
          <td itemprop="publicationDate">2015-11-10</td>
          <td><span itemprop="assigneeOriginal">ä¸è±é»æ©æ ªå¼ä¼ç¤¾</span></td>
          <td itemprop="title">
  Overhead image generation device, overhead image generation method, and overhead image generation program
 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US8649565B1/en">
              <span itemprop="publicationNumber">US8649565B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2009-06-18</td>
          <td itemprop="publicationDate">2014-02-11</td>
          <td><span itemprop="assigneeOriginal">Hrl Laboratories, Llc</span></td>
          <td itemprop="title">System for automatic object localization based on visual simultaneous localization and mapping (SLAM) and cognitive swarm recognition 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/KR101139389B1/en">
              <span itemprop="publicationNumber">KR101139389B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2010-04-28</td>
          <td itemprop="publicationDate">2012-04-27</td>
          <td><span itemprop="assigneeOriginal">ì£¼ìíì¬ ìì´í°ìì¤ìíë¦¬í°</span></td>
          <td itemprop="title">Video Analysing Apparatus and Method Using Stereo Cameras 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US9811731B2/en">
              <span itemprop="publicationNumber">US9811731B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2013-10-04</td>
          <td itemprop="publicationDate">2017-11-07</td>
          <td><span itemprop="assigneeOriginal">Qualcomm Incorporated</span></td>
          <td itemprop="title">Dynamic extension of map data for object detection and tracking 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US9524434B2/en">
              <span itemprop="publicationNumber">US9524434B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2013-10-04</td>
          <td itemprop="publicationDate">2016-12-20</td>
          <td><span itemprop="assigneeOriginal">Qualcomm Incorporated</span></td>
          <td itemprop="title">Object tracking based on dynamically built environment map data 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20180075643A1/en">
              <span itemprop="publicationNumber">US20180075643A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-04-10</td>
          <td itemprop="publicationDate">2018-03-15</td>
          <td><span itemprop="assigneeOriginal">The European Atomic Energy Community (Euratom), Represented By The European Commission</span></td>
          <td itemprop="title">Method and device for real-time mapping and localization 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/KR20160128077A/en">
              <span itemprop="publicationNumber">KR20160128077A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-04-28</td>
          <td itemprop="publicationDate">2016-11-07</td>
          <td><span itemprop="assigneeOriginal">íêµ­ì ìíµì ì°êµ¬ì</span></td>
          <td itemprop="title">Auto calibration apparatus for vihicle sensor and method thereof 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/JP2017009554A/en">
              <span itemprop="publicationNumber">JP2017009554A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-06-26</td>
          <td itemprop="publicationDate">2017-01-12</td>
          <td><span itemprop="assigneeOriginal">æ¥ç£èªåè»æ ªå¼ä¼ç¤¾</span></td>
          <td itemprop="title">Vehicle location determination device and vehicle location determination method 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20180253609A1/en">
              <span itemprop="publicationNumber">US20180253609A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-07-28</td>
          <td itemprop="publicationDate">2018-09-06</td>
          <td><span itemprop="assigneeOriginal">Apple Inc.</span></td>
          <td itemprop="title">System and method for light and image projection 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/KR20170070945A/en">
              <span itemprop="publicationNumber">KR20170070945A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2015-12-14</td>
          <td itemprop="publicationDate">2017-06-23</td>
          <td><span itemprop="assigneeOriginal">íëì¤í¸ë¡  ì£¼ìíì¬</span></td>
          <td itemprop="title">Apparatus for pitch angle detecting of vehicle using previous vehicle recognition and method therof 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20170169300A1/en">
              <span itemprop="publicationNumber">US20170169300A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2015-12-15</td>
          <td itemprop="publicationDate">2017-06-15</td>
          <td><span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span></td>
          <td itemprop="title">System and method for image based vehicle localization 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/KR20180009280A/en">
              <span itemprop="publicationNumber">KR20180009280A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2016-07-18</td>
          <td itemprop="publicationDate">2018-01-26</td>
          <td><span itemprop="assigneeOriginal">ì£¼ìíì¬ í¨ì ìíí¸</span></td>
          <td itemprop="title">System for route providing and detecting land position using 3d map data 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/KR20180069501A/en">
              <span itemprop="publicationNumber">KR20180069501A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2016-12-15</td>
          <td itemprop="publicationDate">2018-06-25</td>
          <td><span itemprop="assigneeOriginal">íëìëì°¨ì£¼ìíì¬</span></td>
          <td itemprop="title">Apparatus for estimating location of vehicle, method for thereof, apparatus for constructing map thereof, and method for constructing map 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20180283892A1/en">
              <span itemprop="publicationNumber">US20180283892A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-04-03</td>
          <td itemprop="publicationDate">2018-10-04</td>
          <td><span itemprop="assigneeOriginal">Robert Bosch Gmbh</span></td>
          <td itemprop="title">Automated image labeling for vehicles based on maps 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20180336697A1/en">
              <span itemprop="publicationNumber">US20180336697A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-05-22</td>
          <td itemprop="publicationDate">2018-11-22</td>
          <td><span itemprop="assigneeOriginal">Honda Motor Co., Ltd.</span></td>
          <td itemprop="title">Monocular localization in urban environments using road markings 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20190120947A1/en">
              <span itemprop="publicationNumber">US20190120947A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-10-19</td>
          <td itemprop="publicationDate">2019-04-25</td>
          <td><span itemprop="assigneeOriginal">DeepMap Inc.</span></td>
          <td itemprop="title">Lidar to camera calibration based on edge detection 
       </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20200249032A1/en">
              <span itemprop="publicationNumber">US20200249032A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-01-15</td>
          <td itemprop="publicationDate">2020-08-06</td>
          <td><span itemprop="assigneeOriginal">Sk Telecom Co., Ltd.</span></td>
          <td itemprop="title">Apparatus and method for updating high definition map for autonomous driving 
     </td>
        </tr>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            <a href="/patent/US20200105017A1/en">
              <span itemprop="publicationNumber">US20200105017A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2018-09-30</td>
          <td itemprop="publicationDate">2020-04-02</td>
          <td><span itemprop="assigneeOriginal">Boe Technology Group Co., Ltd.</span></td>
          <td itemprop="title">Calibration method and calibration device of vehicle-mounted camera, vehicle and storage medium 
       </td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Non-Patent Citations (5)</h2>
    <table>
      <caption>* Cited by examiner, â  Cited by third party</caption>
      <thead>
        <tr>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Engel, Jakob, et al., "<a href='http://scholar.google.com/scholar?q="LSD-SLAM%3A+Large-Scale+Direct+Monocular+SLAM"'>LSD-SLAM: Large-Scale Direct Monocular SLAM</a>", European conference on computer vision, 2014 (pp. 834-849).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Extended European Search Report dated Nov. 7, 2019 in counterpart European Patent Application No. 19182124.8 (7 pages in English).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Kunina, I. A., et al. "<a href='http://scholar.google.com/scholar?q="Aerial+Image+Geolocalization+by+Matching+Its+Line+Structure+with+Route+Map"'>Aerial Image Geolocalization by Matching Its Line Structure with Route Map</a>", Ninth International Conference on Machine Vision (ICMV 2016), vol. 10341, International Society for Optics and Photonics, Mar. 17, 2017 (8 pages in English).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Lepetit, Vincent et al., "<a href='http://scholar.google.com/scholar?q="EPnP%3A+An+Accurate+o+%28n%29+Solution+to+the+PnP+Problem"'>EPnP: An Accurate o (n) Solution to the PnP Problem</a>", International journal of computer vision, vol. 81, No. 2, 2009 (12 pages in English).</span>
            
            
          </td>
        </tr>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Lu, Yan et al., "<a href='http://scholar.google.com/scholar?q="Monocular+Localization+in+Urban+Environments+using+Road+Markings"'>Monocular Localization in Urban Environments using Road Markings</a>", 2017 IEEE Intelligent Vehicles Symposium (IV), Jun. 11-14, 2017 (pp. 468-474).</span>
            
            
          </td>
        </tr>
      </tbody>
    </table>
  </section>

  

  <section>
    <h2>Also Published As</h2>
    <table>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Publication date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/EP4246440A3/en">
              <span itemprop="publicationNumber">EP4246440A3</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2024-01-03</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/EP4246440A2/en">
              <span itemprop="publicationNumber">EP4246440A2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-09-20</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/JP2020076745A/en">
              <span itemprop="publicationNumber">JP2020076745A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-05-21</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/KR20200046437A/en">
              <span itemprop="publicationNumber">KR20200046437A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-05-07</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/JP2024032933A/en">
              <span itemprop="publicationNumber">JP2024032933A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2024-03-12</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/EP3644278A1/en">
              <span itemprop="publicationNumber">EP3644278A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-29</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/US20200134896A1/en">
              <span itemprop="publicationNumber">US20200134896A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-04-30</td>
        </tr>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/CN111089597A/en">
              <span itemprop="publicationNumber">CN111089597A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2020-05-01</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Similar Documents</h2>
    <table>
      <thead>
        <tr>
          <th>Publication</th>
          <th>Publication Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11842447B2/en">
                <span itemprop="publicationNumber">US11842447B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-12-12">2023-12-12</time>
            
            
          </td>
          <td itemprop="title">Localization method and apparatus of displaying virtual object in augmented reality 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11176719B2/en">
                <span itemprop="publicationNumber">US11176719B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-11-16">2021-11-16</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for localization based on images and map data 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/EP3640599B1/en">
                <span itemprop="publicationNumber">EP3640599B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-12-21">2022-12-21</time>
            
            
          </td>
          <td itemprop="title">Vehicle localization method and apparatus 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10599930B2/en">
                <span itemprop="publicationNumber">US10599930B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-03-24">2020-03-24</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus of detecting object of interest 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11113544B2/en">
                <span itemprop="publicationNumber">US11113544B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-09-07">2021-09-07</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus providing information for driving vehicle 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11465642B2/en">
                <span itemprop="publicationNumber">US11465642B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-10-11">2022-10-11</time>
            
            
          </td>
          <td itemprop="title">Real-time map generation system for autonomous vehicles 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11227398B2/en">
                <span itemprop="publicationNumber">US11227398B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-01-18">2022-01-18</time>
            
            
          </td>
          <td itemprop="title">RGB point clouds based map generation system for autonomous vehicles 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11608078B2/en">
                <span itemprop="publicationNumber">US11608078B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-03-21">2023-03-21</time>
            
            
          </td>
          <td itemprop="title">Point clouds registration system for autonomous vehicles 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10732004B2/en">
                <span itemprop="publicationNumber">US10732004B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-08-04">2020-08-04</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for displaying virtual route 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11315317B2/en">
                <span itemprop="publicationNumber">US11315317B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-04-26">2022-04-26</time>
            
            
          </td>
          <td itemprop="title">Point clouds ghosting effects detection system for autonomous driving vehicles 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11294392B2/en">
                <span itemprop="publicationNumber">US11294392B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-04-05">2022-04-05</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for determining road line 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20210373161A1/en">
                <span itemprop="publicationNumber">US20210373161A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-12-02">2021-12-02</time>
            
            
          </td>
          <td itemprop="title">Lidar localization using 3d cnn network for solution inference in autonomous driving vehicles 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/EP3624002A2/en">
                <span itemprop="publicationNumber">EP3624002A2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-03-18">2020-03-18</time>
            
            
          </td>
          <td itemprop="title">Training data generating method for image processing, image processing method, and devices thereof 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN112740268B/en">
                <span itemprop="publicationNumber">CN112740268B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-06-07">2022-06-07</time>
            
            
          </td>
          <td itemprop="title">Target detection method and device 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20210365712A1/en">
                <span itemprop="publicationNumber">US20210365712A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-11-25">2021-11-25</time>
            
            
          </td>
          <td itemprop="title">Deep learning-based feature extraction for lidar localization of autonomous driving vehicles 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10891795B2/en">
                <span itemprop="publicationNumber">US10891795B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-01-12">2021-01-12</time>
            
            
          </td>
          <td itemprop="title">Localization method and apparatus based on 3D color map 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11544507B2/en">
                <span itemprop="publicationNumber">US11544507B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-01-03">2023-01-03</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus to train image recognition model, and image recognition method and apparatus 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN114972136A/en">
                <span itemprop="publicationNumber">CN114972136A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-08-30">2022-08-30</time>
            
            
          </td>
          <td itemprop="title">Training method of 3D target detection model, and 3D target detection method and device 
       </td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Legal Events</h2>
    <table>
      <thead>
        <tr>
          <th>Date</th>
          <th>Code</th>
          <th>Title</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2019-04-01">2019-04-01</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">SAMSUNG ELECTRONCIS CO., LTD., KOREA, REPUBLIC OF</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CHANG, HYUN SUNG;SON, MINJUNG;SAGONG, DONGHOON;AND OTHERS;REEL/FRAME:048752/0800</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Effective date</strong>:
              <span itemprop="value">20190313</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2019-04-01">2019-04-01</time></td>
          <td itemprop="code">FEPP</td>
          <td itemprop="title">Fee payment procedure</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2020-06-24">2020-06-24</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NON FINAL ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-01-29">2021-01-29</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">FINAL REJECTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-04-02">2021-04-02</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-04-08">2021-04-08</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ADVISORY ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-04-12">2021-04-12</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-04-15">2021-04-15</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ADVISORY ACTION MAILED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-05-03">2021-05-03</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">DOCKETED NEW CASE - READY FOR EXAMINATION</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-07-21">2021-07-21</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-10-14">2021-10-14</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2021-10-27">2021-10-27</time></td>
          <td itemprop="code">STCF</td>
          <td itemprop="title">Information on status: patent grant</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">PATENTED CASE</span>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </section>

</article>

    </search-app>
    <script type="text/javascript" src="//www.gstatic.com/feedback/api.js"></script>
    <script type="text/javascript" src="//www.gstatic.com/feedback/js/help/prod/service/lazy.min.js"></script>
    <script type="text/javascript">
      if (window.help && window.help.service) {
        helpApi = window.help.service.Lazy.create(0, {apiKey: 'AIzaSyDTEI_0tLX4varJ7bwK8aT-eOI5qr3BmyI', locale: 'en-US'});
        window.requestedSurveys = new Set();
        window.requestSurvey = function(triggerId) {
          if (window.requestedSurveys.has(triggerId)) {
            return;
          }
          window.requestedSurveys.add(triggerId);
          helpApi.requestSurvey({
            triggerId: triggerId,
            enableTestingMode: false,
            callback: (requestSurveyCallbackParam) => {
              if (!requestSurveyCallbackParam.surveyData) {
                return;
              }
              helpApi.presentSurvey({
                productData: {
                  productVersion: window.version,
                  customData: {
                    "experiments": "72459301,72474719",
                  },
                },
                surveyData: requestSurveyCallbackParam.surveyData,
                colorScheme: 1,
                customZIndex: 10000,
              });
            }
          });
        };

        window.requestSurvey('YXTwAsvoW0kedxbuTdH0RArc9VhT');
      }
    </script>
    <script src="/sw/null_loader.js"></script>
  </body>
</html>
