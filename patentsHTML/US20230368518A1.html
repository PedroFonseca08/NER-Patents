<!DOCTYPE html>
<html lang="en">
  <head>
    <title>US20230368518A1 - Deep neural network-based object detection method, and cloud sever and edge device performing deep neural network-based object detection method 
      - Google Patents</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <meta name="referrer" content="origin-when-crossorigin">
    <link rel="canonical" href="https://patents.google.com/patent/US20230368518A1/en">
    <meta name="description" content="
     The present disclosure relates to a deep neural network-based object detection device, system, and method, and more particularly, to a deep neural network-based object detection device, system, and method capable of rapidly detecting an object in an image in real time. 
   
   ">
    <meta name="DC.type" content="patent">
    <meta name="DC.title" content="Deep neural network-based object detection method, and cloud sever and edge device performing deep neural network-based object detection method 
     ">
    <meta name="DC.date" content="2023-05-16" scheme="dateSubmitted">
    <meta name="DC.description" content="
     The present disclosure relates to a deep neural network-based object detection device, system, and method, and more particularly, to a deep neural network-based object detection device, system, and method capable of rapidly detecting an object in an image in real time. 
   
   ">
    <meta name="citation_patent_application_number" content="US:18/197,891">
    <meta name="citation_pdf_url" content="https://patentimages.storage.googleapis.com/f6/c1/a4/65b965fffb64b3/US20230368518A1.pdf">
    <meta name="citation_patent_publication_number" content="US:20230368518:A1">
    <meta name="DC.date" content="2023-11-16">
    <meta name="DC.contributor" content="Joo Chan LEE" scheme="inventor">
    <meta name="DC.contributor" content="Jong Hwan KO" scheme="inventor">
    <meta name="DC.contributor" content="Sungkyunkwan University Research and Business Foundation" scheme="assignee">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Product+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700">

    <style>
      body { transition: none; }
    </style>

    <script>
      window.version = 'patent-search.search_20240108_RC01';

      function sendFeedback() {
        userfeedback.api.startFeedback({
          'productId': '713680',
          'bucket': 'patent-search-web',
          'productVersion': window.version,
        });
      }

      window.experiments = {};
      window.experiments.patentCountries = "ae,ag,al,am,ao,ap,ar,at,au,aw,az,ba,bb,bd,be,bf,bg,bh,bj,bn,bo,br,bw,bx,by,bz,ca,cf,cg,ch,ci,cl,cm,cn,co,cr,cs,cu,cy,cz,dd,de,dj,dk,dm,do,dz,ea,ec,ee,eg,em,ep,es,fi,fr,ga,gb,gc,gd,ge,gh,gm,gn,gq,gr,gt,gw,hk,hn,hr,hu,ib,id,ie,il,in,ir,is,it,jo,jp,ke,kg,kh,km,kn,kp,kr,kw,kz,la,lc,li,lk,lr,ls,lt,lu,lv,ly,ma,mc,md,me,mg,mk,ml,mn,mo,mr,mt,mw,mx,my,mz,na,ne,ng,ni,nl,no,nz,oa,om,pa,pe,pg,ph,pl,pt,py,qa,ro,rs,ru,rw,sa,sc,sd,se,sg,si,sk,sl,sm,sn,st,su,sv,sy,sz,td,tg,th,tj,tm,tn,tr,tt,tw,tz,ua,ug,us,uy,uz,vc,ve,vn,wo,yu,za,zm,zw";
      
      
      window.experiments.keywordWizard = true;
      
      
      
      window.experiments.definitions = true;

      window.Polymer = {
        dom: 'shady',
        lazyRegister: true,
      };
    </script>

    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/webcomponentsjs/webcomponents-lite.min.js"></script>
    <link rel="import" href="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.html">
  </head>
  <body unresolved>
    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20240108_RC01/scs/compiled_dir/search-app-vulcanized.js"></script>
    <search-app>
      
      

      <article class="result" itemscope itemtype="http://schema.org/ScholarlyArticle">
  <h1 itemprop="pageTitle">US20230368518A1 - Deep neural network-based object detection method, and cloud sever and edge device performing deep neural network-based object detection method 
      - Google Patents</h1>
  <span itemprop="title">Deep neural network-based object detection method, and cloud sever and edge device performing deep neural network-based object detection method 
     </span>

  <meta itemprop="type" content="patent">
  <a href="https://patentimages.storage.googleapis.com/f6/c1/a4/65b965fffb64b3/US20230368518A1.pdf" itemprop="pdfLink">Download PDF</a>
  <h2>Info</h2>

  <dl>
    <dt>Publication number</dt>
    <dd itemprop="publicationNumber">US20230368518A1</dd>
    <meta itemprop="numberWithoutCodes" content="20230368518">
    <meta itemprop="kindCode" content="A1">
    <meta itemprop="publicationDescription" content="Patent application publication">
    <span>US20230368518A1</span>
    <span>US18/197,891</span>
    <span>US202318197891A</span>
    <span>US2023368518A1</span>
    <span>US 20230368518 A1</span>
    <span>US20230368518 A1</span>
    <span>US 20230368518A1</span>
    <span>  </span>
    <span> </span>
    <span> </span>
    <span>US 202318197891 A</span>
    <span>US202318197891 A</span>
    <span>US 202318197891A</span>
    <span>US 2023368518 A1</span>
    <span>US2023368518 A1</span>
    <span>US 2023368518A1</span>

    <dt>Authority</dt>
    <dd itemprop="countryCode">US</dd>
    <dd itemprop="countryName">United States</dd>

    <dt>Prior art keywords</dt>
    <dd itemprop="priorArtKeywords" repeat>extractor</dd>
    <dd itemprop="priorArtKeywords" repeat>cloud sever</dd>
    <dd itemprop="priorArtKeywords" repeat>edge device</dd>
    <dd itemprop="priorArtKeywords" repeat>cloud</dd>
    <dd itemprop="priorArtKeywords" repeat>sever</dd>

    <dt>Prior art date</dt>
    <dd><time itemprop="priorArtDate" datetime="2022-05-16">2022-05-16</time></dd>

    <dt>Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)</dt>
    <dd itemprop="legalStatusIfi" itemscope>
      <span itemprop="status">Pending</span>
    </dd>
  </dl>

  <dt>Application number</dt>
  <dd itemprop="applicationNumber">US18/197,891</dd>

  

  

  <dt>Inventor</dt>
  <dd itemprop="inventor" repeat>Joo Chan LEE</dd>
  <dd itemprop="inventor" repeat>Jong Hwan KO</dd>

  <dt>Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)</dt>
  <dd itemprop="assigneeCurrent" repeat>
    Sungkyunkwan University Research and Business Foundation
  </dd>

  <dt>Original Assignee</dt>
  <dd itemprop="assigneeOriginal" repeat>Sungkyunkwan University Research and Business Foundation</dd>

  <dt>Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)</dt>
  <dd><time itemprop="priorityDate" datetime="2022-05-16">2022-05-16</time></dd>

  <dt>Filing date</dt>
  <dd><time itemprop="filingDate" datetime="2023-05-16">2023-05-16</time></dd>

  <dt>Publication date</dt>
  <dd><time itemprop="publicationDate" datetime="2023-11-16">2023-11-16</time></dd>

  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-05-16">2023-05-16</time>
    <span itemprop="title">Application filed by Sungkyunkwan University Research and Business Foundation</span>
    <span itemprop="type">filed</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    <span itemprop="assigneeSearch">Sungkyunkwan University Research and Business Foundation</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-05-16">2023-05-16</time>
    <span itemprop="title">Assigned to Research &amp; Business Foundation Sungkyunkwan University</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">Research &amp; Business Foundation Sungkyunkwan University</span>
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    <span itemprop="description" repeat>Assignors: KO, JONG HWAN, LEE, JOO CHAN</span>
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2023-11-16">2023-11-16</time>
    <span itemprop="title">Publication of US20230368518A1</span>
    <span itemprop="type">publication</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    <span itemprop="documentId">patent/US20230368518A1/en</span>
    
  </dd>
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date">Status</time>
    <span itemprop="title">Pending</span>
    <span itemprop="type">legal-status</span>
    <span itemprop="critical" content="true" bool>Critical</span>
    <span itemprop="current" content="true" bool>Current</span>
    
    
    
  </dd>

  <h2>Links</h2>
  <ul>
    <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoLink">
          <a href="https://appft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&p=1&u=/netahtml/PTO/srchnum.html&r=1&f=G&l=50&d=PG01&s1=20230368518.PGNR." itemprop="url" target="_blank"><span itemprop="text">USPTO</span></a>
        </li>
        
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoPatentCenterLink">
          <a href="https://patentcenter.uspto.gov/applications/18197891" itemprop="url" target="_blank"><span itemprop="text">USPTO PatentCenter</span></a>
        </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoAssignmentLink">
          <a href="https://assignment.uspto.gov/patent/index.html#/patent/search/resultFilter?searchInput=20230368518" itemprop="url" target="_blank"><span itemprop="text">USPTO Assignment</span></a>
        </li>

    <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="espacenetLink">
        <a href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=2023368518A1&amp;KC=A1&amp;FT=D" itemprop="url" target="_blank"><span itemprop="text">Espacenet</span></a>
      </li>
      

    

    
      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="globalDossierLink">
        <a href="https://globaldossier.uspto.gov/#/result/publication/US/20230368518/1" itemprop="url" target="_blank"><span itemprop="text">Global Dossier</span></a>
      </li>

      

      

      

      <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="stackexchangeLink">
        <a href="https://patents.stackexchange.com/questions/tagged/US20230368518" itemprop="url"><span itemprop="text">Discuss</span></a>
      </li>
      
  </ul>

  <ul itemprop="concept" itemscope>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013528</span>
      <span itemprop="name">artificial neural network</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>title</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">46</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000001514</span>
      <span itemprop="name">detection method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>title</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">41</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012549</span>
      <span itemprop="name">training</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">26</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004458</span>
      <span itemprop="name">analytical method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">6</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000284</span>
      <span itemprop="name">extract</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>claims</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000000034</span>
      <span itemprop="name">method</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>abstract</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">15</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006870</span>
      <span itemprop="name">function</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">12</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012545</span>
      <span itemprop="name">processing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">11</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000013473</span>
      <span itemprop="name">artificial intelligence</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">9</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000010586</span>
      <span itemprop="name">diagram</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">8</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000004065</span>
      <span itemprop="name">semiconductor</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">7</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000011161</span>
      <span itemprop="name">development</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">6</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004891</span>
      <span itemprop="name">communication</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">5</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000005540</span>
      <span itemprop="name">biological transmission</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">4</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004590</span>
      <span itemprop="name">computer program</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">4</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000011160</span>
      <span itemprop="name">research</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">3</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003491</span>
      <span itemprop="name">array</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000007423</span>
      <span itemprop="name">decrease</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000694</span>
      <span itemprop="name">effects</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005516</span>
      <span itemprop="name">engineering process</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000007781</span>
      <span itemprop="name">pre-processing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003860</span>
      <span itemprop="name">storage</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">2</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">101001095089</span>
      <span itemprop="name">Homo sapiens PML-RARA-regulated adapter molecule 1</span>
      <span itemprop="domain">Proteins</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">102100037019</span>
      <span itemprop="name">PML-RARA-regulated adapter molecule 1</span>
      <span itemprop="domain">Human genes</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004140</span>
      <span itemprop="name">cleaning</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003247</span>
      <span itemprop="name">decreasing effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006866</span>
      <span itemprop="name">deterioration</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003745</span>
      <span itemprop="name">diagnosis</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000011156</span>
      <span itemprop="name">evaluation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004519</span>
      <span itemprop="name">manufacturing process</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003278</span>
      <span itemprop="name">mimic effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000012986</span>
      <span itemprop="name">modification</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004048</span>
      <span itemprop="name">modification</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000002569</span>
      <span itemprop="name">neuron</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001151</span>
      <span itemprop="name">other effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005070</span>
      <span itemprop="name">sampling</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000000225</span>
      <span itemprop="name">synapse</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000946</span>
      <span itemprop="name">synaptic effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0.000</span>
      <span itemprop="sections" repeat>description</span>
      <span itemprop="count">1</span>
    </li>
  </ul>

  <section>
    <h2>Images</h2>
    <ul>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/11/fa/ed/ef75eeb80f28db/US20230368518A1-20231116-D00000.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/a6/3d/17/735cd5407e6818/US20230368518A1-20231116-D00000.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/b8/d3/d0/c5366bb9c03352/US20230368518A1-20231116-D00001.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/76/8c/c8/eacb9ec047af45/US20230368518A1-20231116-D00001.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="1">
            <meta itemprop="label" content="object detection system">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="867">
              <meta itemprop="top" content="473">
              <meta itemprop="right" content="888">
              <meta itemprop="bottom" content="514">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="100">
            <meta itemprop="label" content="edge device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="18">
              <meta itemprop="top" content="1106">
              <meta itemprop="right" content="113">
              <meta itemprop="bottom" content="1142">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="1">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="processor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1675">
              <meta itemprop="top" content="902">
              <meta itemprop="right" content="1756">
              <meta itemprop="bottom" content="937">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/46/a1/19/2aed9c24f310cf/US20230368518A1-20231116-D00002.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/3a/1c/2e/12285400f7be4b/US20230368518A1-20231116-D00002.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="100">
            <meta itemprop="label" content="edge device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="518">
              <meta itemprop="top" content="1540">
              <meta itemprop="right" content="560">
              <meta itemprop="bottom" content="1613">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="11">
            <meta itemprop="label" content="first layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1207">
              <meta itemprop="top" content="1739">
              <meta itemprop="right" content="1245">
              <meta itemprop="bottom" content="1779">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="110">
            <meta itemprop="label" content="extractor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="696">
              <meta itemprop="top" content="1669">
              <meta itemprop="right" content="733">
              <meta itemprop="bottom" content="1748">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="processor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="505">
              <meta itemprop="top" content="431">
              <meta itemprop="right" content="549">
              <meta itemprop="bottom" content="510">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="210">
            <meta itemprop="label" content="extractor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="696">
              <meta itemprop="top" content="825">
              <meta itemprop="right" content="734">
              <meta itemprop="bottom" content="908">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="22">
            <meta itemprop="label" content="layer">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="741">
              <meta itemprop="top" content="678">
              <meta itemprop="right" content="776">
              <meta itemprop="bottom" content="731">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="2">
            <meta itemprop="id" content="220">
            <meta itemprop="label" content="reconstructor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="683">
              <meta itemprop="top" content="213">
              <meta itemprop="right" content="721">
              <meta itemprop="bottom" content="295">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/66/66/3f/8ac719c710d19b/US20230368518A1-20231116-D00003.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/50/ca/63/9f79d737008ba7/US20230368518A1-20231116-D00003.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/92/f1/b0/9f671ff1334ce1/US20230368518A1-20231116-D00004.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/63/14/15/7fdce30580f780/US20230368518A1-20231116-D00004.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/db/fe/1b/a7eff0545d099f/US20230368518A1-20231116-D00005.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/81/ba/66/de865e3ac5acb6/US20230368518A1-20231116-D00005.png">
        <ul>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/0d/8e/15/525b0d6165c8db/US20230368518A1-20231116-D00006.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/c5/9c/6b/09e85d8f758485/US20230368518A1-20231116-D00006.png">
        <ul>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="100">
            <meta itemprop="label" content="edge device">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="364">
              <meta itemprop="top" content="655">
              <meta itemprop="right" content="438">
              <meta itemprop="bottom" content="698">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="111">
            <meta itemprop="label" content="extractors">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="236">
              <meta itemprop="top" content="900">
              <meta itemprop="right" content="303">
              <meta itemprop="bottom" content="937">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="200">
            <meta itemprop="label" content="processor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1180">
              <meta itemprop="top" content="654">
              <meta itemprop="right" content="1259">
              <meta itemprop="bottom" content="697">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="211">
            <meta itemprop="label" content="extractors">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="817">
              <meta itemprop="top" content="899">
              <meta itemprop="right" content="892">
              <meta itemprop="bottom" content="937">
            </span>
          </li>
          <li itemprop="callouts" itemscope repeat>
            <meta itemprop="figurePage" content="6">
            <meta itemprop="id" content="221">
            <meta itemprop="label" content="reconstructor">
            <span itemprop="bounds" itemscope>
              <meta itemprop="left" content="1458">
              <meta itemprop="top" content="792">
              <meta itemprop="right" content="1532">
              <meta itemprop="bottom" content="827">
            </span>
          </li>
        </ul>
      </li>
      <li itemprop="images" itemscope repeat>
        <img itemprop="thumbnail" src="https://patentimages.storage.googleapis.com/40/71/0b/00f108e38a2922/US20230368518A1-20231116-D00007.png">
        <meta itemprop="full" content="https://patentimages.storage.googleapis.com/4b/ed/c8/15cd5bc4637e9a/US20230368518A1-20231116-D00007.png">
        <ul>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>Classifications</h2>
    <ul>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/94</span>&mdash;<span itemprop="Description">Hardware or software architectures specially adapted for image or video understanding</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="FirstCode" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F</span>&mdash;<span itemprop="Description">ELECTRIC DIGITAL DATA PROCESSING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F9/00</span>&mdash;<span itemprop="Description">Arrangements for program control, e.g. control units</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F9/06</span>&mdash;<span itemprop="Description">Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F9/46</span>&mdash;<span itemprop="Description">Multiprogramming arrangements</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F9/50</span>&mdash;<span itemprop="Description">Allocation of resources, e.g. of the central processing unit [CPU]</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F9/5061</span>&mdash;<span itemprop="Description">Partitioning or combining of resources</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06F9/5072</span>&mdash;<span itemprop="Description">Grid computing</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/00</span>&mdash;<span itemprop="Description">Computing arrangements based on biological models</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/02</span>&mdash;<span itemprop="Description">Neural networks</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/04</span>&mdash;<span itemprop="Description">Architecture, e.g. interconnection topology</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/045</span>&mdash;<span itemprop="Description">Combinations of networks</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/00</span>&mdash;<span itemprop="Description">Computing arrangements based on biological models</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/02</span>&mdash;<span itemprop="Description">Neural networks</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N3/08</span>&mdash;<span itemprop="Description">Learning methods</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N</span>&mdash;<span itemprop="Description">COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N5/00</span>&mdash;<span itemprop="Description">Computing arrangements using knowledge-based models</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06N5/04</span>&mdash;<span itemprop="Description">Inference or reasoning models</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/40</span>&mdash;<span itemprop="Description">Extraction of image or video features</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/44</span>&mdash;<span itemprop="Description">Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/70</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding using pattern recognition or machine learning</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/77</span>&mdash;<span itemprop="Description">Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/7715</span>&mdash;<span itemprop="Description">Feature extraction, e.g. by transforming the feature space, e.g. multi-dimensional scaling [MDS]; Mappings, e.g. subspace methods</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/70</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding using pattern recognition or machine learning</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/77</span>&mdash;<span itemprop="Description">Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/774</span>&mdash;<span itemprop="Description">Generating sets of training patterns; Bootstrap methods, e.g. bagging or boosting</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/70</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding using pattern recognition or machine learning</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/82</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
      <li>
        <ul itemprop="classifications" itemscope repeat>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06</span>&mdash;<span itemprop="Description">COMPUTING; CALCULATING OR COUNTING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V</span>&mdash;<span itemprop="Description">IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/00</span>&mdash;<span itemprop="Description">Arrangements for image or video recognition or understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/94</span>&mdash;<span itemprop="Description">Hardware or software architectures specially adapted for image or video understanding</span>
            <meta itemprop="IsCPC" content="true">
          </li>
          <li itemprop="classifications" itemscope repeat>
            <span itemprop="Code">G06V10/95</span>&mdash;<span itemprop="Description">Hardware or software architectures specially adapted for image or video understanding structured as a network, e.g. client-server architectures</span>
            <meta itemprop="Leaf" content="true"><meta itemprop="IsCPC" content="true">
          </li>
        </ul>
      </li>
    </ul>
  </section>

  

  

  <section>
    <h2>Definitions</h2>
    <ul>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the present disclosure</span>
        <span itemprop="definition">relates to a deep neural network-based object detection device, system, and method, and more particularly, to a deep neural network-based object detection device, system, and method capable of rapidly detecting an object in an image in real time.</span>
        <meta itemprop="num_attr" content="0001">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">IITP</span>
        <span itemprop="definition">Institute of Information &amp; communications Technology Planning &amp; Evaluation</span>
        <meta itemprop="num_attr" content="0001">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DNNs</span>
        <span itemprop="definition">deep neural networks</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">edge devices</span>
        <span itemprop="definition">Due to resource constraints, it is still difficult for edge devices to acquire images in real time and perform real-time inference of DNN models with complex deep neural networks.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">images obtained by an edge device</span>
        <span itemprop="definition">can be transmitted to a cloud sever equipped with a high-performance GPU for processing.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a cloud sever</span>
        <span itemprop="definition">equipped with a high-performance GPU for processing.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an edge-cloud split inference technique</span>
        <span itemprop="definition">in which an edge device uses a part of a deep neural network, and features smaller than the original image are transmitted to a cloud sever for processing.</span>
        <meta itemprop="num_attr" content="0002">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">object detection</span>
        <span itemprop="definition">is not performed using only the last feature of a feature-extracting layer, but predetermined intermediate features among a plurality of feature-extracting layers are also required.</span>
        <meta itemprop="num_attr" content="0005">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the present disclosure</span>
        <span itemprop="definition">provides a deep neural network-based object detection device, system, and method capable of rapidly detecting an object of an image in real time.</span>
        <meta itemprop="num_attr" content="0007">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a cloud sever</span>
        <span itemprop="definition">for detecting an object of an image based on a single feature received from an edge device</span>
        <meta itemprop="num_attr" content="0010">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever</span>
        <span itemprop="definition">may comprise: a memory configured to store information on a pre-trained extractor of the cloud sever having a first layer and a second layer and a pre-trained reconstructor of the cloud sever; and a processor, wherein the processor is configured to: receive a single feature of an image from the edge device, wherein the edge device includes an extractor having a plurality of layers and the single feature of the image is extracted by a first layer of the extractor of the edge device corresponding to a predetermined split point among the plurality of layers of the extractor of the edge device; and obtain an output feature of the extractor of the cloud sever by inputting the single feature received from the first layer included in the extractor of the edge device to the second layer included in the extractor of the cloud sever, and detect an object in the image by inputting the output feature obtained from the extractor of</span>
        <meta itemprop="num_attr" content="0010">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor of the edge device or the extractor of the cloud sever</span>
        <span itemprop="definition">may be an artificial neural network trained by receiving training images and object information detected from the training images, and the extractor of the edge device or the extractor of the cloud sever may be trained to output a single feature corresponding to each of the training images.</span>
        <meta itemprop="num_attr" content="0011">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructor of the cloud sever</span>
        <span itemprop="definition">may be an artificial neural network trained by receiving training images and object information detected from the training images, and the reconstructor of the cloud sever may be trained to output a plurality of features corresponding to the single feature output from the extractor of the edge device or the extractor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0012">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each layer of the reconstructor of the cloud sever</span>
        <span itemprop="definition">may have an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the edge device or the extractor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0013">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever</span>
        <span itemprop="definition">may be less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0014">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever</span>
        <span itemprop="definition">may be less than a number of the plurality of layers included in the reconstructor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0015">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an edge device</span>
        <span itemprop="definition">for transmitting a single feature obtained from an image to a cloud sever</span>
        <meta itemprop="num_attr" content="0016">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever</span>
        <span itemprop="definition">may comprise: a memory configured to store information on a pre-trained extractor of the edge device having a plurality of layers; and a processor, wherein the processor is configured to: input the image to the pre-trained extractor of the edge device, extract the single feature of the image from a first layer corresponding to a predetermined split point among the plurality of layers included in the extractor of the edge device, and transmit the extracted single feature to the cloud sever.</span>
        <meta itemprop="num_attr" content="0016">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever</span>
        <span itemprop="definition">may include a memory configured to store information on an extractor of the cloud sever including same structure as the extractor of the edge device and a reconstructor of the cloud sever, and a processor.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor of the cloud sever</span>
        <span itemprop="definition">may be configured to: receive the single feature from the edge device, input the single feature received from the extractor of the edge device to a portion of the extractor of the cloud sever, obtain an output feature ouput from the extractor of the cloud sever, input the output feature to the reconstructor of the cloud sever, and obtain a plurality of intermediate features used to detect an object in the image ouput from the reconstructor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0017">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor of the edge device</span>
        <span itemprop="definition">may be configured to analyze at least one of a network resource between the edge device and the cloud sever, a computing resource of the edge device, and a computing resource of the cloud sever, and determine a location of the split point with respect to the first layer based on an analysis result.</span>
        <meta itemprop="num_attr" content="0018">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor of the cloud sever and the reconstructor of the cloud sever</span>
        <span itemprop="definition">may include a plurality of layers, and each layer of the reconstructor of the cloud sever may have an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0019">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor of the cloud sever and the reconstructor of the cloud sever</span>
        <span itemprop="definition">may include a plurality of layers, and a number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever may be less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0020">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor of the cloud sever and the reconstructor of the cloud sever</span>
        <span itemprop="definition">may include a plurality of layers, and a number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever may be less than a number of the plurality of layers included in the reconstructor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0021">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a deep neural network-based object detection method</span>
        <span itemprop="definition">may comprise: inputting, by a processor included in an edge device, an acquired image to a pre-trained extractor of the edge device; extracting, by the processor included in the edge device, a single feature corresponding to the image from a first layer of the extractor corresponding to a predetermined split point among a plurality of layers included in the extractor of the edge device; transmitting, by the processor included in the edge device, the extracted single feature to a cloud sever; receiving, by a processor included in the cloud sever, the single feature from the edge device, wherein the cloud sever includes an extractor of the cloud sever having a first layer and a second layer and a reconstructor; obtaining, by the processor included in the cloud sever, an output feature of the extractor of the cloud sever by inputting the single feature to the second layer included in the extractor of the cloud sever; and obtaining, by the processor included in the cloud</span>
        <meta itemprop="num_attr" content="0022">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the method</span>
        <span itemprop="definition">may comprise: analyzing, by the processor included in the edge device, at least one of a network resource between the edge device and the cloud sever, a computing resource of the edge device, and a computing resource of the cloud sever; and determining a location of the split point with respect to the first layer included in the extractor of the edge device based on an analysis result.</span>
        <meta itemprop="num_attr" content="0023">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor of the edge device or the extractor of the cloud sever</span>
        <span itemprop="definition">may be an artificial neural network trained by receiving training images and object information detected from the training images, and the extractor of the edge device or the extractor of the cloud sever may be trained to output a single feature corresponding to each of the training images.</span>
        <meta itemprop="num_attr" content="0024">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructor of the cloud sever</span>
        <span itemprop="definition">may be an artificial neural network trained by receiving training images and object information detected from the training images, and the reconstructor of the cloud sever may be trained to output a plurality of features corresponding to the single feature output from the extractor of the edge device or the extractor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0025">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each layer of the reconstructor of the cloud sever</span>
        <span itemprop="definition">may have an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the edge device or the extractor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0026">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever</span>
        <span itemprop="definition">may be less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0027">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever</span>
        <span itemprop="definition">may be less than a number of the plurality of layers included in the reconstructor of the cloud sever.</span>
        <meta itemprop="num_attr" content="0028">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an object in an image</span>
        <span itemprop="definition">can be rapidly detected in real time.</span>
        <meta itemprop="num_attr" content="0029">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">is a block diagram showing a configuration of a deep neural network-based object detection system according to an embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0031">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">is a conceptual diagram of an edge device and a cloud sever according to an embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0032">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 3 A to 3 C</span>
        <span itemprop="definition">are diagrams for describing differences between prior arts and an embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0033">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">is a conceptual diagram for describing a deep neural network-based object detection system according to another embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0034">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a flowchart illustrating a deep neural network-based object detection method according to an embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0035">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a term such as a unit or a portion used in the specification</span>
        <span itemprop="definition">means a software component or a hardware component such as FPGA or ASIC, and the unit or the portion performs a certain role.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the unit or the portion</span>
        <span itemprop="definition">is not limited to software or hardware.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the portion or the unit</span>
        <span itemprop="definition">may be configured to be in an addressable storage medium, or may be configured to reproduce one or more processors.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the unit or the portion</span>
        <span itemprop="definition">includes components (such as software components, object-oriented software components, class components, and task components), processes, functions, properties, procedures, subroutines, segments of program code, drivers, firmware, microcode, circuits, data, database, data structures, tables, arrays, and variables.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the functions provided in the components and unit</span>
        <span itemprop="definition">may be combined into a smaller number of components and units or may be further divided into additional components and units.</span>
        <meta itemprop="num_attr" content="0040">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 1</span>
        <span itemprop="definition">is a block diagram showing a configuration of a deep neural network-based object detection system according to an embodiment of the present disclosure</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 2</span>
        <span itemprop="definition">is a conceptual diagram for describing operations of an edge device and a cloud sever according to an embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0042">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a deep neural network-based object detection system 1</span>
        <span itemprop="definition">includes at least one edge device 100 that executes edge computing and a cloud sever 200 that receives data from the edge device 100 , executes cloud computing, and transmits the execution result to the edge.</span>
        <meta itemprop="num_attr" content="0043">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">may correspond to a smart device, a drone, or a wearable device, but is not limited thereto.</span>
        <meta itemprop="num_attr" content="0044">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">may include a camera that acquires images, a memory that stores information on a pre-trained extractor, and a processor that controls components of the edge device 100 .</span>
        <meta itemprop="num_attr" content="0045">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">can collect data, refine data, perform preprocessing such as sampling, cleaning, and combining data, and transmit preprocessing results to the cloud sever 200 .</span>
        <meta itemprop="num_attr" content="0046">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the functions of the edge device 100</span>
        <span itemprop="definition">can be designed in various manners.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the functions</span>
        <span itemprop="definition">may be designed such that the edge device 100 processes data without sending the data to the cloud sever 200 , but in the present disclosure, description will focus on an embodiment in which the edge device 100 and the cloud sever 200 perform data processing in a split manner.</span>
        <meta itemprop="num_attr" content="0047">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">may input an image acquired through a camera to a pre-trained extractor 110 and extract a single feature through a plurality of layers included in the extractor 110 .</span>
        <meta itemprop="num_attr" content="0048">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor</span>
        <span itemprop="definition">may be an artificial neural network that is trained by receiving training images and object information detected from the training images and is trained to output a single feature when an image is input at the time of inference.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor</span>
        <span itemprop="definition">may correspond to an encoder of an auto encoder.</span>
        <meta itemprop="num_attr" content="0049">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">inputs the acquired image to the pre-trained extractor 110 , and may extract a single feature related to the image as an output of a first layer corresponding to a predetermined split point among a plurality of layers included in the extractor 110 and transmit the single feature to the cloud sever 200 .</span>
        <meta itemprop="num_attr" content="0050">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">may analyze at least one of a network resource between the edge device 100 and the cloud sever 200 , a computing resource of the edge device 100 , and a computing resource of the cloud sever 200 and determine the location of the split point according to the analysis result.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a layer corresponding to the split point</span>
        <span itemprop="definition">may be referred to as a first layer.</span>
        <meta itemprop="num_attr" content="0051">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever 200</span>
        <span itemprop="definition">may perform core processing, such as comprehensively performing tasks received from the edge device 100 or distributing some tasks to a specific edge device 100 .</span>
        <meta itemprop="num_attr" content="0052">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Results processed in the cloud sever 200</span>
        <span itemprop="definition">may be delivered to the edge device 100 .</span>
        <meta itemprop="num_attr" content="0053">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever 200</span>
        <span itemprop="definition">may detect an object of an image based on a single feature received from the edge device 100 .</span>
        <meta itemprop="num_attr" content="0054">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever 200</span>
        <span itemprop="definition">may include a pre-trained extractor 210 , a memory that stores information of a pre-trained reconstructor 220 , and a processor that controls components of the cloud sever 200 .</span>
        <meta itemprop="num_attr" content="0055">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever 200</span>
        <span itemprop="definition">may obtain an output feature of the extractor 210 by inputting a single feature received from the edge device 100 to a second layer immediately after a first layer among a plurality of layers of the extractor 210 . Subsequently, the processor 200 may obtain a plurality of predetermined intermediate features required for object detection from the reconstructor 220 by inputting the output feature of the extractor 210 to the reconstructor 220 .</span>
        <meta itemprop="num_attr" content="0056">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor 210</span>
        <span itemprop="definition">may be an artificial neural network pre-trained to output an output feature when receiving training images and object information detected from the training images.</span>
        <meta itemprop="num_attr" content="0057">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor 210 included in the cloud sever 200</span>
        <span itemprop="definition">is the same as the extractor 110 included in the edge device 100 .</span>
        <meta itemprop="num_attr" content="0058">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructor 220</span>
        <span itemprop="definition">may be an artificial neural network pre-trained to output a reconstructed feature when receiving training images and object information detected from the training images.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the reconstructor 220</span>
        <span itemprop="definition">may correspond to a decoder of an auto encoder.</span>
        <meta itemprop="num_attr" content="0059">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Each layer constituting the reconstructor 220</span>
        <span itemprop="definition">is designed to have an inverse operation relationship with the layers of the extractor 210 to reconstruct predetermined intermediate layer features of the extractor 210 .</span>
        <meta itemprop="num_attr" content="0060">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">transmits a low-level feature that has progressed to the corresponding layer to the cloud sever 200 .</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever 200</span>
        <span itemprop="definition">inputs the received feature to the layer immediately after the split point of the extractor 210 to obtain a high-level output feature and passes the obtained output feature through the reconstructor 220 .</span>
        <meta itemprop="num_attr" content="0061">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever 200</span>
        <span itemprop="definition">uses multi-level intermediate features obtained by inputting the output features to the reconstructor 220 for object detection later. Therefore, according to the present disclosure, even if the edge device 100 transmits a single feature to the cloud sever 200 , intermediate features can be obtained and thus normal object detection can be performed.</span>
        <meta itemprop="num_attr" content="0062">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">layers</span>
        <span itemprop="definition">may output features of different sizes, and thus the cloud sever 200 may determine the second layer to which a received single feature will be input from among the plurality of layers included in the extractor 210 using the received single feature.</span>
        <meta itemprop="num_attr" content="0063">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIGS. 3 A to 3 C</span>
        <span itemprop="definition">are diagrams for describing differences between prior arts and an embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0064">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a prior art</span>
        <span itemprop="definition">is a case in which an edge device does not include an extractor and a cloud sever includes an extractor.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device</span>
        <span itemprop="definition">transmits acquired image to the cloud sever, and the cloud sever inputs the image to the extractor and obtains a plurality of predetermined intermediate features to be used for object detection from the extractor. That is, the edge device only serves to acquire an image, and the cloud sever detects an object in the image based on an artificial intelligence model.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device</span>
        <span itemprop="definition">transmits an image to the cloud sever, there may be a problem in that it takes a long to transmit the image.</span>
        <meta itemprop="num_attr" content="0065">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3 B</span>
        <span itemprop="definition">describes a case in which only an edge device includes an extractor.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever</span>
        <span itemprop="definition">detects an object in an image based on the plurality of intermediate features. In this case, the performance of the edge device is limited and the problem related to image transmission time cannot be solved.</span>
        <meta itemprop="num_attr" content="0066">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 3 C</span>
        <span itemprop="definition">describes a case in which an edge device and a cloud sever include the same extractor.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor of the cloud sever</span>
        <span itemprop="definition">obtains a plurality of predetermined intermediate features by inputting the single feature to the layer immediately after the intermediate layer, and detects an object in an image based on the plurality of intermediate features.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">a delay time gain</span>
        <span itemprop="definition">is limited because split layers are defined before the frontmost feature among a plurality of features is obtained.</span>
        <meta itemprop="num_attr" content="0067">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">includes the extractor 110</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever 200</span>
        <span itemprop="definition">includes the extractor 210 and the reconstructor 220 .</span>
        <meta itemprop="num_attr" content="0068">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device 100</span>
        <span itemprop="definition">analyzes a network speed between the edge device 100 and the processor 200 and at least one of the performances of the edge device 100 and the processor 200 , determines the location of a split point according to the analysis result, and transmits a single feature output from the first layer 11 corresponding to the split point to the cloud sever 200 .</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever 200</span>
        <span itemprop="definition">may obtain a high-level feature by inputting the single feature to a layer 22 of the extractor 210 immediately after the first layer 11 from which the single feature transmitted from the edge device 100 is output. Subsequently, the cloud sever 200 may input the obtained feature to the reconstructor 220 to obtain a plurality of predetermined intermediate features required for object detection from the reconstructor 220 .</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">each layer constituting the reconstructor 220</span>
        <span itemprop="definition">has an inverse operation relationship with the layers of the extractor 210 to reconstruct intermediate features corresponding to a predetermined intermediate layer of the extractor 210 .</span>
        <meta itemprop="num_attr" content="0069">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 4</span>
        <span itemprop="definition">is a conceptual diagram for describing a deep neural network-based object detection system according to another embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0070">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the deep neural network-based object detection system 1 described with reference to FIGS. 1 and 2</span>
        <span itemprop="definition">can reduce the amount of transmission the edge device 100 through the reconstructor 220 of the cloud sever 200 , but a problem with respect to transmission time in a wireless network situation cannot be solved. Further, in the edge device 100 having limited resources, a significant delay time may be taken to execute the extractor 110 .</span>
        <meta itemprop="num_attr" content="0071">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the number of channels included in each layer of extractors 111 and 211</span>
        <span itemprop="definition">may be less than the number of channels included in each layer of the extractor 210 shown in FIG. 2 .</span>
        <meta itemprop="num_attr" content="0072">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">An embodiment</span>
        <span itemprop="definition">may be an asymmetric structure in which the number of channels of at least one of a plurality of layers included in the extractors 111 and 211 is less than the number of channels of at least one layer corresponding to at least one of a plurality of layers included in a reconstructor 221 .</span>
        <meta itemprop="num_attr" content="0074">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an embodiment</span>
        <span itemprop="definition">may be an asymmetric structure in which the number of layers included in the extractors 111 and 211 may be different from the number of layers included in the reconstructor 221 . In this case, the number of layers included in the extractors 111 and 211 may be less than the number of layers included in the reconstructor 221 .</span>
        <meta itemprop="num_attr" content="0075">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the extractor 111</span>
        <span itemprop="definition">is executed in the edge device 100 with lower performance, whereas the reconstructor 221 is executed in the cloud sever 200 with better performance That is, since the extractor 111 is executed in the edge device 100 , the execution time is long (that is, delay time is long). Accordingly, the number of layers included in the extractor 111 is less than the number of layers included in the reconstructor 221 , and thus the delay time can be reduced.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the number of channels of at least one of the plurality of layers included in the extractor 111</span>
        <span itemprop="definition">is less than the number of channels of at least one layer corresponding to at least one of the plurality of layers included in the reconstructor 221 , and thus the delay time can be reduced and the amount of data transmitted from the edge device 100 to the cloud sever 200 can be decreased.</span>
        <meta itemprop="num_attr" content="0076">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the performance</span>
        <span itemprop="definition">may deteriorate (i.e. object detection accuracy decreases).</span>
        <meta itemprop="num_attr" content="0077">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">performance deterioration</span>
        <span itemprop="definition">can be prevented by increasing the number of layers included in the reconstructor 221 and/or the number of channels included in at least one of the plurality of layers included in the reconstructor 221 .</span>
        <meta itemprop="num_attr" content="0078">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FIG. 5</span>
        <span itemprop="definition">is a flowchart illustrating a deep neural network-based object detection method according to an embodiment of the present disclosure.</span>
        <meta itemprop="num_attr" content="0079">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the deep neural network-based object detection method according to the present embodiment</span>
        <span itemprop="definition">may be performed in substantially the same configuration as the system 1 of FIG. 1 . Accordingly, components identical to those of the system 1 of FIG. 1 are denoted by the same reference numerals, and redundant descriptions are omitted.</span>
        <meta itemprop="num_attr" content="0080">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an edge device</span>
        <span itemprop="definition">inputs an acquired image to a pre-trained extractor (S 110 ).</span>
        <meta itemprop="num_attr" content="0081">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device</span>
        <span itemprop="definition">extracts a single feature of the image as an output of a first layer corresponding to a predetermined split point among a plurality of layers included in the extractor (S 120 ).</span>
        <meta itemprop="num_attr" content="0082">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the edge device</span>
        <span itemprop="definition">transmits the extracted single feature to the cloud sever (S 130 ).</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever</span>
        <span itemprop="definition">including the same extractor as that of the edge device and a reconstructor obtains an output feature of the extractor by inputting the single feature to a second layer immediately after the first layer (S 140 ).</span>
        <meta itemprop="num_attr" content="0083">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever</span>
        <span itemprop="definition">obtains a plurality of intermediate features to be used to detect an object in the image by inputting the output features to the reconstructor (S 150 ).</span>
        <meta itemprop="num_attr" content="0084">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the cloud sever</span>
        <span itemprop="definition">may detect the object in the image based on the plurality of intermediate features (S 160 ).</span>
        <meta itemprop="num_attr" content="0085">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the deep neural network-based object detection system described above</span>
        <span itemprop="definition">may be implemented by a computing device including at least some of a processor, a memory, a user input device, and a presentation device.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the memory</span>
        <span itemprop="definition">is a medium that stores computer-readable software coded such that a specific task can be performed when executed by the processor, applications, program modules, routines, instructions, and/or data.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the processor</span>
        <span itemprop="definition">may read and execute the computer-readable software, applications, program modules, routines, instructions, and/or data stored in the memory.</span>
        <meta itemprop="num_attr" content="0086">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">may include a variety of devices such as smartphones, tablets, laptops, desktops, servers, and clients.</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computing device</span>
        <span itemprop="definition">may be a single stand-alone device or may include multiple computing devices operating in a distributed environment and cooperating with each other over a communications network.</span>
        <meta itemprop="num_attr" content="0087">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the above-described deep neural network-based object detection method</span>
        <span itemprop="definition">may be executed by a computing device including a processor and a memory that stores computer-readable software coded to perform an object detection method using an artificial intelligence model when executed by the processor, applications, program modules, routines, instructions, and/or data structures.</span>
        <meta itemprop="num_attr" content="0088">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">present embodiments described above</span>
        <span itemprop="definition">may be implemented through various means.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the present embodiments</span>
        <span itemprop="definition">may be implemented by hardware, firmware, software, or a combination thereof.</span>
        <meta itemprop="num_attr" content="0089">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an image diagnosis method using an artificial intelligence model</span>
        <span itemprop="definition">may be implemented by one or more application specific integrated circuits (ASICs), digital signal processors (DSPs), digital signal processing devices (DSPDs), programmable logic devices (PLDs), field programmable gate arrays (FPGAs), processors, controllers, microcontrollers, or microprocessors.</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">ASICs</span>
        <span itemprop="definition">application specific integrated circuits</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DSPs</span>
        <span itemprop="definition">digital signal processors</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">DSPDs</span>
        <span itemprop="definition">digital signal processing devices</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">PLDs</span>
        <span itemprop="definition">programmable logic devices</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">FPGAs</span>
        <span itemprop="definition">field programmable gate arrays</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">processors</span>
        <span itemprop="definition">controllers, microcontrollers, or microprocessors.</span>
        <meta itemprop="num_attr" content="0090">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">an object detection method</span>
        <span itemprop="definition">may be implemented using an artificial intelligence semiconductor device in which neurons and synapses of a deep neural network are implemented as semiconductor devices.</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the semiconductor devices</span>
        <span itemprop="definition">may be currently used semiconductor devices such as SRAMs, DRAMs, and NANDs, next-generation semiconductor devices, RRAMs, STT MRAMs, PRAMs, or a combination thereof.</span>
        <meta itemprop="num_attr" content="0091">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">results (weights) of learning an auto-encoder as software</span>
        <span itemprop="definition">may be transferred to synaptic mimic devices arranged in an array, or learning may be performed in the artificial intelligence semiconductor device.</span>
        <meta itemprop="num_attr" content="0092">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">Combinations of steps in each flowchart attached to the present disclosure</span>
        <span itemprop="definition">may be executed by computer program instructions. Since the computer program instructions can be mounted on a processor of a general-purpose computer, a special purpose computer, or other programmable data processing equipment, the instructions executed by the processor of the computer or other programmable data processing equipment create a means for performing the functions described in each step of the flowchart.</span>
        <meta itemprop="num_attr" content="0093">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computer program instructions</span>
        <span itemprop="definition">can also be stored on a computer-usable or computer-readable storage medium which can be directed to a computer or other programmable data processing equipment to implement a function in a specific manner. Accordingly, the instructions stored on the computer-usable or computer-readable recording medium can also produce an article of manufacture containing an instruction means which performs the functions described in each step of the flowchart.</span>
        <meta itemprop="num_attr" content="0093">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the computer program instructions</span>
        <span itemprop="definition">can also be mounted on a computer or other programmable data processing equipment. Accordingly, a series of operational steps are performed on a computer or other programmable data processing equipment to create a computer-executable process, and it is also possible for instructions to perform a computer or other programmable data processing equipment to provide steps for performing the functions described in each step of the flowchart.</span>
        <meta itemprop="num_attr" content="0093">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">each step</span>
        <span itemprop="definition">may represent a module, a segment, or a portion of codes which contains one or more executable instructions for executing the specified logical function(s).</span>
        <meta itemprop="num_attr" content="0094">
      </li>
      <li itemprop="definitions" itemscope repeat>
        <span itemprop="subject">the functions mentioned in the steps</span>
        <span itemprop="definition">may occur out of order. For example, two steps illustrated in succession may in fact be performed substantially simultaneously, or the steps may sometimes be performed in a reverse order depending on the corresponding function.</span>
        <meta itemprop="num_attr" content="0094">
      </li>
    </ul>
  </section>

  


  <section itemprop="abstract" itemscope>
    <h2>Abstract</h2>
    
    <div itemprop="content" html><abstract mxw-id="PA641683129" lang="EN" load-source="patent-office">
    <div id="p-0001" num="0000" class="abstract">The present disclosure relates to a deep neural network-based object detection device, system, and method, and more particularly, to a deep neural network-based object detection device, system, and method capable of rapidly detecting an object in an image in real time.</div>
  </abstract>
  </div>
  </section>

  <section itemprop="description" itemscope>
    <h2>Description</h2>
    
    <div itemprop="content" html><ul mxw-id="PDES439033742" lang="EN" load-source="patent-office" class="description">
    
    <heading id="h-0001">TECHNICAL FIELD</heading>
    <li> <para-num num="[0001]"> </para-num> <div id="p-0002" num="0001" class="description-line">The present disclosure relates to a deep neural network-based object detection device, system, and method, and more particularly, to a deep neural network-based object detection device, system, and method capable of rapidly detecting an object in an image in real time. This work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by Korea government (MSIT) ([Project unique No.: 1711126132; Project No.: 2019-0-00421-003; R&amp;D project: Information Communication Broadcasting Innovative Talent Development Project; and Research Project Title: Artificial Intelligence Graduate School Program], [Project unique No.: 1711139517; Project No.: 2021-0-02068-001; R&amp;D project: Information Communication Broadcasting Innovative Talent Development Project; and Research Project Title: Development for Artificial Intelligence Innovative Hub], and [Project unique No.: 1711139247; Project No.: 2021-0-02052-001; R&amp;D project: Information Communication Broadcasting Innovative Talent Development Project; and Research Project Title: Development for Artificial Intelligence System on a Chip for Smart Mobility]).</div>
    </li> <heading id="h-0002">BACKGROUND</heading>
    <li> <para-num num="[0002]"> </para-num> <div id="p-0003" num="0002" class="description-line">With the recent development of deep neural networks (DNNs), excellent performance has been implemented in various tasks related to computer vision. However, due to resource constraints, it is still difficult for edge devices to acquire images in real time and perform real-time inference of DNN models with complex deep neural networks. In this case, images obtained by an edge device can be transmitted to a cloud sever equipped with a high-performance GPU for processing. However, due to the characteristics of an edge device that needs to use a wireless network, it may take a long time to transmit images. In order to solve this problem, it is possible to use an edge-cloud split inference technique in which an edge device uses a part of a deep neural network, and features smaller than the original image are transmitted to a cloud sever for processing.</div>
    </li> <li> <para-num num="[0003]"> </para-num> <div id="p-0004" num="0003" class="description-line">In general, since the size of a feature decreases as the number of layers for extracting a feature increases, an appropriate split point needs to be present for each network.</div>
    </li> <li> <para-num num="[0004]"> </para-num> <div id="p-0005" num="0004" class="description-line">However, unlike image classifiers, general deep neural network-based object detectors need to use multiple layers of features in order to effectively detect objects of various sizes.</div>
    </li> <li> <para-num num="[0005]"> </para-num> <div id="p-0006" num="0005" class="description-line">That is, object detection is not performed using only the last feature of a feature-extracting layer, but predetermined intermediate features among a plurality of feature-extracting layers are also required.</div>
    </li> <li> <para-num num="[0006]"> </para-num> <div id="p-0007" num="0006" class="description-line">For this reason, the amount of transmission increases in a split inference situation, and thus it is difficult to find an appropriate split point in a general object detector layer.</div>
    </li> <heading id="h-0003">SUMMARY</heading>
    <li> <para-num num="[0007]"> </para-num> <div id="p-0008" num="0007" class="description-line">In view of the above, the present disclosure provides a deep neural network-based object detection device, system, and method capable of rapidly detecting an object of an image in real time.</div>
    </li> <li> <para-num num="[0008]"> </para-num> <div id="p-0009" num="0008" class="description-line">Objects of the present disclosure are not limited to the aforementioned object, and other objects not mentioned will be clearly understood by those skilled in the art from the description below.</div>
    </li> <li> <para-num num="[0009]"> </para-num> <div id="p-0010" num="0009" class="description-line">The aspects of the present disclosure are not limited to the foregoing, and other aspects not mentioned herein will be clearly understood by those skilled in the art from the following description.</div>
    </li> <li> <para-num num="[0010]"> </para-num> <div id="p-0011" num="0010" class="description-line">In accordance with an aspect of the present disclosure, there is provided a cloud sever for detecting an object of an image based on a single feature received from an edge device, the cloud sever may comprise: a memory configured to store information on a pre-trained extractor of the cloud sever having a first layer and a second layer and a pre-trained reconstructor of the cloud sever; and a processor, wherein the processor is configured to: receive a single feature of an image from the edge device, wherein the edge device includes an extractor having a plurality of layers and the single feature of the image is extracted by a first layer of the extractor of the edge device corresponding to a predetermined split point among the plurality of layers of the extractor of the edge device; and obtain an output feature of the extractor of the cloud sever by inputting the single feature received from the first layer included in the extractor of the edge device to the second layer included in the extractor of the cloud sever, and detect an object in the image by inputting the output feature obtained from the extractor of the cloud sever to the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0011]"> </para-num> <div id="p-0012" num="0011" class="description-line">The extractor of the edge device or the extractor of the cloud sever may be an artificial neural network trained by receiving training images and object information detected from the training images, and the extractor of the edge device or the extractor of the cloud sever may be trained to output a single feature corresponding to each of the training images.</div>
    </li> <li> <para-num num="[0012]"> </para-num> <div id="p-0013" num="0012" class="description-line">The reconstructor of the cloud sever may be an artificial neural network trained by receiving training images and object information detected from the training images, and the reconstructor of the cloud sever may be trained to output a plurality of features corresponding to the single feature output from the extractor of the edge device or the extractor of the cloud sever.</div>
    </li> <li> <para-num num="[0013]"> </para-num> <div id="p-0014" num="0013" class="description-line">Each layer of the reconstructor of the cloud sever may have an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the edge device or the extractor of the cloud sever.</div>
    </li> <li> <para-num num="[0014]"> </para-num> <div id="p-0015" num="0014" class="description-line">A number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever may be less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0015]"> </para-num> <div id="p-0016" num="0015" class="description-line">A number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever may be less than a number of the plurality of layers included in the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0016]"> </para-num> <div id="p-0017" num="0016" class="description-line">In accordance with another aspect of the present disclosure, there is provided an edge device for transmitting a single feature obtained from an image to a cloud sever, the cloud sever may comprise: a memory configured to store information on a pre-trained extractor of the edge device having a plurality of layers; and a processor, wherein the processor is configured to: input the image to the pre-trained extractor of the edge device, extract the single feature of the image from a first layer corresponding to a predetermined split point among the plurality of layers included in the extractor of the edge device, and transmit the extracted single feature to the cloud sever.</div>
    </li> <li> <para-num num="[0017]"> </para-num> <div id="p-0018" num="0017" class="description-line">The cloud sever may include a memory configured to store information on an extractor of the cloud sever including same structure as the extractor of the edge device and a reconstructor of the cloud sever, and a processor. The processor of the cloud sever may be configured to: receive the single feature from the edge device, input the single feature received from the extractor of the edge device to a portion of the extractor of the cloud sever, obtain an output feature ouput from the extractor of the cloud sever, input the output feature to the reconstructor of the cloud sever, and obtain a plurality of intermediate features used to detect an object in the image ouput from the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0018]"> </para-num> <div id="p-0019" num="0018" class="description-line">The processor of the edge device may be configured to analyze at least one of a network resource between the edge device and the cloud sever, a computing resource of the edge device, and a computing resource of the cloud sever, and determine a location of the split point with respect to the first layer based on an analysis result.</div>
    </li> <li> <para-num num="[0019]"> </para-num> <div id="p-0020" num="0019" class="description-line">The extractor of the cloud sever and the reconstructor of the cloud sever may include a plurality of layers, and each layer of the reconstructor of the cloud sever may have an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the cloud sever.</div>
    </li> <li> <para-num num="[0020]"> </para-num> <div id="p-0021" num="0020" class="description-line">The extractor of the cloud sever and the reconstructor of the cloud sever may include a plurality of layers, and a number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever may be less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0021]"> </para-num> <div id="p-0022" num="0021" class="description-line">The extractor of the cloud sever and the reconstructor of the cloud sever may include a plurality of layers, and a number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever may be less than a number of the plurality of layers included in the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0022]"> </para-num> <div id="p-0023" num="0022" class="description-line">In accordance with another aspect of the present disclosure, there is provided a deep neural network-based object detection method, the method may comprise: inputting, by a processor included in an edge device, an acquired image to a pre-trained extractor of the edge device; extracting, by the processor included in the edge device, a single feature corresponding to the image from a first layer of the extractor corresponding to a predetermined split point among a plurality of layers included in the extractor of the edge device; transmitting, by the processor included in the edge device, the extracted single feature to a cloud sever; receiving, by a processor included in the cloud sever, the single feature from the edge device, wherein the cloud sever includes an extractor of the cloud sever having a first layer and a second layer and a reconstructor; obtaining, by the processor included in the cloud sever, an output feature of the extractor of the cloud sever by inputting the single feature to the second layer included in the extractor of the cloud sever; and obtaining, by the processor included in the cloud sever, a plurality of intermediate features used to detect an object in the image by inputting the output feature to the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0023]"> </para-num> <div id="p-0024" num="0023" class="description-line">The method may comprise: analyzing, by the processor included in the edge device, at least one of a network resource between the edge device and the cloud sever, a computing resource of the edge device, and a computing resource of the cloud sever; and determining a location of the split point with respect to the first layer included in the extractor of the edge device based on an analysis result.</div>
    </li> <li> <para-num num="[0024]"> </para-num> <div id="p-0025" num="0024" class="description-line">The extractor of the edge device or the extractor of the cloud sever may be an artificial neural network trained by receiving training images and object information detected from the training images, and the extractor of the edge device or the extractor of the cloud sever may be trained to output a single feature corresponding to each of the training images.</div>
    </li> <li> <para-num num="[0025]"> </para-num> <div id="p-0026" num="0025" class="description-line">The reconstructor of the cloud sever may be an artificial neural network trained by receiving training images and object information detected from the training images, and the reconstructor of the cloud sever may be trained to output a plurality of features corresponding to the single feature output from the extractor of the edge device or the extractor of the cloud sever.</div>
    </li> <li> <para-num num="[0026]"> </para-num> <div id="p-0027" num="0026" class="description-line">Each layer of the reconstructor of the cloud sever may have an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the edge device or the extractor of the cloud sever.</div>
    </li> <li> <para-num num="[0027]"> </para-num> <div id="p-0028" num="0027" class="description-line">A number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever may be less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0028]"> </para-num> <div id="p-0029" num="0028" class="description-line">A number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever may be less than a number of the plurality of layers included in the reconstructor of the cloud sever.</div>
    </li> <li> <para-num num="[0029]"> </para-num> <div id="p-0030" num="0029" class="description-line">According to the deep neural network-based object detection device, system, and method according to an embodiment of the present disclosure, an object in an image can be rapidly detected in real time.</div>
    </li> <li> <para-num num="[0030]"> </para-num> <div id="p-0031" num="0030" class="description-line">The effects of the present disclosure are not limited to the aforementioned effect, and other effects not mentioned will be clearly understood by those skilled in the art from the description below.</div>
    
    
    </li> <description-of-drawings>
      <heading id="h-0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <li> <para-num num="[0031]"> </para-num> <div id="p-0032" num="0031" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> is a block diagram showing a configuration of a deep neural network-based object detection system according to an embodiment of the present disclosure.</div>
      </li> <li> <para-num num="[0032]"> </para-num> <div id="p-0033" num="0032" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> is a conceptual diagram of an edge device and a cloud sever according to an embodiment of the present disclosure.</div>
      </li> <li> <para-num num="[0033]"> </para-num> <div id="p-0034" num="0033" class="description-line"> <figref idrefs="DRAWINGS">FIGS. <b>3</b>A to <b>3</b>C</figref> are diagrams for describing differences between prior arts and an embodiment of the present disclosure.</div>
      </li> <li> <para-num num="[0034]"> </para-num> <div id="p-0035" num="0034" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref> is a conceptual diagram for describing a deep neural network-based object detection system according to another embodiment of the present disclosure.</div>
      </li> <li> <para-num num="[0035]"> </para-num> <div id="p-0036" num="0035" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref> is a flowchart illustrating a deep neural network-based object detection method according to an embodiment of the present disclosure.</div>
    </li> </description-of-drawings>
    
    
    <heading id="h-0005">DETAILED DESCRIPTION</heading>
    <li> <para-num num="[0036]"> </para-num> <div id="p-0037" num="0036" class="description-line">The advantages and features of the embodiments and the methods of accomplishing the embodiments will be clearly understood from the following description taken in conjunction with the accompanying drawings. However, embodiments are not limited to those embodiments described, as embodiments may be implemented in various forms. It should be noted that the present embodiments are provided to make a full disclosure and also to allow those skilled in the art to know the full range of the embodiments. Therefore, the embodiments are to be defined only by the scope of the appended claims.</div>
    </li> <li> <para-num num="[0037]"> </para-num> <div id="p-0038" num="0037" class="description-line">Terms used in the present specification will be briefly described, and the present disclosure will be described in detail.</div>
    </li> <li> <para-num num="[0038]"> </para-num> <div id="p-0039" num="0038" class="description-line">In terms used in the present disclosure, general terms currently as widely used as possible while considering functions in the present disclosure are used. However, the terms may vary according to the intention or precedent of a technician working in the field, the emergence of new technologies, and the like. In addition, in certain cases, there are terms arbitrarily selected by the applicant, and in this case, the meaning of the terms will be described in detail in the description of the corresponding invention. Therefore, the terms used in the present disclosure should be defined based on the meaning of the terms and the overall contents of the present disclosure, not just the name of the terms.</div>
    </li> <li> <para-num num="[0039]"> </para-num> <div id="p-0040" num="0039" class="description-line">When it is described that a part in the overall specification includes a certain component, this means that other components may be further included instead of excluding other components unless specifically stated to the contrary.</div>
    </li> <li> <para-num num="[0040]"> </para-num> <div id="p-0041" num="0040" class="description-line">In addition, a term such as a unit or a portion used in the specification means a software component or a hardware component such as FPGA or ASIC, and the unit or the portion performs a certain role. However, the unit or the portion is not limited to software or hardware. The portion or the unit may be configured to be in an addressable storage medium, or may be configured to reproduce one or more processors. Thus, as an example, the unit or the portion includes components (such as software components, object-oriented software components, class components, and task components), processes, functions, properties, procedures, subroutines, segments of program code, drivers, firmware, microcode, circuits, data, database, data structures, tables, arrays, and variables. The functions provided in the components and unit may be combined into a smaller number of components and units or may be further divided into additional components and units.</div>
    </li> <li> <para-num num="[0041]"> </para-num> <div id="p-0042" num="0041" class="description-line">Hereinafter, the embodiment of the present disclosure will be described in detail with reference to the accompanying drawings so that those of ordinary skill in the art may easily implement the present disclosure. In the drawings, portions not related to the description are omitted in order to clearly describe the present disclosure.</div>
    </li> <li> <para-num num="[0042]"> </para-num> <div id="p-0043" num="0042" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> is a block diagram showing a configuration of a deep neural network-based object detection system according to an embodiment of the present disclosure, and <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref> is a conceptual diagram for describing operations of an edge device and a cloud sever according to an embodiment of the present disclosure.</div>
    </li> <li> <para-num num="[0043]"> </para-num> <div id="p-0044" num="0043" class="description-line">Referring to <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref>, a deep neural network-based <figure-callout id="1" label="object detection system" filenames="US20230368518A1-20231116-D00001.png" state="{{state}}">object detection system</figure-callout> <b>1</b> according to an embodiment of the present disclosure includes at least one <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> that executes edge computing and a cloud sever <b>200</b> that receives data from the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b>, executes cloud computing, and transmits the execution result to the edge.</div>
    </li> <li> <para-num num="[0044]"> </para-num> <div id="p-0045" num="0044" class="description-line">For example, the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> may correspond to a smart device, a drone, or a wearable device, but is not limited thereto.</div>
    </li> <li> <para-num num="[0045]"> </para-num> <div id="p-0046" num="0045" class="description-line">The <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> may include a camera that acquires images, a memory that stores information on a pre-trained extractor, and a processor that controls components of the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b>.</div>
    </li> <li> <para-num num="[0046]"> </para-num> <div id="p-0047" num="0046" class="description-line">Basically, the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> can collect data, refine data, perform preprocessing such as sampling, cleaning, and combining data, and transmit preprocessing results to the cloud sever <b>200</b>.</div>
    </li> <li> <para-num num="[0047]"> </para-num> <div id="p-0048" num="0047" class="description-line">The functions of the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> can be designed in various manners. For example, the functions may be designed such that the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> processes data without sending the data to the cloud sever <b>200</b>, but in the present disclosure, description will focus on an embodiment in which the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> and the cloud sever <b>200</b> perform data processing in a split manner.</div>
    </li> <li> <para-num num="[0048]"> </para-num> <div id="p-0049" num="0048" class="description-line">In an embodiment, the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> may input an image acquired through a camera to a <figure-callout id="110" label="pre-trained extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">pre-trained extractor</figure-callout> <b>110</b> and extract a single feature through a plurality of layers included in the <figure-callout id="110" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>110</b>.</div>
    </li> <li> <para-num num="[0049]"> </para-num> <div id="p-0050" num="0049" class="description-line">Here, the extractor may be an artificial neural network that is trained by receiving training images and object information detected from the training images and is trained to output a single feature when an image is input at the time of inference. Here, the extractor may correspond to an encoder of an auto encoder.</div>
    </li> <li> <para-num num="[0050]"> </para-num> <div id="p-0051" num="0050" class="description-line">In particular, the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> inputs the acquired image to the <figure-callout id="110" label="pre-trained extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">pre-trained extractor</figure-callout> <b>110</b>, and may extract a single feature related to the image as an output of a first layer corresponding to a predetermined split point among a plurality of layers included in the <figure-callout id="110" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>110</b> and transmit the single feature to the cloud sever <b>200</b>.</div>
    </li> <li> <para-num num="[0051]"> </para-num> <div id="p-0052" num="0051" class="description-line">Here, in order to determine the location of a predetermined split point for the plurality of layers, the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> may analyze at least one of a network resource between the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> and the cloud sever <b>200</b>, a computing resource of the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b>, and a computing resource of the cloud sever <b>200</b> and determine the location of the split point according to the analysis result. A layer corresponding to the split point may be referred to as a first layer.</div>
    </li> <li> <para-num num="[0052]"> </para-num> <div id="p-0053" num="0052" class="description-line">The cloud sever <b>200</b> may perform core processing, such as comprehensively performing tasks received from the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> or distributing some tasks to a <figure-callout id="100" label="specific edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">specific edge device</figure-callout> <b>100</b>.</div>
    </li> <li> <para-num num="[0053]"> </para-num> <div id="p-0054" num="0053" class="description-line">Results processed in the cloud sever <b>200</b> may be delivered to the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b>.</div>
    </li> <li> <para-num num="[0054]"> </para-num> <div id="p-0055" num="0054" class="description-line">In an embodiment, the cloud sever <b>200</b> may detect an object of an image based on a single feature received from the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b>.</div>
    </li> <li> <para-num num="[0055]"> </para-num> <div id="p-0056" num="0055" class="description-line">To this end, the cloud sever <b>200</b> may include a <figure-callout id="210" label="pre-trained extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">pre-trained extractor</figure-callout> <b>210</b>, a memory that stores information of a <figure-callout id="220" label="pre-trained reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">pre-trained reconstructor</figure-callout> <b>220</b>, and a processor that controls components of the cloud sever <b>200</b>.</div>
    </li> <li> <para-num num="[0056]"> </para-num> <div id="p-0057" num="0056" class="description-line">The cloud sever <b>200</b> may obtain an output feature of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> by inputting a single feature received from the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> to a second layer immediately after a first layer among a plurality of layers of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b>. Subsequently, the <figure-callout id="200" label="processor" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">processor</figure-callout> <b>200</b> may obtain a plurality of predetermined intermediate features required for object detection from the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b> by inputting the output feature of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> to the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b>.</div>
    </li> <li> <para-num num="[0057]"> </para-num> <div id="p-0058" num="0057" class="description-line">Here, the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> may be an artificial neural network pre-trained to output an output feature when receiving training images and object information detected from the training images.</div>
    </li> <li> <para-num num="[0058]"> </para-num> <div id="p-0059" num="0058" class="description-line">The <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> included in the cloud sever <b>200</b> is the same as the <figure-callout id="110" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>110</b> included in the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b>.</div>
    </li> <li> <para-num num="[0059]"> </para-num> <div id="p-0060" num="0059" class="description-line">The <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b> may be an artificial neural network pre-trained to output a reconstructed feature when receiving training images and object information detected from the training images. The <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b> may correspond to a decoder of an auto encoder.</div>
    </li> <li> <para-num num="[0060]"> </para-num> <div id="p-0061" num="0060" class="description-line">Each layer constituting the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b> is designed to have an inverse operation relationship with the layers of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> to reconstruct predetermined intermediate layer features of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b>.</div>
    </li> <li> <para-num num="[0061]"> </para-num> <div id="p-0062" num="0061" class="description-line">In an embodiment, when the last layer of the <figure-callout id="110" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>110</b> is determined as a split point, the highest level feature is transmitted to the cloud sever <b>200</b> and passed through the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b>. On the other hand, when splitting occurs in a previous layer of the <figure-callout id="110" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>110</b>, the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> transmits a low-level feature that has progressed to the corresponding layer to the cloud sever <b>200</b>. The cloud sever <b>200</b> inputs the received feature to the layer immediately after the split point of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> to obtain a high-level output feature and passes the obtained output feature through the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b>.</div>
    </li> <li> <para-num num="[0062]"> </para-num> <div id="p-0063" num="0062" class="description-line">The cloud sever <b>200</b> uses multi-level intermediate features obtained by inputting the output features to the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b> for object detection later. Therefore, according to the present disclosure, even if the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> transmits a single feature to the cloud sever <b>200</b>, intermediate features can be obtained and thus normal object detection can be performed.</div>
    </li> <li> <para-num num="[0063]"> </para-num> <div id="p-0064" num="0063" class="description-line">Meanwhile, layers may output features of different sizes, and thus the cloud sever <b>200</b> may determine the second layer to which a received single feature will be input from among the plurality of layers included in the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> using the received single feature.</div>
    </li> <li> <para-num num="[0064]"> </para-num> <div id="p-0065" num="0064" class="description-line"> <figref idrefs="DRAWINGS">FIGS. <b>3</b>A to <b>3</b>C</figref> are diagrams for describing differences between prior arts and an embodiment of the present disclosure.</div>
    </li> <li> <para-num num="[0065]"> </para-num> <div id="p-0066" num="0065" class="description-line">Referring to <figref idrefs="DRAWINGS">FIG. <b>3</b>A</figref>, a prior art is a case in which an edge device does not include an extractor and a cloud sever includes an extractor. In this case, the edge device transmits acquired image to the cloud sever, and the cloud sever inputs the image to the extractor and obtains a plurality of predetermined intermediate features to be used for object detection from the extractor. That is, the edge device only serves to acquire an image, and the cloud sever detects an object in the image based on an artificial intelligence model. In this case, when the edge device transmits an image to the cloud sever, there may be a problem in that it takes a long to transmit the image.</div>
    </li> <li> <para-num num="[0066]"> </para-num> <div id="p-0067" num="0066" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>3</b>B</figref> describes a case in which only an edge device includes an extractor. When a plurality of predetermined intermediate features is extracted from the extractor of the edge device and the extracted intermediate features are transmitted to a cloud sever, the cloud sever detects an object in an image based on the plurality of intermediate features. In this case, the performance of the edge device is limited and the problem related to image transmission time cannot be solved.</div>
    </li> <li> <para-num num="[0067]"> </para-num> <div id="p-0068" num="0067" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>3</b>C</figref> describes a case in which an edge device and a cloud sever include the same extractor. When a single feature is extracted from an intermediate layer among a plurality of layers of the extractor of the edge device and transmitted to the cloud sever, the extractor of the cloud sever obtains a plurality of predetermined intermediate features by inputting the single feature to the layer immediately after the intermediate layer, and detects an object in an image based on the plurality of intermediate features. In this case, a delay time gain is limited because split layers are defined before the frontmost feature among a plurality of features is obtained.</div>
    </li> <li> <para-num num="[0068]"> </para-num> <div id="p-0069" num="0068" class="description-line">Distinguished from these prior arts, in the deep neural network-based <figure-callout id="1" label="object detection system" filenames="US20230368518A1-20231116-D00001.png" state="{{state}}">object detection system</figure-callout> <b>1</b> according to an embodiment of the present disclosure shown in <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref>, the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> includes the <figure-callout id="110" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>110</b>, and the cloud sever <b>200</b> includes the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> and the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b>.</div>
    </li> <li> <para-num num="[0069]"> </para-num> <div id="p-0070" num="0069" class="description-line">In summary, the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> analyzes a network speed between the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> and the <figure-callout id="200" label="processor" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">processor</figure-callout> <b>200</b> and at least one of the performances of the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> and the <figure-callout id="200" label="processor" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">processor</figure-callout> <b>200</b>, determines the location of a split point according to the analysis result, and transmits a single feature output from the <figure-callout id="11" label="first layer" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">first layer</figure-callout> <b>11</b> corresponding to the split point to the cloud sever <b>200</b>. The cloud sever <b>200</b> may obtain a high-level feature by inputting the single feature to a <figure-callout id="22" label="layer" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">layer</figure-callout> <b>22</b> of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> immediately after the <figure-callout id="11" label="first layer" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">first layer</figure-callout> <b>11</b> from which the single feature transmitted from the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> is output. Subsequently, the cloud sever <b>200</b> may input the obtained feature to the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b> to obtain a plurality of predetermined intermediate features required for object detection from the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b>. Here, each layer constituting the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b> has an inverse operation relationship with the layers of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> to reconstruct intermediate features corresponding to a predetermined intermediate layer of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b>.</div>
    </li> <li> <para-num num="[0070]"> </para-num> <div id="p-0071" num="0070" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref> is a conceptual diagram for describing a deep neural network-based object detection system according to another embodiment of the present disclosure.</div>
    </li> <li> <para-num num="[0071]"> </para-num> <div id="p-0072" num="0071" class="description-line">The deep neural network-based <figure-callout id="1" label="object detection system" filenames="US20230368518A1-20231116-D00001.png" state="{{state}}">object detection system</figure-callout> <b>1</b> described with reference to <figref idrefs="DRAWINGS">FIGS. <b>1</b> and <b>2</b> </figref> can reduce the amount of transmission the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> through the <figure-callout id="220" label="reconstructor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">reconstructor</figure-callout> <b>220</b> of the cloud sever <b>200</b>, but a problem with respect to transmission time in a wireless network situation cannot be solved. Further, in the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> having limited resources, a significant delay time may be taken to execute the <figure-callout id="110" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>110</b>.</div>
    </li> <li> <para-num num="[0072]"> </para-num> <div id="p-0073" num="0072" class="description-line">In order to solve these problems, in the deep neural network-based <figure-callout id="11" label="object detection system" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">object detection system</figure-callout> <b>11</b> according to another embodiment of the present disclosure shown in <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref>, the number of channels included in each layer of <figure-callout id="111" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}"> <figure-callout id="211" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractors</figure-callout> </figure-callout> <b>111</b> and <b>211</b> may be less than the number of channels included in each layer of the <figure-callout id="210" label="extractor" filenames="US20230368518A1-20231116-D00002.png" state="{{state}}">extractor</figure-callout> <b>210</b> shown in <figref idrefs="DRAWINGS">FIG. <b>2</b> </figref>.</div>
    </li> <li> <para-num num="[0073]"> </para-num> <div id="p-0074" num="0073" class="description-line">However, if only the number of channels of the <figure-callout id="111" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}"> <figure-callout id="211" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractors</figure-callout> </figure-callout> <b>111</b> and <b>211</b> is reduced, object detection accuracy is reduced. To compensate for this, asymmetric scaling in which the scale of the <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b> is increased is applied.</div>
    </li> <li> <para-num num="[0074]"> </para-num> <div id="p-0075" num="0074" class="description-line">An embodiment may be an asymmetric structure in which the number of channels of at least one of a plurality of layers included in the <figure-callout id="111" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}"> <figure-callout id="211" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractors</figure-callout> </figure-callout> <b>111</b> and <b>211</b> is less than the number of channels of at least one layer corresponding to at least one of a plurality of layers included in a <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b>.</div>
    </li> <li> <para-num num="[0075]"> </para-num> <div id="p-0076" num="0075" class="description-line">Further, an embodiment may be an asymmetric structure in which the number of layers included in the <figure-callout id="111" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}"> <figure-callout id="211" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractors</figure-callout> </figure-callout> <b>111</b> and <b>211</b> may be different from the number of layers included in the <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b>. In this case, the number of layers included in the <figure-callout id="111" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}"> <figure-callout id="211" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractors</figure-callout> </figure-callout> <b>111</b> and <b>211</b> may be less than the number of layers included in the <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b>.</div>
    </li> <li> <para-num num="[0076]"> </para-num> <div id="p-0077" num="0076" class="description-line">This is because the <figure-callout id="111" label="extractor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractor</figure-callout> <b>111</b> is executed in the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> with lower performance, whereas the <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b> is executed in the cloud sever <b>200</b> with better performance That is, since the <figure-callout id="111" label="extractor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractor</figure-callout> <b>111</b> is executed in the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b>, the execution time is long (that is, delay time is long). Accordingly, the number of layers included in the <figure-callout id="111" label="extractor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractor</figure-callout> <b>111</b> is less than the number of layers included in the <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b>, and thus the delay time can be reduced. In addition, the number of channels of at least one of the plurality of layers included in the <figure-callout id="111" label="extractor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractor</figure-callout> <b>111</b> is less than the number of channels of at least one layer corresponding to at least one of the plurality of layers included in the <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b>, and thus the delay time can be reduced and the amount of data transmitted from the <figure-callout id="100" label="edge device" filenames="US20230368518A1-20231116-D00001.png,US20230368518A1-20231116-D00002.png" state="{{state}}">edge device</figure-callout> <b>100</b> to the cloud sever <b>200</b> can be decreased.</div>
    </li> <li> <para-num num="[0077]"> </para-num> <div id="p-0078" num="0077" class="description-line">Meanwhile, if the number of layers included in the <figure-callout id="111" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}"> <figure-callout id="211" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractors</figure-callout> </figure-callout> <b>111</b> and <b>211</b> and/or the number of channels included in at least one of the plurality of layers included in the <figure-callout id="111" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}"> <figure-callout id="211" label="extractors" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">extractors</figure-callout> </figure-callout> <b>111</b> and <b>211</b> are reduced, the performance may deteriorate (i.e. object detection accuracy decreases).</div>
    </li> <li> <para-num num="[0078]"> </para-num> <div id="p-0079" num="0078" class="description-line">Accordingly, as shown in <figref idrefs="DRAWINGS">FIG. <b>4</b> </figref>, performance deterioration can be prevented by increasing the number of layers included in the <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b> and/or the number of channels included in at least one of the plurality of layers included in the <figure-callout id="221" label="reconstructor" filenames="US20230368518A1-20231116-D00006.png" state="{{state}}">reconstructor</figure-callout> <b>221</b>.</div>
    </li> <li> <para-num num="[0079]"> </para-num> <div id="p-0080" num="0079" class="description-line"> <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref> is a flowchart illustrating a deep neural network-based object detection method according to an embodiment of the present disclosure.</div>
    </li> <li> <para-num num="[0080]"> </para-num> <div id="p-0081" num="0080" class="description-line">The deep neural network-based object detection method according to the present embodiment may be performed in substantially the same configuration as the <figure-callout id="1" label="system" filenames="US20230368518A1-20231116-D00001.png" state="{{state}}">system</figure-callout> <b>1</b> of <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref>. Accordingly, components identical to those of the <figure-callout id="1" label="system" filenames="US20230368518A1-20231116-D00001.png" state="{{state}}">system</figure-callout> <b>1</b> of <figref idrefs="DRAWINGS">FIG. <b>1</b> </figref> are denoted by the same reference numerals, and redundant descriptions are omitted.</div>
    </li> <li> <para-num num="[0081]"> </para-num> <div id="p-0082" num="0081" class="description-line">Referring to <figref idrefs="DRAWINGS">FIG. <b>5</b> </figref>, in the deep neural network-based object detection method according to an embodiment of the present disclosure, an edge device inputs an acquired image to a pre-trained extractor (S<b>110</b>).</div>
    </li> <li> <para-num num="[0082]"> </para-num> <div id="p-0083" num="0082" class="description-line">Next, the edge device extracts a single feature of the image as an output of a first layer corresponding to a predetermined split point among a plurality of layers included in the extractor (S<b>120</b>).</div>
    </li> <li> <para-num num="[0083]"> </para-num> <div id="p-0084" num="0083" class="description-line">Next, the edge device transmits the extracted single feature to the cloud sever (S<b>130</b>). Next, the cloud sever including the same extractor as that of the edge device and a reconstructor obtains an output feature of the extractor by inputting the single feature to a second layer immediately after the first layer (S<b>140</b>).</div>
    </li> <li> <para-num num="[0084]"> </para-num> <div id="p-0085" num="0084" class="description-line">Next, the cloud sever obtains a plurality of intermediate features to be used to detect an object in the image by inputting the output features to the reconstructor (S<b>150</b>).</div>
    </li> <li> <para-num num="[0085]"> </para-num> <div id="p-0086" num="0085" class="description-line">Next, the cloud sever may detect the object in the image based on the plurality of intermediate features (S<b>160</b>).</div>
    </li> <li> <para-num num="[0086]"> </para-num> <div id="p-0087" num="0086" class="description-line">The deep neural network-based object detection system described above may be implemented by a computing device including at least some of a processor, a memory, a user input device, and a presentation device. The memory is a medium that stores computer-readable software coded such that a specific task can be performed when executed by the processor, applications, program modules, routines, instructions, and/or data. The processor may read and execute the computer-readable software, applications, program modules, routines, instructions, and/or data stored in the memory.</div>
    </li> <li> <para-num num="[0087]"> </para-num> <div id="p-0088" num="0087" class="description-line">The computing device may include a variety of devices such as smartphones, tablets, laptops, desktops, servers, and clients. The computing device may be a single stand-alone device or may include multiple computing devices operating in a distributed environment and cooperating with each other over a communications network.</div>
    </li> <li> <para-num num="[0088]"> </para-num> <div id="p-0089" num="0088" class="description-line">In addition, the above-described deep neural network-based object detection method may be executed by a computing device including a processor and a memory that stores computer-readable software coded to perform an object detection method using an artificial intelligence model when executed by the processor, applications, program modules, routines, instructions, and/or data structures.</div>
    </li> <li> <para-num num="[0089]"> </para-num> <div id="p-0090" num="0089" class="description-line">The present embodiments described above may be implemented through various means. For example, the present embodiments may be implemented by hardware, firmware, software, or a combination thereof.</div>
    </li> <li> <para-num num="[0090]"> </para-num> <div id="p-0091" num="0090" class="description-line">In the case of implementation by hardware, an image diagnosis method using an artificial intelligence model according to the present embodiments may be implemented by one or more application specific integrated circuits (ASICs), digital signal processors (DSPs), digital signal processing devices (DSPDs), programmable logic devices (PLDs), field programmable gate arrays (FPGAs), processors, controllers, microcontrollers, or microprocessors.</div>
    </li> <li> <para-num num="[0091]"> </para-num> <div id="p-0092" num="0091" class="description-line">For example, an object detection method according to embodiments may be implemented using an artificial intelligence semiconductor device in which neurons and synapses of a deep neural network are implemented as semiconductor devices. In this case, the semiconductor devices may be currently used semiconductor devices such as SRAMs, DRAMs, and NANDs, next-generation semiconductor devices, RRAMs, STT MRAMs, PRAMs, or a combination thereof.</div>
    </li> <li> <para-num num="[0092]"> </para-num> <div id="p-0093" num="0092" class="description-line">When the object detection method according to the embodiments is implemented using an artificial intelligence semiconductor device, results (weights) of learning an auto-encoder as software may be transferred to synaptic mimic devices arranged in an array, or learning may be performed in the artificial intelligence semiconductor device.</div>
    </li> <li> <para-num num="[0093]"> </para-num> <div id="p-0094" num="0093" class="description-line">Combinations of steps in each flowchart attached to the present disclosure may be executed by computer program instructions. Since the computer program instructions can be mounted on a processor of a general-purpose computer, a special purpose computer, or other programmable data processing equipment, the instructions executed by the processor of the computer or other programmable data processing equipment create a means for performing the functions described in each step of the flowchart. The computer program instructions can also be stored on a computer-usable or computer-readable storage medium which can be directed to a computer or other programmable data processing equipment to implement a function in a specific manner. Accordingly, the instructions stored on the computer-usable or computer-readable recording medium can also produce an article of manufacture containing an instruction means which performs the functions described in each step of the flowchart. The computer program instructions can also be mounted on a computer or other programmable data processing equipment. Accordingly, a series of operational steps are performed on a computer or other programmable data processing equipment to create a computer-executable process, and it is also possible for instructions to perform a computer or other programmable data processing equipment to provide steps for performing the functions described in each step of the flowchart.</div>
    </li> <li> <para-num num="[0094]"> </para-num> <div id="p-0095" num="0094" class="description-line">In addition, each step may represent a module, a segment, or a portion of codes which contains one or more executable instructions for executing the specified logical function(s). It should also be noted that in some alternative embodiments, the functions mentioned in the steps may occur out of order. For example, two steps illustrated in succession may in fact be performed substantially simultaneously, or the steps may sometimes be performed in a reverse order depending on the corresponding function.</div>
    </li> <li> <para-num num="[0095]"> </para-num> <div id="p-0096" num="0095" class="description-line">The above description is merely exemplary description of the technical scope of the present disclosure, and it will be understood by those skilled in the art that various changes and modifications can be made without departing from original characteristics of the present disclosure. Therefore, the embodiments disclosed in the present disclosure are intended to explain, not to limit, the technical scope of the present disclosure, and the technical scope of the present disclosure is not limited by the embodiments. The protection scope of the present disclosure should be interpreted based on the following claims and it should be appreciated that all technical scopes included within a range equivalent thereto are included in the protection scope of the present disclosure.</div>
    
  </li> </ul>
  </div>
  </section>

  <section itemprop="claims" itemscope>
    <h2>Claims (<span itemprop="count">19</span>)</h2>
    
    <div itemprop="content" html><div mxw-id="PCLM435165540" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement>
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text"> <b>1</b>. A cloud sever for detecting an object of an image based on a single feature received from an edge device, comprising:
<div class="claim-text">a memory configured to store information on a pre-trained extractor of the cloud sever having a first layer and a second layer and a pre-trained reconstructor of the cloud sever; and</div> <div class="claim-text">a processor,</div> <div class="claim-text">wherein the processor is configured to:</div> <div class="claim-text">receive a single feature of an image from the edge device, wherein the edge device includes an extractor having a plurality of layers and the single feature of the image is extracted by a first layer of the extractor of the edge device corresponding to a predetermined split point among the plurality of layers of the extractor of the edge device; and</div> <div class="claim-text">obtain an output feature of the extractor of the cloud sever by inputting the single feature received from the first layer included in the extractor of the edge device to the second layer included in the extractor of the cloud sever, and</div> <div class="claim-text">detect an object in the image by inputting the output feature obtained from the extractor of the cloud sever to the reconstructor of the cloud sever.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text"> <b>2</b>. The cloud sever of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the extractor of the edge device or the extractor of the cloud sever is an artificial neural network trained by receiving training images and object information detected from the training images, and
<div class="claim-text">wherein the extractor of the edge device or the extractor of the cloud sever is trained to output a single feature corresponding to each of the training images.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text"> <b>3</b>. The cloud sever of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the reconstructor of the cloud sever is an artificial neural network trained by receiving training images and object information detected from the training images, and
<div class="claim-text">wherein the reconstructor of the cloud sever is trained to output a plurality of features corresponding to the single feature output from the extractor of the edge device or the extractor of the cloud sever.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text"> <b>4</b>. The cloud sever of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each layer of the reconstructor of the cloud sever has an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the edge device or the extractor of the cloud sever.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text"> <b>5</b>. The cloud sever of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever is less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text"> <b>6</b>. The cloud sever of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever is less than a number of the plurality of layers included in the reconstructor of the cloud sever.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text"> <b>7</b>. An edge device for transmitting a single feature obtained from an image to a cloud sever, comprising:
<div class="claim-text">a memory configured to store information on a pre-trained extractor of the edge device having a plurality of layers; and</div> <div class="claim-text">a processor,</div> <div class="claim-text">wherein the processor is configured to:</div> <div class="claim-text">input the image to the pre-trained extractor of the edge device,</div> <div class="claim-text">extract the single feature of the image from a first layer corresponding to a predetermined split point among the plurality of layers included in the extractor of the edge device, and</div> <div class="claim-text">transmit the extracted single feature to the cloud sever.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text"> <b>8</b>. The edge device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the cloud sever includes a memory configured to store information on an extractor of the cloud sever including same structure as the extractor of the edge device and a reconstructor of the cloud sever, and a processor,
<div class="claim-text">wherein the processor of the cloud sever is configured to:</div> <div class="claim-text">receive the single feature from the edge device,</div> <div class="claim-text">input the single feature received from the extractor of the edge device to a portion of the extractor of the cloud sever,</div> <div class="claim-text">obtain an output feature ouput from the extractor of the cloud sever,</div> <div class="claim-text">input the output feature to the reconstructor of the cloud sever, and</div> <div class="claim-text">obtain a plurality of intermediate features used to detect an object in the image ouput from the reconstructor of the cloud sever.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text"> <b>9</b>. The edge device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the processor of the edge device is configured to analyze at least one of a network resource between the edge device and the cloud sever, a computing resource of the edge device, and a computing resource of the cloud sever, and determine a location of the split point with respect to the first layer based on an analysis result.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text"> <b>10</b>. The edge device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the extractor of the cloud sever and the reconstructor of the cloud sever includes a plurality of layers, and
<div class="claim-text">wherein each layer of the reconstructor of the cloud sever has an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the cloud sever.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text"> <b>11</b>. The edge device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the extractor of the cloud sever and the reconstructor of the cloud sever includes a plurality of layers, and
<div class="claim-text">wherein a number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever is less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text"> <b>12</b>. The edge device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the extractor of the cloud sever and the reconstructor of the cloud sever includes a plurality of layers, and
<div class="claim-text">wherein a number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever is less than a number of the plurality of layers included in the reconstructor of the cloud sever.</div> </div>
    </div>
    </div> <div class="claim"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text"> <b>13</b>. A deep neural network-based object detection method comprising:
<div class="claim-text">inputting, by a processor included in an edge device, an acquired image to a pre-trained extractor of the edge device;</div> <div class="claim-text">extracting, by the processor included in the edge device, a single feature corresponding to the image from a first layer of the extractor of the edge device corresponding to a predetermined split point among a plurality of layers included in the extractor of the edge device;</div> <div class="claim-text">transmitting, by the processor included in the edge device, the extracted single feature to a cloud sever;</div> <div class="claim-text">receiving, by a processor included in the cloud sever, the single feature from the edge device, wherein the cloud sever includes an extractor of the cloud sever having a first layer and a second layer and a reconstructor;</div> <div class="claim-text">obtaining, by the processor included in the cloud sever, an output feature of the extractor of the cloud sever by inputting the single feature to the second layer included in the extractor of the cloud sever; and</div> <div class="claim-text">obtaining, by the processor included in the cloud sever, a plurality of intermediate features used to detect an object in the image by inputting the output feature to the reconstructor of the cloud sever.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text"> <b>14</b>. The deep neural network-based object detection method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:
<div class="claim-text">analyzing, by the processor included in the edge device, at least one of a network resource between the edge device and the cloud sever, a computing resource of the edge device, and a computing resource of the cloud sever; and</div> <div class="claim-text">determining a location of the split point with respect to the first layer included in the extractor of the edge device based on an analysis result.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text"> <b>15</b>. The deep neural network-based object detection method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the extractor of the edge device or the extractor of the cloud sever is an artificial neural network trained by receiving training images and object information detected from the training images, and
<div class="claim-text">wherein the extractor of the edge device or the extractor of the cloud sever is trained to output a single feature corresponding to each of the training images.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text"> <b>16</b>. The deep neural network-based object detection method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the reconstructor of the cloud sever is an artificial neural network trained by receiving training images and object information detected from the training images, and
<div class="claim-text">wherein the reconstructor of the cloud sever is trained to output a plurality of features corresponding to the single feature output from the extractor of the edge device or the extractor of the cloud sever.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text"> <b>17</b>. The deep neural network-based object detection method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each layer of the reconstructor of the cloud sever has an inverse operation relationship with each layer of the extractor of the edge device or the extractor of the cloud sever to reconstruct predetermined intermediate layer features of the extractor of the edge device or the extractor of the cloud sever.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text"> <b>18</b>. The deep neural network-based object detection method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a number of channels of at least one of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever is less than a number of channels corresponding to at least one of a plurality of layers included in the reconstructor of the cloud sever.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text"> <b>19</b>. The deep neural network-based object detection method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a number of the plurality of layers included in the extractor of the edge device or the extractor of the cloud sever is less than a number of the plurality of layers included in the reconstructor of the cloud sever.</div>
    </div>
  </div> </div>
  </div>
  </section>

  <section itemprop="application" itemscope>

    <section itemprop="metadata" itemscope>
      <span itemprop="applicationNumber">US18/197,891</span>
      <span itemprop="priorityDate">2022-05-16</span>
      <span itemprop="filingDate">2023-05-16</span>
      <span itemprop="title">Deep neural network-based object detection method, and cloud sever and edge device performing deep neural network-based object detection method 
     </span>
      <span itemprop="ifiStatus">Pending</span>
      
      <a href="/patent/US20230368518A1/en">
        <span itemprop="representativePublication">US20230368518A1</span>
        (<span itemprop="primaryLanguage">en</span>)
      </a>
    </section>

    

    <h2>Applications Claiming Priority (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">KR1020220059423A</span>
            <a href="/patent/KR20230159971A/en">
              <span itemprop="representativePublication">KR20230159971A</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2022-05-16</td>
          <td itemprop="filingDate">2022-05-16</td>
          <td itemprop="title">Apparatus, system and method for detecting object based on deep neural network 
       </td>
        </tr>
        <tr itemprop="appsClaimingPriority" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">KR10-2022-0059423</span>
            
          </td>
          <td itemprop="priorityDate"></td>
          <td itemprop="filingDate">2022-05-16</td>
          <td itemprop="title"></td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Publications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Publication Number</th>
          <th>Publication Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="pubs" itemscope repeat>
          <td>
            <span itemprop="publicationNumber">US20230368518A1</span>
            
            <span itemprop="thisPatent">true</span>
            <a href="/patent/US20230368518A1/en">
              US20230368518A1
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-11-16</td>
        </tr>
      </tbody>
    </table>

  </section>

  <section itemprop="family" itemscope>
    <h1>Family</h1>
    <h2>ID=88699317</h2>

    <h2>Family Applications (1)</h2>
    <table>
      <thead>
        <tr>
          <th>Application Number</th>
          <th>Title</th>
          <th>Priority Date</th>
          <th>Filing Date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="applications" itemscope repeat>
          <td>
            <span itemprop="applicationNumber">US18/197,891</span>
            <span itemprop="ifiStatus">Pending</span>
            
            <a href="/patent/US20230368518A1/en">
              <span itemprop="representativePublication">US20230368518A1</span>
                (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="priorityDate">2022-05-16</td>
          <td itemprop="filingDate">2023-05-16</td>
          <td itemprop="title">Deep neural network-based object detection method, and cloud sever and edge device performing deep neural network-based object detection method 
     </td>
        </tr>
      </tbody>
    </table>

    

    

    <h2>Country Status (2)</h2>
    <table>
      <thead>
        <tr>
          <th>Country</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">US</span>
            (<span itemprop="num">1</span>)
            <meta itemprop="thisCountry" content="true">
          </td>
          <td>
            <a href="/patent/US20230368518A1/en">
              <span itemprop="representativePublication">US20230368518A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
        <tr itemprop="countryStatus" itemscope repeat>
          <td>
            <span itemprop="countryCode">KR</span>
            (<span itemprop="num">1</span>)
            
          </td>
          <td>
            <a href="/patent/KR20230159971A/en">
              <span itemprop="representativePublication">KR20230159971A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
        </tr>
      </tbody>
    </table>

    

    

    

    <h2>Family Cites Families (2)</h2>
    <table>
      <caption>* Cited by examiner,  Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/KR102310187B1/en">
              <span itemprop="publicationNumber">KR102310187B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-09-25</td>
          <td itemprop="publicationDate">2021-10-08</td>
          <td><span itemprop="assigneeOriginal"></span></td>
          <td itemprop="title">A distributed computing system including multiple edges and cloud, and method for providing model for using adaptive intelligence thereof 
       </td>
        </tr>
        <tr itemprop="backwardReferencesFamily" itemscope repeat>
          <td>
            <a href="/patent/KR20210062346A/en">
              <span itemprop="publicationNumber">KR20210062346A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2019-11-21</td>
          <td itemprop="publicationDate">2021-05-31</td>
          <td><span itemprop="assigneeOriginal"></span></td>
          <td itemprop="title">Artifical intelligence node and method for compressing feature map thereof 
     </td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2022</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2022-05-16</span>
            <span itemprop="countryCode">KR</span>
            <span itemprop="applicationNumber">KR1020220059423A</span>
            <a href="/patent/KR20230159971A/en"><span itemprop="documentId">patent/KR20230159971A/en</span></a>
            <span itemprop="legalStatusCat">unknown</span>
            <span itemprop="legalStatus"></span>
            
          </li>
        </ul>
      </li>
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2023</span>
        <ul>
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2023-05-16</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US18/197,891</span>
            <a href="/patent/US20230368518A1/en"><span itemprop="documentId">patent/US20230368518A1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Pending</span>
            <span itemprop="thisApp" content="true" bool></span>
          </li>
        </ul>
      </li>
    </ul>

    </section>

  

  

  

  <section>
    <h2>Also Published As</h2>
    <table>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Publication date</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="docdbFamily" itemscope repeat>
          <td>
            <a href="/patent/KR20230159971A/en">
              <span itemprop="publicationNumber">KR20230159971A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
          </td>
          <td itemprop="publicationDate">2023-11-23</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Similar Documents</h2>
    <table>
      <thead>
        <tr>
          <th>Publication</th>
          <th>Publication Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/EP3673417B1/en">
                <span itemprop="publicationNumber">EP3673417B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-10-04">2023-10-04</time>
            
            
          </td>
          <td itemprop="title">System and method for distributive training and weight distribution in a neural network 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US10860837B2/en">
                <span itemprop="publicationNumber">US10860837B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-12-08">2020-12-08</time>
            
            
          </td>
          <td itemprop="title">Deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20190095764A1/en">
                <span itemprop="publicationNumber">US20190095764A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2019-03-28">2019-03-28</time>
            
            
          </td>
          <td itemprop="title">Method and system for determining objects depicted in images 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/KR102469261B1/en">
                <span itemprop="publicationNumber">KR102469261B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-11-22">2022-11-22</time>
            
            
          </td>
          <td itemprop="title">Adaptive artificial neural network selection techniques 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN110799992B/en">
                <span itemprop="publicationNumber">CN110799992B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-09-12">2023-09-12</time>
            
            
          </td>
          <td itemprop="title">Use of simulation and domain adaptation for robot control 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN106462940A/en">
                <span itemprop="publicationNumber">CN106462940A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2017-02-22">2017-02-22</time>
            
            
          </td>
          <td itemprop="title">Generic object detection in images 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/WO2021257128A3/en">
                <span itemprop="publicationNumber">WO2021257128A3</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-03-10">2022-03-10</time>
            
            
          </td>
          <td itemprop="title">Quantum computing based deep learning for detection, diagnosis and other applications 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US11423577B2/en">
                <span itemprop="publicationNumber">US11423577B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2022-08-23">2022-08-23</time>
            
            
          </td>
          <td itemprop="title">Printed circuit board assembly defect detection 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN111027576A/en">
                <span itemprop="publicationNumber">CN111027576A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-04-17">2020-04-17</time>
            
            
          </td>
          <td itemprop="title">Cooperative significance detection method based on cooperative significance generation type countermeasure network 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/JP7332238B2/en">
                <span itemprop="publicationNumber">JP7332238B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-08-23">2023-08-23</time>
            
            
          </td>
          <td itemprop="title">
  Methods and Apparatus for Physics-Guided Deep Multimodal Embedding for Task-Specific Data Utilization
 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/KR20200084444A/en">
                <span itemprop="publicationNumber">KR20200084444A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-07-13">2020-07-13</time>
            
            
          </td>
          <td itemprop="title">Deep learning based classification system using image data augmentation, and cotrol method thereof 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="6037986743696442448">
              <a href="/scholar/6037986743696442448"><span itemprop="scholarAuthors">Tsai et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2022">2022</time>
            
          </td>
          <td itemprop="title">MobileNet-JDE: a lightweight multi-object tracking model for embedded systems</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230368518A1/en">
                <span itemprop="publicationNumber">US20230368518A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-11-16">2023-11-16</time>
            
            
          </td>
          <td itemprop="title">Deep neural network-based object detection method, and cloud sever and edge device performing deep neural network-based object detection method 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN116665114B/en">
                <span itemprop="publicationNumber">CN116665114B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-10-10">2023-10-10</time>
            
            
          </td>
          <td itemprop="title">Multi-mode-based remote sensing scene identification method, system and medium 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN112465847A/en">
                <span itemprop="publicationNumber">CN112465847A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-03-09">2021-03-09</time>
            
            
          </td>
          <td itemprop="title">Edge detection method, device and equipment based on clear boundary prediction 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20210174163A1/en">
                <span itemprop="publicationNumber">US20210174163A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2021-06-10">2021-06-10</time>
            
            
          </td>
          <td itemprop="title">Edge inference for artifical intelligence (ai) models 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20190370675A1/en">
                <span itemprop="publicationNumber">US20190370675A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2019-12-05">2019-12-05</time>
            
            
          </td>
          <td itemprop="title">Methods, systems and apparatus to improve deep learning resource efficiency 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="781414869978416956">
              <a href="/scholar/781414869978416956"><span itemprop="scholarAuthors">Yang et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2023">2023</time>
            
          </td>
          <td itemprop="title">Foreground enhancement network for object detection in sonar images</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="15703643930131719847">
              <a href="/scholar/15703643930131719847"><span itemprop="scholarAuthors">Sundarama et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2020">2020</time>
            
          </td>
          <td itemprop="title">FSSCaps-DetCountNet: fuzzy soft sets and CapsNet-based detection and counting network for monitoring animals from aerial images</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="12215235577747659570">
              <a href="/scholar/12215235577747659570"><span itemprop="scholarAuthors">Cuperlier et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2016">2016</time>
            
          </td>
          <td itemprop="title">FPGA-based bio-inspired architecture for multi-scale attentional vision</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/US20230386192A1/en">
                <span itemprop="publicationNumber">US20230386192A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2023-11-30">2023-11-30</time>
            
            
          </td>
          <td itemprop="title">Deep neural network-based real-time inference method, and cloud device and edge device performing deep neural network-based real-time inference method 
     </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="18222368096051053449">
              <a href="/scholar/18222368096051053449"><span itemprop="scholarAuthors">Ozaeta et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2023">2023</time>
            
          </td>
          <td itemprop="title">Seagrass Classification Using Differentiable Architecture Search</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="855588076728048860">
              <a href="/scholar/855588076728048860"><span itemprop="scholarAuthors">Kaarud et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2020">2020</time>
            
          </td>
          <td itemprop="title">Drone-based Detection of Sheep using Thermal and Visual Cameras: A Complete Approach</td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              <a href="/patent/CN109740593B/en">
                <span itemprop="publicationNumber">CN109740593B</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2020-11-13">2020-11-13</time>
            
            
          </td>
          <td itemprop="title">Method and device for determining position of at least one predetermined target in sample 
       </td>
        </tr>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="15699320066541636637">
              <a href="/scholar/15699320066541636637"><span itemprop="scholarAuthors">Chen et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2020">2020</time>
            
          </td>
          <td itemprop="title">Bearing fault diagnosis based on improved denoising auto-encoders</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Legal Events</h2>
    <table>
      <thead>
        <tr>
          <th>Date</th>
          <th>Code</th>
          <th>Title</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-05-16">2023-05-16</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">RESEARCH &amp; BUSINESS FOUNDATION SUNGKYUNKWAN UNIVERSITY, KOREA, REPUBLIC OF</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LEE, JOO CHAN;KO, JONG HWAN;REEL/FRAME:063656/0174</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Effective date</strong>:
              <span itemprop="value">20230512</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2023-06-16">2023-06-16</time></td>
          <td itemprop="code">STPP</td>
          <td itemprop="title">Information on status: patent application and granting procedure in general</td>
          <td>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">DOCKETED NEW CASE - READY FOR EXAMINATION</span>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </section>

</article>

    </search-app>
    <script type="text/javascript" src="//www.gstatic.com/feedback/api.js"></script>
    <script type="text/javascript" src="//www.gstatic.com/feedback/js/help/prod/service/lazy.min.js"></script>
    <script type="text/javascript">
      if (window.help && window.help.service) {
        helpApi = window.help.service.Lazy.create(0, {apiKey: 'AIzaSyDTEI_0tLX4varJ7bwK8aT-eOI5qr3BmyI', locale: 'en-US'});
        window.requestedSurveys = new Set();
        window.requestSurvey = function(triggerId) {
          if (window.requestedSurveys.has(triggerId)) {
            return;
          }
          window.requestedSurveys.add(triggerId);
          helpApi.requestSurvey({
            triggerId: triggerId,
            enableTestingMode: false,
            callback: (requestSurveyCallbackParam) => {
              if (!requestSurveyCallbackParam.surveyData) {
                return;
              }
              helpApi.presentSurvey({
                productData: {
                  productVersion: window.version,
                  customData: {
                    "experiments": "72459301,72474719",
                  },
                },
                surveyData: requestSurveyCallbackParam.surveyData,
                colorScheme: 1,
                customZIndex: 10000,
              });
            }
          });
        };

        window.requestSurvey('YXTwAsvoW0kedxbuTdH0RArc9VhT');
      }
    </script>
    <script src="/sw/null_loader.js"></script>
  </body>
</html>
